"""
RAPTOR Treeæ§‹ç¯‰ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
æ®µéšçš„ã«ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’å¢—ã‚„ã—ã¦æ€§èƒ½ã¨å“è³ªã‚’è©•ä¾¡

ãƒ†ã‚¹ãƒˆæ®µéš: 4250ãƒãƒ£ãƒ³ã‚¯ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°4ã‚±ãƒ¼ã‚¹
1)  250ãƒãƒ£ãƒ³ã‚¯
2)  500ãƒãƒ£ãƒ³ã‚¯  
3) 1000ãƒãƒ£ãƒ³ã‚¯
4) 2000ãƒãƒ£ãƒ³ã‚¯ï¼ˆNote. time up exitï¼‰

è©•ä¾¡æŒ‡æ¨™:
- æ§‹ç¯‰æ™‚é–“ï¼ˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€è¦ç´„ï¼‰
- GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
- ãƒ„ãƒªãƒ¼æ§‹é€ ï¼ˆæ·±ã•ã€ãƒãƒ¼ãƒ‰æ•°ï¼‰
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªï¼ˆSilhouette Scoreç­‰ï¼‰
"""

import os
import sys
import time
import json
import random
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from pathlib import Path
from datetime import datetime
import subprocess

# UTF-8å‡ºåŠ›è¨­å®šï¼ˆWindowså¯¾å¿œï¼‰
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
matplotlib.rcParams['font.family'] = ['MS Gothic', 'Yu Gothic', 'Meiryo', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '0_base_tsunami-lesson-rag'))

# æ©‹æ¢è¨ºæ–­ç”¨ã®ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from visual_raptor_colbert_bridge import BridgeRAPTORColBERT, VisualDocument
from PIL import Image

print("=" * 80)
print("RAPTOR Treeæ§‹ç¯‰ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
print("ColBERT v2.0 + BLIP2 (OPT-2.7B + 8bité‡å­åŒ–) ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä½¿ç”¨")
print("=" * 80)

# GPUæƒ…å ±å–å¾—
def get_gpu_memory_usage():
    """GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=memory.used,memory.total,utilization.gpu', 
             '--format=csv,noheader,nounits'],
            capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            if lines:
                values = lines[0].split(',')
                return {
                    'memory_used_mb': float(values[0].strip()),
                    'memory_total_mb': float(values[1].strip()),
                    'gpu_utilization': float(values[2].strip())
                }
    except Exception as e:
        print(f"  âš ï¸ GPUæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    return {'memory_used_mb': 0, 'memory_total_mb': 0, 'gpu_utilization': 0}

def count_tree_nodes(tree):
    """ãƒ„ãƒªãƒ¼ã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—"""
    if not tree or not isinstance(tree, dict):
        return {'num_leaf_nodes': 0, 'num_internal_nodes': 0, 'total_nodes': 0, 'max_depth': 0}
    
    def count_recursive(node, depth=0):
        if not node or not isinstance(node, dict):
            return (0, 0, depth)
        
        clusters = node.get('clusters', {})
        if not clusters:
            return (1, 0, depth)
        
        total_leaf = 0
        total_internal = 1
        max_child_depth = depth
        
        for cluster_id, cluster_data in clusters.items():
            if isinstance(cluster_data, dict) and 'children' in cluster_data:
                leaf, internal, child_depth = count_recursive(cluster_data['children'], depth + 1)
                total_leaf += leaf
                total_internal += internal
                max_child_depth = max(max_child_depth, child_depth)
        
        return (total_leaf, total_internal, max_child_depth)
    
    leaf_count, internal_count, max_depth = count_recursive(tree, 0)
    
    return {
        'num_leaf_nodes': leaf_count,
        'num_internal_nodes': internal_count,
        'total_nodes': leaf_count + internal_count,
        'max_depth': max_depth
    }

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
# data_dir = Path("data/encoder_comparison_46pdfs")  # ç½å®³æ•™è¨“ãƒ‡ãƒ¼ã‚¿
data_dir = Path("data/doken_bridge_diagnosis_logic")  # æ©‹æ¢è¨ºæ–­ãƒ‡ãƒ¼ã‚¿
images_dir = data_dir / "images"
cache_file = data_dir / "pdf_text_cache.json"
results_dir = data_dir / "results"
results_dir.mkdir(exist_ok=True, parents=True)

print(f"\n[ã‚¹ãƒ†ãƒƒãƒ— 1/6] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
print(f"  ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {images_dir}")
print(f"  ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cache_file}")

# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
image_files = sorted(list(images_dir.glob("*.png")))
print(f"âœ… {len(image_files)}æšã®ç”»åƒã‚’ç™ºè¦‹")

with open(cache_file, 'r', encoding='utf-8') as f:
    text_cache = json.load(f)
print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿: {len(text_cache)}ã‚¨ãƒ³ãƒˆãƒª")

# VisualDocumentã‚’ä½œæˆã—ã¦ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
print(f"\n[ã‚¹ãƒ†ãƒƒãƒ— 2/6] VisualDocumentã‚’ä½œæˆã—ã¦ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¸­...")
visual_documents = []
chunk_size = 500
chunk_overlap = 50

total_pages_with_text = 0
total_chunks_created = 0

for img_file in image_files:
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚­ãƒ¼ã¯ç›¸å¯¾ãƒ‘ã‚¹å½¢å¼
    page_key_short = img_file.stem  # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿
    page_key_full = str(img_file)   # ãƒ•ãƒ«ãƒ‘ã‚¹
    
    # ä¸¡æ–¹ã®å½¢å¼ã§è©¦ã™
    cached_text = None
    if page_key_full in text_cache:
        cached_text = text_cache[page_key_full]
    elif page_key_short in text_cache:
        cached_text = text_cache[page_key_short]
    
    if cached_text:
        total_pages_with_text += 1
        
        # ç©ºãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
        if not cached_text or len(cached_text.strip()) == 0:
            continue
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        text_length = len(cached_text)
        if text_length <= chunk_size:
            chunks = [cached_text]
        else:
            chunks = []
            start = 0
            while start < text_length:
                end = min(start + chunk_size, text_length)
                chunk_text = cached_text[start:end]
                chunks.append(chunk_text)
                start += chunk_size - chunk_overlap
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã‚’VisualDocumentã¨ã—ã¦ä½œæˆ
        for chunk_idx, chunk_text in enumerate(chunks):
            visual_doc = VisualDocument(
                image_path=str(img_file),
                text_content=chunk_text,
                metadata={
                    'source': page_key_short,
                    'chunk_index': chunk_idx,
                    'total_chunks': len(chunks)
                }
            )
            visual_documents.append(visual_doc)
            total_chunks_created += 1

print(f"âœ… {len(image_files)}ãƒšãƒ¼ã‚¸ä¸­{total_pages_with_text}ãƒšãƒ¼ã‚¸ã«ãƒ†ã‚­ã‚¹ãƒˆã‚ã‚Š")
print(f"âœ… {total_chunks_created}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆå®Œäº†")
print(f"   å¹³å‡ãƒãƒ£ãƒ³ã‚¯æ•°/ãƒšãƒ¼ã‚¸: {total_chunks_created/max(total_pages_with_text, 1):.1f}")

# ãƒãƒ£ãƒ³ã‚¯æ•°ãƒã‚§ãƒƒã‚¯
if len(visual_documents) == 0:
    print("\nâŒ ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ã‚¯ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    print("  åŸå› : ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã€å…¨ã¦ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™")
    sys.exit(1)

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚µã‚¤ã‚ºï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿é‡ã‚’è¶…ãˆãªã„ã‚ˆã†ã«èª¿æ•´ï¼‰
max_chunks = len(visual_documents)
# sample_sizes = []
# for size in [250, 500, 1000, 2000]:
#     if size <= max_chunks:
#         sample_sizes.append(size)
# 
# # é‡è¤‡ã‚’å‰Šé™¤ã—ã¦ã‚½ãƒ¼ãƒˆ
# sample_sizes = sorted(list(set(sample_sizes)))

# æ©‹æ¢è¨ºæ–­ãƒ‡ãƒ¼ã‚¿: 1000ãƒãƒ£ãƒ³ã‚¯ã§ãƒ†ã‚¹ãƒˆï¼ˆ5å±¤éšå±¤ã€depth=5ï¼‰
sample_sizes = [1000] if 1000 <= max_chunks else [max_chunks]

print(f"\nå®Ÿéš›ã®ãƒãƒ£ãƒ³ã‚¯æ•°: {max_chunks}")
print(f"èª¿æ•´å¾Œã®ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º: {sample_sizes}")

results = []

print(f"\n[ã‚¹ãƒ†ãƒƒãƒ— 3/6] ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆé–‹å§‹")
print(f"  ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º: {sample_sizes}")
print("=" * 80)

# å„ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§ãƒ†ã‚¹ãƒˆ
for test_num, sample_size in enumerate(sample_sizes, 1):
    print(f"\n{'='*80}")
    print(f"ãƒ†ã‚¹ãƒˆ {test_num}/{len(sample_sizes)}: {sample_size}ãƒãƒ£ãƒ³ã‚¯")
    print(f"{'='*80}")
    
    # ã“ã®ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    test_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå†ç¾æ€§ã®ãŸã‚seedå›ºå®šï¼‰
    random.seed(42)
    if sample_size < len(visual_documents):
        sampled_docs = random.sample(visual_documents, sample_size)
        print(f"  ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {sample_size}/{len(visual_documents)}ãƒãƒ£ãƒ³ã‚¯")
    else:
        sampled_docs = visual_documents
        print(f"  å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨: {sample_size}ãƒãƒ£ãƒ³ã‚¯")
    
    # ColVBERTã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    print(f"\n[åˆæœŸåŒ–] ColVBERT (BLIP) ã‚·ã‚¹ãƒ†ãƒ ...")
    init_start = time.time()
    
    # Embeddings ã¨ LLM ã®ä½œæˆ
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_ollama import ChatOllama
    import torch
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    embeddings_model = HuggingFaceEmbeddings(
        model_name="mixedbread-ai/mxbai-embed-large-v1",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True, 'batch_size': 64}
    )
    
    llm = ChatOllama(
        model="gpt-oss:20b",
        base_url="http://localhost:11434",
        temperature=0.3,
        num_ctx=32768,      # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ç¸®å° (100000 â†’ 32768)
        num_predict=2048    # ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™ã—ã¦ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
    )
    
    colbert_config = {
        'encoder_type': 'blip2_colbert',  # BLIP2 + ColVBERTv2.0ã‚’ä½¿ç”¨
        'blip2_model': 'Salesforce/blip2-opt-2.7b',  # BLIP2 with OPT-2.7B (è»½é‡ç‰ˆ)
        'colbert_model': 'colbert-ir/colbertv2.0',  # ColBERT v2.0
        'embedding_dim': 768,
        'use_quantization': True,  # 8-bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
        'image_batch_size': 4  # ç”»åƒãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ï¼ˆGPU OOMå¯¾ç­–ï¼‰
    }
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡æŒ‡æ¨™è¨­å®š
    # Combinedæˆ¦ç•¥: ã‚¯ãƒ©ã‚¹ã‚¿å“è³ªã¨åˆ†é›¢åº¦ã®ãƒãƒ©ãƒ³ã‚¹
    colbert_system = BridgeRAPTORColBERT(
        embeddings_model=embeddings_model,
        llm=llm,
        use_blip2_colbert=True,  # BLIP2 + ColVBERTv2.0ã‚’æœ‰åŠ¹åŒ–
        colbert_config=colbert_config,
        use_multimodal=True,
        multimodal_weight=0.3,
        max_depth=5,  # ãƒ„ãƒªãƒ¼æ·±åº¦ã‚’5ã«è¨­å®šï¼ˆæ§‹é€ ææ–™â†’éƒ¨æâ†’æå‚·â†’åŸå› â†’è£œä¿®å·¥æ³•ï¼‰
        min_clusters=2,  # k-meansã®æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿æ•°
        max_clusters=6,  # k-meansã®æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿æ•°ï¼ˆ2-6ã®ç¯„å›²ã§è©•ä¾¡ã€å®‰å®šã—ãŸæŒ‡æ¨™ï¼‰
        selection_strategy='combined',  # è¤‡æ•°æŒ‡æ¨™ã®çµ„ã¿åˆã‚ã›
        metric_weights={
            'silhouette': 0.5,  # ã‚¯ãƒ©ã‚¹ã‚¿å“è³ªï¼ˆãƒŸã‚¯ãƒ­è¦–ç‚¹ï¼‰- ãƒãƒ©ãƒ³ã‚¹
            'dbi': 0.5,         # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†é›¢åº¦ï¼ˆãƒã‚¯ãƒ­è¦–ç‚¹ï¼‰- ãƒãƒ©ãƒ³ã‚¹
            'chi': 0.0          # CHIé™¤å¤–ï¼ˆk=2ãƒã‚¤ã‚¢ã‚¹å›é¿ï¼‰
        }
    )
    
    init_time = time.time() - init_start
    print(f"âœ… åˆæœŸåŒ–å®Œäº† ({init_time:.1f}ç§’)")
    
    # GPUçŠ¶æ…‹è¨˜éŒ²
    gpu_before = get_gpu_memory_usage()
    print(f"GPUçŠ¶æ…‹ (é–‹å§‹å‰): {gpu_before['memory_used_mb']:.0f}MB / {gpu_before['memory_total_mb']:.0f}MB")
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
    log_file = results_dir / f"scaling_test_log_{sample_size}chunks_{test_timestamp}.txt"
    
    # ãƒ„ãƒªãƒ¼æ§‹ç¯‰ï¼ˆãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚å‡ºåŠ›ï¼‰
    print(f"\n[æ§‹ç¯‰] RAPTOR Treeæ§‹ç¯‰ä¸­...")
    print(f"  ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file.name}")
    build_start = time.time()
    
    # æ¨™æº–å‡ºåŠ›ã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚æ›¸ãè¾¼ã‚€
    import io
    log_buffer = io.StringIO()
    
    class TeeOutput:
        def __init__(self, *files):
            self.files = files
        def write(self, data):
            for f in self.files:
                f.write(data)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()
    
    try:
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ¼ãƒ—ãƒ³
        with open(log_file, 'w', encoding='utf-8') as log_f:
            # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
            log_f.write("="*80 + "\n")
            log_f.write(f"RAPTOR Treeæ§‹ç¯‰ãƒ­ã‚° - {sample_size}ãƒãƒ£ãƒ³ã‚¯\n")
            log_f.write(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {test_timestamp}\n")
            log_f.write("="*80 + "\n\n")
            log_f.write(f"ãƒ†ã‚¹ãƒˆ {test_num}/4: {sample_size}ãƒãƒ£ãƒ³ã‚¯\n")
            log_f.write(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–¹æ³•: {'ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°' if sample_size < len(visual_documents) else 'å…¨ãƒ‡ãƒ¼ã‚¿'}\n")
            log_f.write(f"GPUçŠ¶æ…‹ (é–‹å§‹å‰): {gpu_before['memory_used_mb']:.0f}MB / {gpu_before['memory_total_mb']:.0f}MB\n")
            log_f.write(f"åˆæœŸåŒ–æ™‚é–“: {init_time:.1f}ç§’\n")
            log_f.write("\n" + "-"*80 + "\n")
            log_f.write("ãƒ„ãƒªãƒ¼æ§‹ç¯‰é–‹å§‹\n")
            log_f.write("-"*80 + "\n\n")
            log_f.flush()
            
            # å…ƒã®æ¨™æº–å‡ºåŠ›ã‚’ä¿å­˜
            original_stdout = sys.stdout
            original_stderr = sys.stderr
            
            # æ¨™æº–å‡ºåŠ›ã‚’TeeOutputã«åˆ‡ã‚Šæ›¿ãˆï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¸¡æ–¹ã«å‡ºåŠ›ï¼‰
            sys.stdout = TeeOutput(original_stdout, log_f)
            sys.stderr = TeeOutput(original_stderr, log_f)
            
            try:
                tree = colbert_system.build_tree(sampled_docs)
                build_time = time.time() - build_start
            finally:
                # æ¨™æº–å‡ºåŠ›ã‚’å…ƒã«æˆ»ã™
                sys.stdout = original_stdout
                sys.stderr = original_stderr
            
            # æ§‹ç¯‰å®Œäº†æƒ…å ±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            log_f.write("\n" + "-"*80 + "\n")
            log_f.write("ãƒ„ãƒªãƒ¼æ§‹ç¯‰å®Œäº†\n")
            log_f.write("-"*80 + "\n")
        
        # GPUçŠ¶æ…‹è¨˜éŒ²
        gpu_after = get_gpu_memory_usage()
        gpu_peak = max(gpu_before['memory_used_mb'], gpu_after['memory_used_mb'])
        
        # ãƒ„ãƒªãƒ¼çµ±è¨ˆ
        tree_stats = count_tree_nodes(tree)
        
        # çµæœè¨˜éŒ²
        result = {
            'test_number': test_num,
            'sample_size': sample_size,
            'init_time': init_time,
            'build_time': build_time,
            'total_time': init_time + build_time,
            'gpu_memory_before_mb': gpu_before['memory_used_mb'],
            'gpu_memory_after_mb': gpu_after['memory_used_mb'],
            'gpu_memory_peak_mb': gpu_peak,
            'tree_depth': tree_stats['max_depth'],
            'total_nodes': tree_stats['total_nodes'],
            'leaf_nodes': tree_stats['num_leaf_nodes'],
            'internal_nodes': tree_stats['num_internal_nodes'],
            'success': True,
            'log_file': str(log_file)
        }
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆã‚’è¿½åŠ 
        if hasattr(colbert_system, 'stats'):
            stats = colbert_system.stats
            if 'silhouette_scores' in stats and stats['silhouette_scores']:
                result['avg_silhouette'] = np.mean(stats['silhouette_scores'])
                result['avg_dbi'] = np.mean(stats['dbi_scores'])
                result['avg_chi'] = np.mean(stats['chi_scores'])
        
        results.append(result)
        
        # ãƒ„ãƒªãƒ¼ã‚’pickleã§ä¿å­˜
        tree_file = results_dir / f"scaling_test_tree_{sample_size}chunks_{test_timestamp}.pkl"
        try:
            import pickle
            with open(tree_file, 'wb') as f:
                pickle.dump({
                    'tree': tree,
                    'sample_size': sample_size,
                    'build_time': build_time,
                    'stats': tree_stats,
                    'timestamp': test_timestamp
                }, f)
            print(f"  ğŸ’¾ ãƒ„ãƒªãƒ¼ä¿å­˜: {tree_file.name}")
            result['tree_file'] = str(tree_file)
        except Exception as e:
            print(f"  âš ï¸ ãƒ„ãƒªãƒ¼ä¿å­˜å¤±æ•—: {e}")
        
        # çµæœã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚è¨˜éŒ²
        with open(log_file, 'a', encoding='utf-8') as log_f:
            log_f.write(f"\næ§‹ç¯‰æ™‚é–“: {build_time:.1f}ç§’ ({build_time/60:.1f}åˆ†)\n")
            log_f.write(f"GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:\n")
            log_f.write(f"  é–‹å§‹å‰: {gpu_before['memory_used_mb']:.0f}MB\n")
            log_f.write(f"  å®Œäº†å¾Œ: {gpu_after['memory_used_mb']:.0f}MB\n")
            log_f.write(f"  ãƒ”ãƒ¼ã‚¯: {gpu_peak:.0f}MB\n")
            log_f.write(f"ãƒ„ãƒªãƒ¼çµ±è¨ˆ:\n")
            log_f.write(f"  æ·±åº¦: {tree_stats['max_depth']}\n")
            log_f.write(f"  ç·ãƒãƒ¼ãƒ‰æ•°: {tree_stats['total_nodes']}\n")
            log_f.write(f"  ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰: {tree_stats['num_leaf_nodes']}\n")
            log_f.write(f"  å†…éƒ¨ãƒãƒ¼ãƒ‰: {tree_stats['num_internal_nodes']}\n")
            if 'avg_silhouette' in result:
                log_f.write(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ª:\n")
                log_f.write(f"  å¹³å‡Silhouette: {result['avg_silhouette']:.3f}\n")
                log_f.write(f"  å¹³å‡DBI: {result['avg_dbi']:.3f}\n")
                log_f.write(f"  å¹³å‡CHI: {result['avg_chi']:.1f}\n")
            log_f.write("\n" + "="*80 + "\n")
            log_f.write("ãƒ†ã‚¹ãƒˆå®Œäº†\n")
            log_f.write("="*80 + "\n")
        
        print(f"\nâœ… ãƒ†ã‚¹ãƒˆ {test_num} å®Œäº†")
        print(f"  æ§‹ç¯‰æ™‚é–“: {build_time:.1f}ç§’ ({build_time/60:.1f}åˆ†)")
        print(f"  GPU ãƒ¡ãƒ¢ãƒªãƒ”ãƒ¼ã‚¯: {gpu_peak:.0f}MB")
        print(f"  ãƒ„ãƒªãƒ¼æ·±åº¦: {tree_stats['max_depth']}")
        print(f"  ç·ãƒãƒ¼ãƒ‰æ•°: {tree_stats['total_nodes']}")
        print(f"  ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰: {tree_stats['num_leaf_nodes']}")
        print(f"  å†…éƒ¨ãƒãƒ¼ãƒ‰: {tree_stats['num_internal_nodes']}")
        
        if 'avg_silhouette' in result:
            print(f"  å¹³å‡Silhouette: {result['avg_silhouette']:.3f}")
        print(f"  ğŸ“„ ãƒ­ã‚°ä¿å­˜: {log_file.name}")
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆ {test_num} å¤±æ•—: {e}")
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’è¨˜éŒ²
        import traceback
        error_detail = traceback.format_exc()
        
        with open(log_file, 'a', encoding='utf-8') as log_f:
            log_f.write("\n" + "="*80 + "\n")
            log_f.write("âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ\n")
            log_f.write("="*80 + "\n")
            log_f.write(f"ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}\n\n")
            log_f.write("è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n")
            log_f.write(error_detail)
            log_f.write("\n" + "="*80 + "\n")
        
        result = {
            'test_number': test_num,
            'sample_size': sample_size,
            'success': False,
            'error': str(e),
            'log_file': str(log_file)
        }
        results.append(result)
        print(f"  ğŸ“„ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ä¿å­˜: {log_file.name}")
    
    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    del colbert_system
    import gc
    gc.collect()
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print(f"  ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

# çµæœä¿å­˜
print(f"\n[ã‚¹ãƒ†ãƒƒãƒ— 4/6] çµæœã‚’JSONä¿å­˜ä¸­...")
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
results_file = results_dir / f"scaling_test_{timestamp}.json"

with open(results_file, 'w', encoding='utf-8') as f:
    json.dump({
        'timestamp': timestamp,
        'test_description': 'RAPTOR Tree Scaling Test',
        'sample_sizes': sample_sizes,
        'results': results
    }, f, indent=2, ensure_ascii=False)

print(f"âœ… çµæœä¿å­˜: {results_file}")

# ã‚°ãƒ©ãƒ•ç”Ÿæˆ
print(f"\n[ã‚¹ãƒ†ãƒƒãƒ— 5/6] ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­...")

successful_results = [r for r in results if r.get('success', False)]

if len(successful_results) > 0:
    # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    sizes = [r['sample_size'] for r in successful_results]
    build_times = [r['build_time'] for r in successful_results]
    gpu_peaks = [r['gpu_memory_peak_mb'] for r in successful_results]
    tree_depths = [r['tree_depth'] for r in successful_results]
    total_nodes = [r['total_nodes'] for r in successful_results]
    
    # 4ã¤ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('RAPTOR Treeæ§‹ç¯‰ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆçµæœ', fontsize=16, fontweight='bold')
    
    # 1. æ§‹ç¯‰æ™‚é–“
    ax1 = axes[0, 0]
    ax1.plot(sizes, build_times, marker='o', linewidth=2, markersize=8, color='#2E86AB')
    ax1.set_xlabel('ãƒãƒ£ãƒ³ã‚¯æ•°', fontsize=11)
    ax1.set_ylabel('æ§‹ç¯‰æ™‚é–“ (ç§’)', fontsize=11)
    ax1.set_title('1. æ§‹ç¯‰æ™‚é–“ã®æ¨ç§»', fontsize=12, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    for i, (x, y) in enumerate(zip(sizes, build_times)):
        ax1.annotate(f'{y:.1f}ç§’\n({y/60:.1f}åˆ†)', 
                    xy=(x, y), xytext=(0, 10), 
                    textcoords='offset points', ha='center', fontsize=9)
    
    # 2. GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    ax2 = axes[0, 1]
    ax2.plot(sizes, gpu_peaks, marker='s', linewidth=2, markersize=8, color='#A23B72')
    ax2.set_xlabel('ãƒãƒ£ãƒ³ã‚¯æ•°', fontsize=11)
    ax2.set_ylabel('GPU ãƒ¡ãƒ¢ãƒªãƒ”ãƒ¼ã‚¯ (MB)', fontsize=11)
    ax2.set_title('2. GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨ç§»', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    for i, (x, y) in enumerate(zip(sizes, gpu_peaks)):
        ax2.annotate(f'{y:.0f}MB', 
                    xy=(x, y), xytext=(0, 10), 
                    textcoords='offset points', ha='center', fontsize=9)
    
    # 3. ãƒ„ãƒªãƒ¼æ·±åº¦
    ax3 = axes[1, 0]
    ax3.plot(sizes, tree_depths, marker='^', linewidth=2, markersize=8, color='#F18F01')
    ax3.set_xlabel('ãƒãƒ£ãƒ³ã‚¯æ•°', fontsize=11)
    ax3.set_ylabel('ãƒ„ãƒªãƒ¼æ·±åº¦', fontsize=11)
    ax3.set_title('3. ãƒ„ãƒªãƒ¼æ·±åº¦ã®æ¨ç§»', fontsize=12, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim(0, max(tree_depths) + 1)
    for i, (x, y) in enumerate(zip(sizes, tree_depths)):
        ax3.annotate(f'{y}', 
                    xy=(x, y), xytext=(0, 10), 
                    textcoords='offset points', ha='center', fontsize=9)
    
    # 4. ç·ãƒãƒ¼ãƒ‰æ•°
    ax4 = axes[1, 1]
    ax4.plot(sizes, total_nodes, marker='D', linewidth=2, markersize=8, color='#6A994E')
    ax4.set_xlabel('ãƒãƒ£ãƒ³ã‚¯æ•°', fontsize=11)
    ax4.set_ylabel('ç·ãƒãƒ¼ãƒ‰æ•°', fontsize=11)
    ax4.set_title('4. ç·ãƒãƒ¼ãƒ‰æ•°ã®æ¨ç§»', fontsize=12, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    for i, (x, y) in enumerate(zip(sizes, total_nodes)):
        ax4.annotate(f'{y}', 
                    xy=(x, y), xytext=(0, 10), 
                    textcoords='offset points', ha='center', fontsize=9)
    
    plt.tight_layout()
    
    # ã‚°ãƒ©ãƒ•ä¿å­˜
    graph_file = results_dir / f"scaling_test_graph_{timestamp}.png"
    plt.savefig(graph_file, dpi=300, bbox_inches='tight')
    print(f"âœ… ã‚°ãƒ©ãƒ•ä¿å­˜: {graph_file}")
    
    # è¿½åŠ ã‚°ãƒ©ãƒ•: åŠ¹ç‡æ€§åˆ†æ
    fig2, axes2 = plt.subplots(1, 2, figsize=(14, 5))
    fig2.suptitle('åŠ¹ç‡æ€§åˆ†æ', fontsize=16, fontweight='bold')
    
    # ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®å‡¦ç†æ™‚é–“
    ax5 = axes2[0]
    time_per_chunk = [t/s for t, s in zip(build_times, sizes)]
    ax5.plot(sizes, time_per_chunk, marker='o', linewidth=2, markersize=8, color='#BC4749')
    ax5.set_xlabel('ãƒãƒ£ãƒ³ã‚¯æ•°', fontsize=11)
    ax5.set_ylabel('ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šå‡¦ç†æ™‚é–“ (ç§’)', fontsize=11)
    ax5.set_title('5. ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šå‡¦ç†æ™‚é–“', fontsize=12, fontweight='bold')
    ax5.grid(True, alpha=0.3)
    for i, (x, y) in enumerate(zip(sizes, time_per_chunk)):
        ax5.annotate(f'{y:.3f}ç§’', 
                    xy=(x, y), xytext=(0, 10), 
                    textcoords='offset points', ha='center', fontsize=9)
    
    # ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šã®å‡¦ç†æ™‚é–“
    ax6 = axes2[1]
    time_per_node = [t/n if n > 0 else 0 for t, n in zip(build_times, total_nodes)]
    ax6.plot(sizes, time_per_node, marker='s', linewidth=2, markersize=8, color='#386641')
    ax6.set_xlabel('ãƒãƒ£ãƒ³ã‚¯æ•°', fontsize=11)
    ax6.set_ylabel('ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šå‡¦ç†æ™‚é–“ (ç§’)', fontsize=11)
    ax6.set_title('6. ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šå‡¦ç†æ™‚é–“', fontsize=12, fontweight='bold')
    ax6.grid(True, alpha=0.3)
    for i, (x, y) in enumerate(zip(sizes, time_per_node)):
        ax6.annotate(f'{y:.2f}ç§’', 
                    xy=(x, y), xytext=(0, 10), 
                    textcoords='offset points', ha='center', fontsize=9)
    
    plt.tight_layout()
    graph_file2 = results_dir / f"scaling_test_efficiency_{timestamp}.png"
    plt.savefig(graph_file2, dpi=300, bbox_inches='tight')
    print(f"âœ… åŠ¹ç‡æ€§ã‚°ãƒ©ãƒ•ä¿å­˜: {graph_file2}")

# çŸ¥è¦‹ã¾ã¨ã‚
print(f"\n[ã‚¹ãƒ†ãƒƒãƒ— 6/6] çŸ¥è¦‹ã®ã¾ã¨ã‚")
print("=" * 80)

if len(successful_results) > 0:
    print("\nğŸ“Š ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰¹æ€§:")
    
    # æ™‚é–“ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    if len(successful_results) >= 2:
        time_ratio = build_times[-1] / build_times[0]
        size_ratio = sizes[-1] / sizes[0]
        print(f"  ãƒ»æ§‹ç¯‰æ™‚é–“ã®å¢—åŠ ç‡: {time_ratio:.2f}x (ãƒ‡ãƒ¼ã‚¿é‡ {size_ratio:.2f}x)")
        print(f"    â†’ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°: O(n^{np.log(time_ratio)/np.log(size_ratio):.2f})")
    
    # ãƒ¡ãƒ¢ãƒªã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    if len(successful_results) >= 2:
        mem_increase = gpu_peaks[-1] - gpu_peaks[0]
        print(f"  ãƒ»GPU ãƒ¡ãƒ¢ãƒªå¢—åŠ é‡: +{mem_increase:.0f}MB ({gpu_peaks[0]:.0f}MB â†’ {gpu_peaks[-1]:.0f}MB)")
    
    # ãƒ„ãƒªãƒ¼æ§‹é€ ã®å¤‰åŒ–
    print(f"\nğŸŒ³ ãƒ„ãƒªãƒ¼æ§‹é€ ã®æ¨ç§»:")
    for r in successful_results:
        print(f"  ãƒ»{r['sample_size']:4d}ãƒãƒ£ãƒ³ã‚¯: æ·±åº¦{r['tree_depth']}, ãƒãƒ¼ãƒ‰{r['total_nodes']:3d}å€‹ "
              f"(ãƒªãƒ¼ãƒ•{r['leaf_nodes']:3d}, å†…éƒ¨{r['internal_nodes']:2d})")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ª
    if any('avg_silhouette' in r for r in successful_results):
        print(f"\nğŸ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ª:")
        for r in successful_results:
            if 'avg_silhouette' in r:
                print(f"  ãƒ»{r['sample_size']:4d}ãƒãƒ£ãƒ³ã‚¯: Silhouette={r['avg_silhouette']:.3f}, "
                      f"DBI={r['avg_dbi']:.3f}, CHI={r['avg_chi']:.1f}")
    
    # å®Ÿç”¨æ€§è©•ä¾¡
    print(f"\nğŸ’¡ å®Ÿç”¨æ€§è©•ä¾¡:")
    for r in successful_results:
        time_min = r['build_time'] / 60
        if time_min < 5:
            rating = "â­â­â­ é«˜é€Ÿ"
        elif time_min < 15:
            rating = "â­â­ å®Ÿç”¨çš„"
        else:
            rating = "â­ è¦æ¤œè¨"
        print(f"  ãƒ»{r['sample_size']:4d}ãƒãƒ£ãƒ³ã‚¯: {time_min:.1f}åˆ† {rating}")

print("\n" + "=" * 80)
print("âœ… ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Œäº†")
print("=" * 80)
