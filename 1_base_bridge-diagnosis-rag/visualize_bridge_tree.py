"""
æ©‹æ¢è¨ºæ–­RAPTOR Treeå¯è¦–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
NetworkXã¨Matplotlibã‚’ä½¿ç”¨ã—ã¦ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’å¯è¦–åŒ–

ä¿å­˜ã•ã‚ŒãŸscaling_test_tree_*.pklãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰RAPTOR Treeã‚’èª­ã¿è¾¼ã¿ã€
éšå±¤æ§‹é€ ã‚’è¦–è¦šåŒ–ã—ã¦PNG/PDFã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚
"""

import os
import sys
import pickle
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib
from pathlib import Path
from typing import Dict, List, Tuple, Any
import numpy as np
from datetime import datetime
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer

# å½¢æ…‹ç´ è§£æ
try:
    from fugashi import Tagger, GenericTagger
    FUGASHI_AVAILABLE = True
except ImportError:
    FUGASHI_AVAILABLE = False
    print("âš ï¸ fugashiãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å½¢æ…‹ç´ è§£æãƒ©ãƒ™ãƒ«ã¯TF-IDFã®ã¿ä½¿ç”¨ã•ã‚Œã¾ã™")

# æ©‹æ¢ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™
from bridge_diagnosis_vocab import (
    filter_bridge_keywords, 
    is_bridge_keyword, 
    is_stop_word,
    STOP_WORDS,
    BRIDGE_DOMAIN_KEYWORDS, 
    BRIDGE_TRANSLATION_DICT,
    get_priority_keywords_by_depth,
)

# pykakasiã®ã‚°ãƒ­ãƒ¼ãƒãƒ«åˆæœŸåŒ–ï¼ˆ1å›ã®ã¿ï¼‰
_KAKASI_INSTANCE = None

def get_kakasi():
    """pykakasiã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _KAKASI_INSTANCE
    if _KAKASI_INSTANCE is None:
        try:
            import pykakasi
            _KAKASI_INSTANCE = pykakasi.kakasi()
        except ImportError:
            _KAKASI_INSTANCE = False  # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—ã‚’ãƒãƒ¼ã‚¯
    return _KAKASI_INSTANCE

def translate_keyword(keyword: str) -> str:
    """
    æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‹±èªã«ç¿»è¨³ï¼ˆè¾æ›¸ãƒ™ãƒ¼ã‚¹ + ãƒ­ãƒ¼ãƒå­—åŒ–ï¼‰
    
    1. æ—¢ã«ASCIIãªã‚‰ãã®ã¾ã¾è¿”ã™
    2. ç¿»è¨³è¾æ›¸ã‹ã‚‰æ¤œç´¢
    3. pykakasiã§ãƒ­ãƒ¼ãƒå­—åŒ–
    4. å¤±æ•—æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    # æ—¢ã«è‹±æ•°å­—ã®ã¿ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    if keyword.isascii():
        return keyword
    
    # æ©‹æ¢è¨ºæ–­è¾æ›¸ã‹ã‚‰ç¿»è¨³ã‚’å–å¾—
    translated = BRIDGE_TRANSLATION_DICT.get(keyword)
    if translated:
        return translated
    
    # pykakasiã§ãƒ­ãƒ¼ãƒå­—åŒ–ã‚’è©¦è¡Œ
    kks = get_kakasi()
    if kks and kks is not False:
        try:
            result = kks.convert(keyword)
            # hepburnã‚­ãƒ¼ã§ãƒ­ãƒ¼ãƒå­—ã‚’å–å¾—ã—ã€å˜èªã”ã¨ã«å…ˆé ­å¤§æ–‡å­—åŒ–
            romaji_parts = []
            for item in result:
                if 'hepburn' in item and item['hepburn']:
                    romaji_parts.append(item['hepburn'].capitalize())
            
            if romaji_parts:
                return ' '.join(romaji_parts)
        except Exception as e:
            print(f"âš ï¸ pykakasiå¤‰æ›ã‚¨ãƒ©ãƒ¼ ({keyword}): {e}")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã‚’"[JA]"ã«ç½®æ›
    return f"[JA:{keyword[:3]}]"


def extract_keywords_from_text(text: str, top_n: int = 5, depth: int = 0) -> List[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆæ©‹æ¢è¨ºæ–­ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ï¼‰
    
    æ·±ã•ã«å¿œã˜ã¦å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªã‚’å¤‰æ›´ï¼š
    - depth=0: ROOTï¼ˆå…¨ã‚«ãƒ†ã‚´ãƒªï¼‰
    - depth=1: éƒ¨æï¼ˆåºŠç‰ˆã€æ¡ã€æ”¯æ‰¿ãªã©ï¼‰
    - depth=2: æå‚·ï¼ˆè…é£Ÿã€ã²ã³å‰²ã‚Œã€ç–²åŠ´ãªã©ï¼‰
    - depth=3: åŸå› ï¼ˆå¡©å®³ã€å‡å®³ã€è·é‡ãªã©ï¼‰
    - depth>=4: è£œä¿®å·¥æ³•ï¼ˆè£œä¿®ã€è£œå¼·ã€è¡¨é¢è¢«è¦†ãªã©ï¼‰
    
    Args:
        text: æŠ½å‡ºå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        top_n: æŠ½å‡ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°
        depth: ãƒãƒ¼ãƒ‰ã®æ·±ã•ï¼ˆéšå±¤ãƒ¬ãƒ™ãƒ«ï¼‰
    """
    keywords = []
    
    # æ·±ã•ã«å¿œã˜ãŸå„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆã‚’å–å¾—
    priority_keywords = get_priority_keywords_by_depth(depth)
    
    # 1. å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æŠ½å‡º
    if priority_keywords:
        priority_found = []
        for keyword in priority_keywords:
            if keyword in text and not is_stop_word(keyword):
                priority_found.append(keyword)
        
        # å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…ˆé ­ã«è¿½åŠ 
        keywords.extend(priority_found[:top_n])
    
    # 2. ã¾ã ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã€ä¸€èˆ¬çš„ãªæ©‹æ¢ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
    if len(keywords) < top_n:
        domain_keywords = filter_bridge_keywords(text)
        for kw in domain_keywords:
            if kw not in keywords:
                keywords.append(kw)
            if len(keywords) >= top_n:
                break
    
    # 2. å½¢æ…‹ç´ è§£æã§åè©ã‚’æŠ½å‡ºï¼ˆfugashiåˆ©ç”¨å¯èƒ½æ™‚ï¼‰
    if FUGASHI_AVAILABLE and len(keywords) < top_n:
        try:
            # MeCabã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
            mecabrc_paths = [
                r'C:\Program Files\MeCab\etc\mecabrc',
                r'C:\Program Files (x86)\MeCab\etc\mecabrc',
                r'C:\mecab\etc\mecabrc'
            ]
            
            for mecabrc_path in mecabrc_paths:
                if os.path.exists(mecabrc_path):
                    os.environ['MECABRC'] = mecabrc_path
                    break
            
            # MeCabã®è¾æ›¸ãƒ‘ã‚¹è¨­å®š
            dicdir = None
            
            # 1. unidic-liteã‚’è©¦è¡Œ
            try:
                import unidic_lite
                dicdir = unidic_lite.DICDIR
            except ImportError:
                pass
            
            # 2. ã‚·ã‚¹ãƒ†ãƒ MeCabã‚’è©¦è¡Œ
            if dicdir is None:
                mecab_dic_paths = [
                    r'C:\Program Files\MeCab\dic\ipadic',
                    r'C:\Program Files (x86)\MeCab\dic\ipadic',
                    r'C:\mecab\dic\ipadic'
                ]
                for path in mecab_dic_paths:
                    if os.path.exists(path):
                        dicdir = path
                        break
            
            # TaggeråˆæœŸåŒ–
            if dicdir:
                if 'ipadic' in dicdir.lower():
                    tagger = GenericTagger(f'-d "{dicdir}"')
                else:
                    tagger = Tagger(f'-d "{dicdir}"')
            else:
                try:
                    tagger = Tagger()
                except:
                    tagger = GenericTagger()
            
            words = []
            for word in tagger(text):
                # åè©ã®ã¿æŠ½å‡ºï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–ï¼‰
                # GenericTaggerã¯featureãŒã‚¿ãƒ—ãƒ«ã€Taggerã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
                try:
                    # Taggerå½¢å¼ (UniDic)
                    pos = word.feature.pos1
                except AttributeError:
                    # GenericTaggerå½¢å¼ (IPADIC) - ã‚¿ãƒ—ãƒ«[0]ãŒå“è©
                    pos = word.feature[0] if isinstance(word.feature, tuple) else None
                
                if pos == 'åè©' and len(word.surface) > 1:
                    if not is_stop_word(word.surface):
                        words.append(word.surface)
            
            # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
            word_counts = Counter(words)
            most_common = [w for w, c in word_counts.most_common(top_n * 2)]
            
            # ã¾ã å«ã¾ã‚Œã¦ã„ãªã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
            for word in most_common:
                if word not in keywords:
                    keywords.append(word)
                if len(keywords) >= top_n:
                    break
        except Exception as e:
            print(f"âš ï¸ å½¢æ…‹ç´ è§£æã‚¨ãƒ©ãƒ¼: {e}")
    
    # 3. TF-IDFã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã¾ã ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆï¼‰
    if len(keywords) < top_n:
        # ç°¡æ˜“çš„ãªå˜èªåˆ†å‰²ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãƒ»å¥èª­ç‚¹ã§åˆ†å‰²ï¼‰
        words = re.findall(r'\w+', text)
        # 2æ–‡å­—ä»¥ä¸Šã®å˜èªã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–ï¼‰
        word_counts = Counter([w for w in words if len(w) >= 2 and not is_stop_word(w)])
        most_common = [w for w, c in word_counts.most_common(top_n * 2)]
        
        for word in most_common:
            if word not in keywords:
                keywords.append(word)
            if len(keywords) >= top_n:
                break
    
    return keywords[:top_n]


def load_raptor_tree(pkl_file: str) -> Dict[str, Any]:
    """
    .pklãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰RAPTOR Treeã‚’èª­ã¿è¾¼ã¿
    
    æˆ»ã‚Šå€¤:
        tree_data: {
            'tree': {
                'summaries': List[Dict],  # ã‚µãƒãƒªãƒ¼ãƒãƒ¼ãƒ‰æƒ…å ±
                'clusters': Dict,         # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±
                'documents': int,         # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
                'depth': int,             # ãƒ„ãƒªãƒ¼ã®æ·±ã•
            },
            'stats': Dict,  # çµ±è¨ˆæƒ…å ±
            ...
        }
    """
    print(f"\nğŸ“‚ Loading RAPTOR Tree from: {pkl_file}")
    
    if not os.path.exists(pkl_file):
        raise FileNotFoundError(f"Tree file not found: {pkl_file}")
    
    with open(pkl_file, 'rb') as f:
        tree_data = pickle.load(f)
    
    # åŸºæœ¬çµ±è¨ˆ
    tree = tree_data.get('tree', {})
    summaries = tree.get('summaries', [])
    stats = tree_data.get('stats', {})
    
    print(f"âœ… Loaded tree data")
    print(f"   Depth: {tree.get('depth', 0)}")
    print(f"   Documents: {tree.get('documents', 0)}")
    print(f"   Summaries: {len(summaries)}")
    print(f"   Stats: {stats}")
    
    return tree_data


def build_graph_from_tree(tree_data: Dict[str, Any], max_depth: int = None) -> Tuple[nx.DiGraph, Dict]:
    """
    RAPTOR Treeã‹ã‚‰NetworkXã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
    
    Args:
        tree_data: ãƒ„ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿
        max_depth: å¯è¦–åŒ–ã™ã‚‹æœ€å¤§æ·±åº¦ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
    
    visualize_raptor_tree.pyã®å®Ÿè£…ã‚’å‚è€ƒã«ã€
    clustersã‚’å†å¸°çš„ã«ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆã—ã¦ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
    """
    stats = tree_data.get('stats', {})
    num_leaf_nodes = stats.get('num_leaf_nodes', 0)
    num_internal_nodes = stats.get('num_internal_nodes', 0)
    tree_max_depth = stats.get('max_depth', 3)
    
    # max_depthãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ„ãƒªãƒ¼ã®æœ€å¤§æ·±åº¦ã‚’ä½¿ç”¨
    if max_depth is None:
        max_depth = tree_max_depth
    
    G = nx.DiGraph()
    node_info = {}
    node_counter = 0
    
    print("\nğŸ”¨ Building NetworkX graph...")
    print(f"   Target: {num_internal_nodes} internal + {num_leaf_nodes} leaf = {num_internal_nodes + num_leaf_nodes} nodes")
    print(f"   Tree max depth: {tree_max_depth}")
    print(f"   Visualizing up to depth: {max_depth}")
    
    def get_node_id():
        """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒãƒ¼ãƒ‰IDã‚’ç”Ÿæˆ"""
        nonlocal node_counter
        node_counter += 1
        return f"node_{node_counter}"
    
    def process_clusters(tree_dict, depth=0, parent_id=None):
        """
        å†å¸°çš„ã«clustersã‚’å‡¦ç†ã—ã¦ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’ä½œæˆ
        
        é‡è¦: ã™ã¹ã¦ã®depthã§ã€clustersã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆã—ã¦ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
        """
        # æ·±åº¦åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if max_depth is not None and depth > max_depth:
            return
        
        if not isinstance(tree_dict, dict):
            return
        
        # clustersã‚’å‡¦ç†
        if 'clusters' not in tree_dict or not tree_dict['clusters']:
            return
        
        clusters = tree_dict['clusters']
        
        for cluster_id, cluster_data in clusters.items():
            # ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
            summary_text = ""
            if 'summary' in cluster_data:
                summary = cluster_data['summary']
                if hasattr(summary, 'page_content'):
                    summary_text = summary.page_content
                elif isinstance(summary, dict):
                    summary_text = summary.get('page_content', summary.get('text', ''))
            
            # å­ã®æœ‰ç„¡åˆ¤å®š (visualize_raptor_tree.pyã¨åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³)
            has_children = ('children' in cluster_data and 
                          cluster_data['children'] and 
                          'clusters' in cluster_data['children'] and
                          cluster_data['children']['clusters'])
            
            # ãƒãƒ¼ãƒ‰IDç”Ÿæˆ
            node_id = get_node_id()
            layer = tree_max_depth - depth
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆæ·±ã•ã«å¿œã˜ãŸå„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ï¼‰
            keywords = extract_keywords_from_text(summary_text, top_n=3, depth=depth)
            translated = [translate_keyword(kw) for kw in keywords]
            # 2ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
            if len(translated) >= 2:
                label_en = f"{translated[0]}\n{translated[1]}"
            else:
                label_en = translated[0] if translated else f"C{cluster_id}"
            
            if len(keywords) >= 2:
                label_ja = f"{keywords[0]}\n{keywords[1]}"
            else:
                label_ja = keywords[0] if keywords else f"C{cluster_id}"
            
            # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—
            node_type = 'internal' if has_children else 'leaf'
            
            # ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’ä¿å­˜
            node_info[node_id] = {
                'label': label_en if label_en else f"C{cluster_id}",
                'label_ja': label_ja if label_ja else f"C{cluster_id}",
                'layer': layer,
                'depth': depth,
                'cluster_id': cluster_id,
                'text': summary_text[:100],
                'keywords': keywords,
                'node_type': node_type,
            }
            
            # ã‚°ãƒ©ãƒ•ã«ãƒãƒ¼ãƒ‰è¿½åŠ 
            G.add_node(node_id, **node_info[node_id])
            
            # è¦ªãƒãƒ¼ãƒ‰ã¨ã®æ¥ç¶š
            if parent_id:
                G.add_edge(parent_id, node_id)
            
            # å­ãƒ„ãƒªãƒ¼ã‚’å†å¸°çš„ã«å‡¦ç†
            if has_children:
                process_clusters(
                    cluster_data['children'],
                    depth + 1,
                    node_id
                )
    
    # ãƒ«ãƒ¼ãƒˆãƒ„ãƒªãƒ¼ã‹ã‚‰å‡¦ç†é–‹å§‹
    root_tree = tree_data.get('tree', {})
    if not isinstance(root_tree, dict):
        print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ„ãƒªãƒ¼ãŒè¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        return G, node_info
    
    # ä»®æƒ³ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ã‚’ä½œæˆ (statsã§1å€‹ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ã‚‹ãŸã‚)
    root_node_id = get_node_id()
    layer = tree_max_depth
    
    node_info[root_node_id] = {
        'label': 'ROOT',
        'layer': layer,
        'depth': 0,
        'cluster_id': 'root',
        'text': 'Root of tree',
        'keywords': [],
        'node_type': 'internal',
    }
    
    G.add_node(root_node_id, **node_info[root_node_id])
    
    # ãƒ«ãƒ¼ãƒˆãƒ„ãƒªãƒ¼ã®clustersã‚’å‡¦ç† (rootãƒãƒ¼ãƒ‰ã®å­ã¨ã—ã¦)
    if 'clusters' in root_tree and root_tree['clusters']:
        clusters = root_tree['clusters']
        
        for cluster_id, cluster_data in clusters.items():
            # ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
            summary_text = ""
            if 'summary' in cluster_data:
                summary = cluster_data['summary']
                if hasattr(summary, 'page_content'):
                    summary_text = summary.page_content
                elif isinstance(summary, dict):
                    summary_text = summary.get('page_content', summary.get('text', ''))
            
            # å­ã®æœ‰ç„¡åˆ¤å®š
            has_children = ('children' in cluster_data and 
                          cluster_data['children'] and 
                          'clusters' in cluster_data['children'] and
                          cluster_data['children']['clusters'])
            
            # ãƒãƒ¼ãƒ‰IDç”Ÿæˆ
            node_id = get_node_id()
            layer = tree_max_depth - 1  # depth=1ç›¸å½“
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆæ·±ã•=1ãªã®ã§éƒ¨æã‚«ãƒ†ã‚´ãƒªå„ªå…ˆï¼‰
            keywords = extract_keywords_from_text(summary_text, top_n=3, depth=1)
            translated = [translate_keyword(kw) for kw in keywords]
            # 2ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
            if len(translated) >= 2:
                label_en = f"{translated[0]}\n{translated[1]}"
            else:
                label_en = translated[0] if translated else f"C{cluster_id}"
            
            if len(keywords) >= 2:
                label_ja = f"{keywords[0]}\n{keywords[1]}"
            else:
                label_ja = keywords[0] if keywords else f"C{cluster_id}"
            
            # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—
            node_type = 'internal' if has_children else 'leaf'
            
            # ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’ä¿å­˜
            node_info[node_id] = {
                'label': label_en if label_en else f"C{cluster_id}",
                'label_ja': label_ja if label_ja else f"C{cluster_id}",
                'layer': layer,
                'depth': 1,
                'cluster_id': cluster_id,
                'text': summary_text[:100],
                'keywords': keywords,
                'node_type': node_type,
            }
            
            # ã‚°ãƒ©ãƒ•ã«ãƒãƒ¼ãƒ‰è¿½åŠ 
            G.add_node(node_id, **node_info[node_id])
            
            # ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ã¨ã®æ¥ç¶š
            G.add_edge(root_node_id, node_id)
            
            # å­ãƒ„ãƒªãƒ¼ã‚’å†å¸°çš„ã«å‡¦ç†
            if has_children:
                process_clusters(
                    cluster_data['children'],
                    depth=2,  # æ¬¡ã¯depth=2
                    parent_id=node_id
                )
    
    # çµ±è¨ˆæƒ…å ±
    leaf_count = sum(1 for info in node_info.values() if info['node_type'] == 'leaf')
    internal_count = sum(1 for info in node_info.values() if info['node_type'] == 'internal')
    
    # Depthåˆ¥ãƒãƒ¼ãƒ‰æ•°
    from collections import Counter
    depth_counts = Counter(info['depth'] for info in node_info.values())
    
    print(f"âœ… Graph built: {len(G.nodes)} nodes, {len(G.edges)} edges")
    print(f"   Internal nodes: {internal_count}")
    print(f"   Leaf nodes: {leaf_count}")
    print(f"   Layers: {sorted(set(info['layer'] for info in node_info.values()))}")
    print(f"   Nodes by depth: {dict(sorted(depth_counts.items()))}")
    
    return G, node_info


def compute_hierarchical_layout(G: nx.DiGraph, node_info: Dict) -> Dict[int, Tuple[float, float]]:
    """
    éšå±¤ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è¨ˆç®—ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«é…ç½®ï¼‰
    
    æˆ»ã‚Šå€¤:
        pos: ãƒãƒ¼ãƒ‰ID â†’ (x, y) åº§æ¨™ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    """
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ãƒãƒ¼ãƒ‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    layers = {}
    for node_id, info in node_info.items():
        layer = info['layer']
        if layer not in layers:
            layers[layer] = []
        layers[layer].append(node_id)
    
    pos = {}
    max_layer = max(layers.keys())
    
    for layer, nodes in layers.items():
        # Yåº§æ¨™: ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒé«˜ã„ã»ã©ä¸Šï¼ˆrootãŒä¸Šã«ãªã‚‹ã‚ˆã†ã«åè»¢ï¼‰
        y = layer * 4.0  # ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®ç¸¦é–“éš”ã‚’æ‹¡å¤§ï¼ˆ3.0 â†’ 4.0ï¼‰
        
        # Xåº§æ¨™: ãƒãƒ¼ãƒ‰ã‚’ç­‰é–“éš”ã«é…ç½®
        num_nodes = len(nodes)
        for i, node_id in enumerate(sorted(nodes)):
            x = (i - num_nodes / 2) * 5.0  # æ¨ªé–“éš”ã‚’æ‹¡å¤§ï¼ˆ3.5 â†’ 5.0ï¼‰
            pos[node_id] = (x, y)
    
    return pos


def compute_circular_layout(G: nx.DiGraph, node_info: Dict) -> Dict[int, Tuple[float, float]]:
    """
    å††å½¢ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è¨ˆç®—ï¼ˆãƒ«ãƒ¼ãƒˆã‚’ä¸­å¿ƒã«åŒå¿ƒå††çŠ¶ã«é…ç½®ï¼‰
    
    æˆ»ã‚Šå€¤:
        pos: ãƒãƒ¼ãƒ‰ID â†’ (x, y) åº§æ¨™ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    """
    import math
    
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ãƒãƒ¼ãƒ‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    layers = {}
    for node_id, info in node_info.items():
        layer = info['layer']
        if layer not in layers:
            layers[layer] = []
        layers[layer].append(node_id)
    
    pos = {}
    max_layer = max(layers.keys())
    
    for layer, nodes in layers.items():
        num_nodes = len(nodes)
        
        if layer == max_layer:
            # ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ã¯ä¸­å¿ƒã«é…ç½®
            pos[nodes[0]] = (0, 0)
        else:
            # åŒå¿ƒå††ã®åŠå¾„ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒä½ã„ã»ã©å¤–å´ï¼‰
            radius = (max_layer - layer) * 8.0  # å††ã®é–“éš”
            
            # ãƒãƒ¼ãƒ‰ã‚’å††å‘¨ä¸Šã«ç­‰é–“éš”ã«é…ç½®
            for i, node_id in enumerate(sorted(nodes)):
                angle = 2 * math.pi * i / num_nodes
                x = radius * math.cos(angle)
                y = radius * math.sin(angle)
                pos[node_id] = (x, y)
    
    return pos


def visualize_tree(
    G: nx.DiGraph,
    node_info: Dict,
    output_file: str = "raptor_tree_bridge.png",
    title: str = "Bridge Diagnosis RAPTOR Tree",
    figsize: Tuple[int, int] = (20, 12),
    dpi: int = 150
):
    """
    RAPTOR Treeã‚’å¯è¦–åŒ–ã—ã¦ä¿å­˜ï¼ˆæ—¥æœ¬èªç‰ˆã¨è‹±èªç‰ˆã®2ã¤ã‚’ç”Ÿæˆï¼‰
    """
    print(f"\nğŸ¨ Visualizing tree (Japanese and English versions)...")
    
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    matplotlib.rcParams['font.family'] = 'Yu Gothic'
    matplotlib.rcParams['font.sans-serif'] = ['Yu Gothic', 'Meiryo', 'MS Gothic']
    matplotlib.rcParams['axes.unicode_minus'] = False
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—ï¼ˆå…±é€šï¼‰
    pos = compute_hierarchical_layout(G, node_info)
    
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«è‰²åˆ†ã‘ï¼ˆå…±é€šï¼‰
    layers = set(info['layer'] for info in node_info.values())
    max_layer = max(layers)
    colors = plt.cm.viridis(np.linspace(0, 1, max_layer + 1))
    
    # ãƒ©ãƒ™ãƒ«æº–å‚™
    labels_en = {nid: info['label'] for nid, info in node_info.items()}  # è‹±èªãƒ©ãƒ™ãƒ«
    labels_ja = {nid: info.get('label_ja', info['label']) for nid, info in node_info.items()}  # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«
    
    # çµ±è¨ˆæƒ…å ±
    stats_text = f"Nodes: {len(G.nodes)} | Edges: {len(G.edges)} | Depth: {max_layer + 1}"
    
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¡ä¾‹
    legend_elements = [
        plt.Line2D([0], [0], marker='o', color='w',
                  markerfacecolor=colors[layer], markersize=10,
                  label=f'Layer {layer}')
        for layer in sorted(layers)
    ]
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹éƒ¨åˆ†ã‚’å–å¾—
    if output_file.endswith('.png'):
        base_name = output_file[:-4]
    else:
        base_name = output_file
    
    # ========== æ—¥æœ¬èªç‰ˆç”Ÿæˆ ==========
    output_file_ja = f"{base_name}_ja.png"
    print(f"ğŸ’¾ Saving Japanese version to: {output_file_ja}")
    
    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
    
    for layer in sorted(layers):
        layer_nodes = [nid for nid, info in node_info.items() if info['layer'] == layer]
        nx.draw_networkx_nodes(G, pos, nodelist=layer_nodes,
                              node_color=[colors[layer]], node_size=1500, alpha=0.8, ax=ax)  # 2000 â†’ 1500ï¼ˆãƒãƒ¼ãƒ‰ã‚’å°‘ã—å°ã•ãï¼‰
    
    nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True,
                          arrowsize=15, arrowstyle='->', alpha=0.5, ax=ax)
    nx.draw_networkx_labels(G, pos, labels=labels_ja, font_size=9, font_weight='bold', ax=ax)  # 10 â†’ 9ï¼ˆãƒ•ã‚©ãƒ³ãƒˆã‚‚å°‘ã—å°ã•ãï¼‰
    
    ax.set_title(f"{title} (Japanese)", fontsize=16, fontweight='bold', pad=20)
    ax.axis('off')
    ax.legend(handles=legend_elements, loc='upper right', fontsize=10)
    ax.text(0.5, 0.02, stats_text, transform=ax.transAxes,
           fontsize=10, ha='center', va='bottom',
           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig(output_file_ja, dpi=dpi, bbox_inches='tight')
    plt.close()
    print(f"âœ… Japanese PNG saved: {output_file_ja}")
    
    print(f"\nâœ… Visualization complete! Generated file:")
    print(f"   - {output_file_ja}")


def visualize_tree_circular(
    G: nx.DiGraph,
    node_info: Dict,
    output_file: str = "raptor_tree_bridge_circular.png",
    title: str = "Bridge Diagnosis RAPTOR Tree (Circular Layout)",
    figsize: Tuple[int, int] = (24, 24),
    dpi: int = 150
):
    """
    RAPTOR Treeã‚’å††å½¢ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§å¯è¦–åŒ–ã—ã¦ä¿å­˜ï¼ˆæ—¥æœ¬èªç‰ˆï¼‰
    """
    print(f"\nğŸ¨ Visualizing tree with circular layout...")
    
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    matplotlib.rcParams['font.family'] = 'Yu Gothic'
    matplotlib.rcParams['font.sans-serif'] = ['Yu Gothic', 'Meiryo', 'MS Gothic']
    matplotlib.rcParams['axes.unicode_minus'] = False
    
    # å††å½¢ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—
    pos = compute_circular_layout(G, node_info)
    
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«è‰²åˆ†ã‘
    layers = set(info['layer'] for info in node_info.values())
    max_layer = max(layers)
    colors = plt.cm.viridis(np.linspace(0, 1, max_layer + 1))
    
    # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«æº–å‚™
    labels_ja = {nid: info.get('label_ja', info['label']) for nid, info in node_info.items()}
    
    # çµ±è¨ˆæƒ…å ±
    stats_text = f"Nodes: {len(G.nodes)} | Edges: {len(G.edges)} | Depth: {max_layer + 1}"
    
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¡ä¾‹
    legend_elements = [
        plt.Line2D([0], [0], marker='o', color='w',
                  markerfacecolor=colors[layer], markersize=10,
                  label=f'Layer {layer}')
        for layer in sorted(layers)
    ]
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹éƒ¨åˆ†ã‚’å–å¾—
    if output_file.endswith('.png'):
        base_name = output_file[:-4]
    else:
        base_name = output_file
    
    # å††å½¢ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆç‰ˆç”Ÿæˆ
    output_file_circular = f"{base_name}_circular_ja.png"
    print(f"ğŸ’¾ Saving circular layout to: {output_file_circular}")
    
    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
    
    # ãƒãƒ¼ãƒ‰æç”»ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ¥ã«è‰²åˆ†ã‘ï¼‰
    for layer in sorted(layers):
        layer_nodes = [nid for nid, info in node_info.items() if info['layer'] == layer]
        node_size = 2500 if layer == max_layer else 1800  # ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ã¯å¤§ãã
        nx.draw_networkx_nodes(G, pos, nodelist=layer_nodes,
                              node_color=[colors[layer]], node_size=node_size, alpha=0.8, ax=ax)
    
    # ã‚¨ãƒƒã‚¸æç”»
    nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True,
                          arrowsize=15, arrowstyle='->', alpha=0.3, ax=ax, width=1.5)
    
    # ãƒ©ãƒ™ãƒ«æç”»
    nx.draw_networkx_labels(G, pos, labels=labels_ja, font_size=9, font_weight='bold', ax=ax)
    
    ax.set_title(f"{title}", fontsize=18, fontweight='bold', pad=20)
    ax.axis('off')
    ax.legend(handles=legend_elements, loc='upper right', fontsize=11)
    ax.text(0.5, 0.02, stats_text, transform=ax.transAxes,
           fontsize=11, ha='center', va='bottom',
           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig(output_file_circular, dpi=dpi, bbox_inches='tight')
    plt.close()
    print(f"âœ… Circular layout PNG saved: {output_file_circular}")
    
    print(f"\nâœ… Circular visualization complete! Generated file:")
    print(f"   - {output_file_circular}")



def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Visualize Bridge Diagnosis RAPTOR Tree')
    parser.add_argument('--tree_file', type=str, default=None,
                       help='Path to tree .pkl file (default: latest in results/)')
    parser.add_argument('--output', type=str, default=None,
                       help='Output file path (default: auto-generated)')
    parser.add_argument('--title', type=str, default='Bridge Diagnosis RAPTOR Tree',
                       help='Chart title')
    parser.add_argument('--figsize', type=int, nargs=2, default=[40, 30],
                       help='Figure size (width height)')  # 30x20 â†’ 40x30ã«æ‹¡å¤§
    parser.add_argument('--dpi', type=int, default=150,
                       help='Output DPI')
    parser.add_argument('--max_depth', type=int, default=None,
                       help='Maximum depth to visualize (default: all depths)')
    
    args = parser.parse_args()
    
    # ãƒ„ãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    if args.tree_file is None:
        # æ©‹æ¢è¨ºæ–­ç”¨ã®resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æœ€æ–°ã‚’æ¤œç´¢
        results_dir = Path("data/doken_bridge_diagnosis_logic/results")
        if not results_dir.exists():
            print(f"âŒ Results directory not found: {results_dir}")
            return
        
        tree_files = sorted(results_dir.glob("scaling_test_tree_*.pkl"))
        if not tree_files:
            print(f"âŒ No tree files found in {results_dir}")
            return
        
        args.tree_file = str(tree_files[-1])  # æœ€æ–°
        print(f"Using latest tree file: {args.tree_file}")
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    if args.output is None:
        # ãƒ„ãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å‡ºåŠ›åã‚’ç”Ÿæˆ
        tree_name = Path(args.tree_file).stem  # "scaling_test_tree_250chunks_20251028_172826"
        results_dir = Path("data/doken_bridge_diagnosis_logic/results")
        args.output = str(results_dir / f"{tree_name}_visualization.png")
    
    # å®Ÿè¡Œ
    try:
        # 1. ãƒ„ãƒªãƒ¼èª­ã¿è¾¼ã¿
        tree_data = load_raptor_tree(args.tree_file)
        
        # 2. ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        G, node_info = build_graph_from_tree(tree_data, max_depth=args.max_depth)
        
        # 3. éšå±¤å‹ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§å¯è¦–åŒ–
        print("\n" + "="*60)
        print("ğŸ“Š Generating Hierarchical Layout...")
        print("="*60)
        visualize_tree(
            G, node_info,
            output_file=args.output,
            title=args.title,
            figsize=tuple(args.figsize),
            dpi=args.dpi
        )
        
        # 4. å††å½¢ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§å¯è¦–åŒ–
        print("\n" + "="*60)
        print("ğŸ¯ Generating Circular Layout...")
        print("="*60)
        visualize_tree_circular(
            G, node_info,
            output_file=args.output,
            title=args.title + " (Circular)",
            figsize=(24, 24),  # å††å½¢ã¯æ­£æ–¹å½¢
            dpi=args.dpi
        )
        
        print("\n" + "="*60)
        print("âœ… All visualizations complete!")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
