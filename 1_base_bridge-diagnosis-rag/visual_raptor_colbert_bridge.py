"""
Bridge Diagnosis RAPTOR with ColBERT Integration
æ©‹æ¢è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯ã«ç‰¹åŒ–ã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ 

æ©‹æ¢è¨ºæ–­æ–‡æ›¸ã®æå‚·ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãƒ»å¯¾ç­–å·¥æ³•ã‚’æŠ½å‡ºã™ã‚‹è¦–è¦šçš„æ–‡æ›¸æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
- ColVBERT (Transformers base) ã«ã‚ˆã‚‹ç”»åƒãƒ»æ–‡æ›¸çµ±åˆæ¤œç´¢
- æ©‹æ¢è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯å°‚ç”¨ã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
- OCRãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”»åƒå‡¦ç†
- è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ–‡æ›¸ï¼ˆã‚°ãƒ©ãƒ•ã€è¡¨ã€ã‚¹ã‚­ãƒ£ãƒ³ã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼‰ã¸ã®å¯¾å¿œ

Version: 2.0 - Bridge Diagnosis Edition
"""

import torch
import torch.nn as nn
from transformers import (
    AutoModel, AutoTokenizer, AutoProcessor,
    VisionEncoderDecoderModel, BlipProcessor, BlipForConditionalGeneration,
    Blip2Processor, Blip2ForConditionalGeneration, Blip2VisionModel
)
from PIL import Image
import pytesseract
import cv2
import fitz  # PyMuPDF for PDF text extraction
import numpy as np
import faiss
import json
import pickle
from typing import List, Dict, Tuple, Optional, Union
from pathlib import Path
import time
from dataclasses import dataclass
from langchain_core.documents import Document

# ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
sys.path.append(str(Path(__file__).parent / "0_base_tsunami-lesson-rag"))
from tsunami_lesson_raptor import TsunamiLessonRAPTOR


@dataclass
class VisualDocument:
    """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’è¡¨ç¾ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    image_path: str
    text_content: str = ""  # OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    layout_elements: List[Dict] = None  # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ ï¼ˆè¡¨ã€ã‚°ãƒ©ãƒ•ãªã©ï¼‰
    metadata: Dict = None
    
    def __post_init__(self):
        if self.layout_elements is None:
            self.layout_elements = []
        if self.metadata is None:
            self.metadata = {}


class ColModernVBERTEncoder(nn.Module):
    """
    ColModernVBERT ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆæœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰
    SigLIPã‚’ä½¿ç”¨ã—ãŸé«˜æ€§èƒ½ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¡¨ç¾å­¦ç¿’
    
    ç‰¹å¾´:
    - SigLIP (Sigmoid Loss for Language-Image Pre-training)
    - æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’
    - ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒˆãƒ¼ã‚¯ãƒ³è¡¨ç¾
    - ãƒãƒƒãƒå˜ä½ã®æœ€é©åŒ–
    """
    
    def __init__(
        self,
        text_model_name: str = "google/siglip-base-patch16-224",
        vision_model_name: str = "google/siglip-base-patch16-224",
        embedding_dim: int = 768,
        use_cross_attention: bool = True,
        device: str = None
    ):
        super().__init__()
        
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.use_cross_attention = use_cross_attention
        
        # GPUæœ€é©åŒ–è¨­å®š
        if self.device == "cuda":
            print(f"ğŸš€ GPUåŠ é€ŸãŒæœ‰åŠ¹ã§ã™: {torch.cuda.get_device_name()}")
            print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            torch.backends.cudnn.benchmark = True  # cuDNNæœ€é©åŒ–
        
        try:
            # SigLIPçµ±åˆãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ãƒ“ã‚¸ãƒ§ãƒ³ï¼‰
            from transformers import AutoModel, AutoProcessor
            self.model = AutoModel.from_pretrained(
                text_model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            self.processor = AutoProcessor.from_pretrained(vision_model_name)
            
            # SigLIPã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å–å¾—
            siglip_dim = self.model.config.projection_dim if hasattr(self.model.config, 'projection_dim') else 768
            
            # ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if self.use_cross_attention:
                self.cross_attention = nn.MultiheadAttention(
                    embed_dim=siglip_dim,
                    num_heads=8,
                    dropout=0.1,
                    batch_first=True
                )
            
            # æ¬¡å…ƒèª¿æ•´ãƒ¬ã‚¤ãƒ¤ãƒ¼
            self.projection = nn.Sequential(
                nn.Linear(siglip_dim, embedding_dim),
                nn.LayerNorm(embedding_dim),
                nn.GELU(),
                nn.Dropout(0.1)
            )
            
            # çµ±åˆãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒï¼‰
            self.fusion_layer = nn.Sequential(
                nn.Linear(embedding_dim * 2, embedding_dim * 2),
                nn.LayerNorm(embedding_dim * 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(embedding_dim * 2, embedding_dim),
                nn.LayerNorm(embedding_dim)
            )
            
            self.embedding_dim = embedding_dim
            self.to(self.device)
            
            # FP16æ··åˆç²¾åº¦ã®æœ‰åŠ¹åŒ–
            if self.device == "cuda":
                # è‡ªå‹•æ··åˆç²¾åº¦ã®è¨­å®š
                self.scaler = torch.amp.GradScaler()
                print(f"   âœ… FP16æ··åˆç²¾åº¦ãŒæœ‰åŠ¹")
                print(f"   âœ… cuDNNæœ€é©åŒ–ãŒæœ‰åŠ¹")
            
            print(f"âœ… ColModernVBERT initialized with SigLIP")
            print(f"   Device: {self.device}")
            print(f"   Embedding dim: {embedding_dim}")
            print(f"   Cross-attention: {use_cross_attention}")
            print(f"   Data type: {'FP16' if self.device == 'cuda' else 'FP32'}")
            
        except ImportError:
            raise ImportError(
                "SigLIP requires transformers>=4.35.0. "
                "Please upgrade: pip install --upgrade transformers"
            )
    
    def encode_text(self, texts: List[str], batch_size: int = 32) -> torch.Tensor:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆSigLIPä½¿ç”¨ï¼‰- GPUæœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†"""
        if not texts:
            return torch.empty(0, self.embedding_dim, device=self.device)
        
        all_embeddings = []
        
        # ãƒãƒƒãƒå‡¦ç†ã§GPUãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å‘ä¸Š
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            inputs = self.processor(
                text=batch_texts,
                padding=True,
                truncation=True,
                max_length=64,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                with torch.amp.autocast('cuda', enabled=(self.device == "cuda"), dtype=torch.float16):
                    outputs = self.model.get_text_features(**inputs)
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
                    embeddings = self.projection(outputs)
                    # L2æ­£è¦åŒ–ã‚’è¿½åŠ 
                    embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
            
            all_embeddings.append(embeddings.float())  # FP32ã«æˆ»ã™
        
        return torch.cat(all_embeddings, dim=0) if all_embeddings else torch.empty(0, self.embedding_dim, device=self.device)
    
    def encode_image(self, images: List[Image.Image], batch_size: int = 16) -> torch.Tensor:
        """ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆSigLIPä½¿ç”¨ï¼‰- GPUæœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†"""
        if not images:
            return torch.empty(0, self.embedding_dim, device=self.device)
        
        all_embeddings = []
        
        # ç”»åƒã¯å¤§ãã„ã®ã§ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ã
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i + batch_size]
            
            inputs = self.processor(
                images=batch_images,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                with torch.amp.autocast('cuda', enabled=(self.device == "cuda"), dtype=torch.float16):
                    outputs = self.model.get_image_features(**inputs)
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
                    embeddings = self.projection(outputs)
                    # L2æ­£è¦åŒ–ã‚’è¿½åŠ 
                    embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
            
            all_embeddings.append(embeddings.float())  # FP32ã«æˆ»ã™
            
            # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if self.device == "cuda":
                torch.cuda.empty_cache()
        
        return torch.cat(all_embeddings, dim=0) if all_embeddings else torch.empty(0, self.embedding_dim, device=self.device)
    
    def encode_multimodal(
        self,
        texts: List[str],
        images: List[Image.Image]
    ) -> torch.Tensor:
        """ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®çµ±åˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        text_embeddings = self.encode_text(texts)
        image_embeddings = self.encode_image(images)
        
        # ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹ç›¸äº’ä½œç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.use_cross_attention:
            # ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
            text_attended, _ = self.cross_attention(
                text_embeddings.unsqueeze(1),
                image_embeddings.unsqueeze(1),
                image_embeddings.unsqueeze(1)
            )
            text_embeddings = text_attended.squeeze(1)
        
        # çµåˆã¨èåˆ
        combined = torch.cat([text_embeddings, image_embeddings], dim=1)
        fused_embeddings = self.fusion_layer(combined)
        
        # æœ€çµ‚çš„ã«L2æ­£è¦åŒ–
        fused_embeddings = nn.functional.normalize(fused_embeddings, p=2, dim=1)
        
        return fused_embeddings
    
    def compute_similarity(
        self,
        text_embeddings: torch.Tensor,
        image_embeddings: torch.Tensor
    ) -> torch.Tensor:
        """
        SigLIPé¢¨ã®ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é¡ä¼¼åº¦è¨ˆç®—
        
        å¾“æ¥ã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’ï¼ˆsoftmaxï¼‰ã§ã¯ãªãã€
        sigmoid lossã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦
        """
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        text_norm = nn.functional.normalize(text_embeddings, p=2, dim=1)
        image_norm = nn.functional.normalize(image_embeddings, p=2, dim=1)
        
        similarity = torch.matmul(text_norm, image_norm.t())
        
        # SigLIPã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ï¼‰
        temperature = 10.0
        similarity = similarity * temperature
        
        return similarity
    
    def get_gpu_memory_info(self) -> Dict[str, float]:
        """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            cached = torch.cuda.memory_reserved() / 1024**3      # GB
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            free = total - allocated
            
            return {
                "allocated_gb": allocated,
                "cached_gb": cached, 
                "free_gb": free,
                "total_gb": total,
                "utilization_percent": (allocated / total) * 100
            }
        return {"message": "CUDA not available"}
    
    def clear_gpu_cache(self):
        """GPUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPU cache cleared")


class ColBERT2BLIP2Encoder(nn.Module):
    """
    ColBERT v2.0 + BLIP2 çµ±åˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    
    ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:
    - Vision: BLIP2 with ViT-G/14 (giant vision encoder)
    - Language: Flan-T5-Large (780M parameters)
    - Text Retrieval: ColBERTv2.0 (colbert-ir/colbertv2.0)
    
    ç‰¹å¾´:
    - BLIP2ã®Q-Formerã§ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆ
    - ColBERTv2.0ã®å¾ŒæœŸç›¸äº’ä½œç”¨(late interaction)ã§é«˜é€Ÿæ¤œç´¢
    - ã‚¯ã‚¨ãƒªå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰(query-only)ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆäº‹å‰è¨ˆç®—ã‚’ã‚µãƒãƒ¼ãƒˆ
    - ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®é‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å‘ä¸Š
    """
    
    def __init__(
        self,
        blip2_model_name: str = "Salesforce/blip2-flan-t5-xl",  # BLIP2 with Flan-T5-XL
        colbert_model_name: str = "colbert-ir/colbertv2.0",
        embedding_dim: int = 768,
        use_quantization: bool = False,  # é‡å­åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        image_batch_size: int = 8,  # ç”»åƒãƒãƒƒãƒã‚µã‚¤ã‚º
        device: str = None
    ):
        super().__init__()
        
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.embedding_dim = embedding_dim
        self.use_quantization = use_quantization
        self.image_batch_size = image_batch_size  # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦ä¿å­˜
        
        # GPUæœ€é©åŒ–è¨­å®š
        if self.device == "cuda":
            print(f"ğŸš€ GPUåŠ é€ŸãŒæœ‰åŠ¹ã§ã™: {torch.cuda.get_device_name()}")
            print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            torch.backends.cudnn.benchmark = True
        
        # BLIP2ãƒ¢ãƒ‡ãƒ«ï¼ˆVision + Q-Former + Language Modelï¼‰
        print(f"ğŸ“¥ Loading BLIP2 model: {blip2_model_name}")
        
        # é‡å­åŒ–è¨­å®š
        load_kwargs = {
            "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32,
            "device_map": "auto" if self.device == "cuda" else None,
        }
        
        if use_quantization and self.device == "cuda":
            from transformers import BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
            )
            load_kwargs["quantization_config"] = quantization_config
            print("   âš™ï¸  8-bit quantization enabled")
        
        self.blip2_processor = Blip2Processor.from_pretrained(blip2_model_name)
        self.blip2_model = Blip2ForConditionalGeneration.from_pretrained(
            blip2_model_name,
            **load_kwargs
        )
        
        # BLIP2ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å–å¾—
        # Q-Formerã®å‡ºåŠ›æ¬¡å…ƒ
        blip2_hidden_size = self.blip2_model.config.qformer_config.hidden_size
        
        # ColBERTv2.0ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ç”¨ï¼‰
        print(f"ğŸ“¥ Loading ColBERTv2.0 model: {colbert_model_name}")
        self.colbert_tokenizer = AutoTokenizer.from_pretrained(colbert_model_name)
        self.colbert_model = AutoModel.from_pretrained(
            colbert_model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
        )
        
        # ColBERTã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
        colbert_hidden_size = self.colbert_model.config.hidden_size
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å±¤
        self.blip2_projection = nn.Linear(blip2_hidden_size, embedding_dim)
        self.colbert_projection = nn.Linear(colbert_hidden_size, embedding_dim)
        
        # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èåˆå±¤
        self.fusion_layer = nn.Sequential(
            nn.Linear(embedding_dim * 2, embedding_dim),
            nn.LayerNorm(embedding_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim, embedding_dim)
        )
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆäº‹å‰è¨ˆç®—ç”¨ï¼‰
        self.document_embeddings_cache = {}
        
        self.to(self.device)
        
        if self.device == "cuda":
            print(f"   âœ… FP16æ··åˆç²¾åº¦ãŒæœ‰åŠ¹")
            print(f"   âœ… cuDNNæœ€é©åŒ–ãŒæœ‰åŠ¹")
        
        print(f"âœ… ColBERT v2.0 + BLIP2 initialized")
        print(f"   Device: {self.device}")
        print(f"   BLIP2 Model: {blip2_model_name}")
        print(f"   ColBERT Model: {colbert_model_name}")
        print(f"   Embedding dim: {embedding_dim}")
        print(f"   Quantization: {'Enabled' if use_quantization else 'Disabled'}")
        print(f"   Data type: {'FP16' if self.device == 'cuda' else 'FP32'}")
    
    def encode_text_colbert(self, texts: List[str], batch_size: int = 32, mode: str = "query") -> torch.Tensor:
        """
        ColBERTv2.0ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        
        Args:
            texts: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            mode: "query"ï¼ˆã‚¯ã‚¨ãƒªãƒ¢ãƒ¼ãƒ‰ï¼‰ã¾ãŸã¯ "doc"ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰
        """
        if not texts:
            return torch.empty(0, self.embedding_dim, device=self.device)
        
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # ColBERTv2.0ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
            if mode == "query":
                # ã‚¯ã‚¨ãƒªãƒ¢ãƒ¼ãƒ‰: [Q]ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
                batch_texts = [f"[Q] {text}" for text in batch_texts]
            elif mode == "doc":
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰: [D]ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
                batch_texts = [f"[D] {text}" for text in batch_texts]
            
            inputs = self.colbert_tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                with torch.amp.autocast('cuda', enabled=(self.device == "cuda"), dtype=torch.float16):
                    outputs = self.colbert_model(**inputs)
                    # æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ã®å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
                    embeddings = outputs.last_hidden_state.mean(dim=1)
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
                    embeddings = self.colbert_projection(embeddings)
                    # L2æ­£è¦åŒ–
                    embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
            
            all_embeddings.append(embeddings.float())
        
        return torch.cat(all_embeddings, dim=0) if all_embeddings else torch.empty(0, self.embedding_dim, device=self.device)
    
    def encode_image_blip2(self, images: List[Image.Image], batch_size: int = None) -> torch.Tensor:
        """
        BLIP2ã§ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        
        BLIP2ã®Q-Formerã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‹ã‚‰æ„å‘³çš„ç‰¹å¾´ã‚’æŠ½å‡º
        """
        if not images:
            return torch.empty(0, self.embedding_dim, device=self.device)
        
        # batch_sizeãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã‚’ä½¿ç”¨
        if batch_size is None:
            batch_size = self.image_batch_size
        
        all_embeddings = []
        
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i + batch_size]
            
            # BLIP2ã®ãƒ—ãƒ­ã‚»ãƒƒã‚µã§å‰å‡¦ç†
            inputs = self.blip2_processor(
                images=batch_images,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                with torch.amp.autocast('cuda', enabled=(self.device == "cuda"), dtype=torch.float16):
                    # Vision Encoder + Q-Former
                    vision_outputs = self.blip2_model.vision_model(inputs['pixel_values'])
                    image_embeds = vision_outputs[0]  # [batch, num_patches, hidden_size]
                    
                    # Q-Formerã§ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
                    image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=self.device)
                    query_tokens = self.blip2_model.query_tokens.expand(image_embeds.shape[0], -1, -1)
                    
                    query_outputs = self.blip2_model.qformer(
                        query_embeds=query_tokens,
                        encoder_hidden_states=image_embeds,
                        encoder_attention_mask=image_attention_mask,
                        return_dict=True
                    )
                    
                    # ã‚¯ã‚¨ãƒªå‡ºåŠ›ã®å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
                    image_features = query_outputs.last_hidden_state.mean(dim=1)
                    
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
                    image_features = self.blip2_projection(image_features)
                    # L2æ­£è¦åŒ–
                    image_features = nn.functional.normalize(image_features, p=2, dim=1)
            
            all_embeddings.append(image_features.float())
            
            # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if self.device == "cuda":
                torch.cuda.empty_cache()
        
        return torch.cat(all_embeddings, dim=0) if all_embeddings else torch.empty(0, self.embedding_dim, device=self.device)
    
    def encode_multimodal(
        self,
        texts: List[str],
        images: List[Image.Image],
        mode: str = "doc"
    ) -> torch.Tensor:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®çµ±åˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        
        Args:
            texts: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            images: ç”»åƒã®ãƒªã‚¹ãƒˆ
            mode: "query"ï¼ˆæ¤œç´¢æ™‚ï¼‰ã¾ãŸã¯ "doc"ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚ï¼‰
        """
        # ColBERTã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        text_embeddings = self.encode_text_colbert(texts, mode=mode)
        
        # BLIP2ã§ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        image_embeddings = self.encode_image_blip2(images)
        
        # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èåˆ
        combined = torch.cat([text_embeddings, image_embeddings], dim=1)
        fused_embeddings = self.fusion_layer(combined)
        
        # æœ€çµ‚L2æ­£è¦åŒ–
        fused_embeddings = nn.functional.normalize(fused_embeddings, p=2, dim=1)
        
        return fused_embeddings
    
    def encode_image(self, images: List[Image.Image], batch_size: int = None) -> torch.Tensor:
        """
        ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆencode_image_blip2ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰
        ä»–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹çµ±ä¸€ã®ãŸã‚
        """
        return self.encode_image_blip2(images, batch_size)
    
    def encode_text(self, texts: List[str], batch_size: int = 32) -> torch.Tensor:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆencode_text_colbertã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰
        ä»–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹çµ±ä¸€ã®ãŸã‚
        """
        return self.encode_text_colbert(texts, batch_size, mode="doc")
    
    def precompute_document_embeddings(
        self,
        documents: List[Dict],  # {'text': str, 'image': Image.Image}
        doc_ids: List[str],
        batch_size: int = 16
    ):
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        
        Args:
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’å«ã‚€ï¼‰
            doc_ids: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID ã®ãƒªã‚¹ãƒˆ
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        """
        print(f"ğŸ“Š Pre-computing document embeddings for {len(documents)} documents...")
        
        texts = [doc['text'] for doc in documents]
        images = [doc['image'] for doc in documents]
        
        # ãƒãƒƒãƒå‡¦ç†ã§åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
        embeddings = self.encode_multimodal(texts, images, mode="doc")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        for doc_id, embedding in zip(doc_ids, embeddings):
            self.document_embeddings_cache[doc_id] = embedding.cpu().numpy()
        
        print(f"âœ… Document embeddings cached: {len(self.document_embeddings_cache)} documents")
    
    def save_document_cache(self, cache_path: str):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        import pickle
        with open(cache_path, 'wb') as f:
            pickle.dump(self.document_embeddings_cache, f)
        print(f"ğŸ’¾ Document cache saved to: {cache_path}")
    
    def load_document_cache(self, cache_path: str):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        import pickle
        with open(cache_path, 'rb') as f:
            self.document_embeddings_cache = pickle.load(f)
        print(f"ğŸ“‚ Document cache loaded: {len(self.document_embeddings_cache)} documents")
    
    def late_interaction_score(
        self,
        query_embedding: torch.Tensor,
        doc_id: str
    ) -> float:
        """
        ColBERTã®å¾ŒæœŸç›¸äº’ä½œç”¨ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        
        Args:
            query_embedding: ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
            doc_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
        
        Returns:
            é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        """
        if doc_id not in self.document_embeddings_cache:
            raise ValueError(f"Document {doc_id} not found in cache")
        
        doc_embedding = torch.tensor(
            self.document_embeddings_cache[doc_id],
            device=self.device
        )
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        score = torch.dot(query_embedding, doc_embedding).item()
        
        return score


class ColVBERTEncoder(nn.Module):
    """
    ColVBERT ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆTransformers baseï¼‰
    ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®çµ±åˆè¡¨ç¾ã‚’å­¦ç¿’
    """
    
    def __init__(
        self,
        text_model_name: str = "intfloat/multilingual-e5-large",
        vision_model_name: str = "Salesforce/blip-image-captioning-base",
        embedding_dim: int = 768,
        device: str = None
    ):
        super().__init__()
        
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # GPUæœ€é©åŒ–è¨­å®š
        if self.device == "cuda":
            print(f"ğŸš€ GPUåŠ é€ŸãŒæœ‰åŠ¹ã§ã™: {torch.cuda.get_device_name()}")
            print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            torch.backends.cudnn.benchmark = True  # cuDNNæœ€é©åŒ–
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.text_encoder = AutoModel.from_pretrained(
            text_model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            trust_remote_code=True
        )
        self.text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)
        
        # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å–å¾—
        text_hidden_size = self.text_encoder.config.hidden_size
        
        # ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.vision_processor = BlipProcessor.from_pretrained(vision_model_name)
        self.vision_encoder = BlipForConditionalGeneration.from_pretrained(
            vision_model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            trust_remote_code=True,
            use_safetensors=True
        )
        
        # ãƒ“ã‚¸ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å–å¾—
        vision_hidden_size = self.vision_encoder.config.vision_config.hidden_size
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ“ã‚¸ãƒ§ãƒ³ã®æŠ•å½±å±¤
        self.text_projection = nn.Linear(text_hidden_size, embedding_dim)
        self.vision_projection = nn.Linear(vision_hidden_size, embedding_dim)
        
        # çµ±åˆãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.fusion_layer = nn.Sequential(
            nn.Linear(embedding_dim * 2, embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim, embedding_dim)
        )
        
        self.embedding_dim = embedding_dim
        self.to(self.device)
        
        # GPUæœ€é©åŒ–è¨­å®šã®è¡¨ç¤º
        if self.device == "cuda":
            print(f"   âœ… FP16æ··åˆç²¾åº¦ãŒæœ‰åŠ¹")
            print(f"   âœ… cuDNNæœ€é©åŒ–ãŒæœ‰åŠ¹")
            
        print(f"âœ… ColVBERT initialized with BLIP")
        print(f"   Device: {self.device}")
        print(f"   Text Model: {text_model_name}")
        print(f"   Vision Model: {vision_model_name}")
        print(f"   Embedding dim: {embedding_dim}")
        print(f"   Data type: {'FP16' if self.device == 'cuda' else 'FP32'}")
    
    def encode_text(self, texts: List[str], batch_size: int = 32) -> torch.Tensor:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ - GPUæœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†"""
        if not texts:
            return torch.empty(0, self.embedding_dim, device=self.device)
        
        all_embeddings = []
        
        # ãƒãƒƒãƒå‡¦ç†ã§GPUãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å‘ä¸Š
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            inputs = self.text_tokenizer(
                batch_texts, 
                padding=True, 
                truncation=True, 
                max_length=512,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                with torch.amp.autocast('cuda', enabled=(self.device == "cuda"), dtype=torch.float16):
                    outputs = self.text_encoder(**inputs)
                    embeddings = outputs.last_hidden_state.mean(dim=1)
                    # æŠ•å½±å±¤ã‚’é€šã—ã¦åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã«å¤‰æ›
                    embeddings = self.text_projection(embeddings)
                    # L2æ­£è¦åŒ–ã‚’è¿½åŠ 
                    embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
            
            all_embeddings.append(embeddings.float())  # FP32ã«æˆ»ã™
        
        return torch.cat(all_embeddings, dim=0) if all_embeddings else torch.empty(0, self.embedding_dim, device=self.device)
    
    def encode_image(self, images: List[Image.Image], batch_size: int = 16) -> torch.Tensor:
        """ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ - GPUæœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†"""
        if not images:
            return torch.empty(0, self.embedding_dim, device=self.device)
        
        all_embeddings = []
        
        # ç”»åƒã¯å¤§ãã„ã®ã§ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ã
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i + batch_size]
            
            inputs = self.vision_processor(
                images=batch_images,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                with torch.amp.autocast('cuda', enabled=(self.device == "cuda"), dtype=torch.float16):
                    # BLIPForConditionalGeneration uses vision_model for image features
                    outputs = self.vision_encoder.vision_model(inputs['pixel_values'])
                    # CLSãƒˆãƒ¼ã‚¯ãƒ³ã§ã¯ãªãå¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨
                    image_features = outputs.last_hidden_state.mean(dim=1)
                    # æŠ•å½±å±¤ã‚’é€šã—ã¦åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã«å¤‰æ›
                    image_features = self.vision_projection(image_features)
                    # L2æ­£è¦åŒ–ã‚’è¿½åŠ 
                    image_features = nn.functional.normalize(image_features, p=2, dim=1)
            
            all_embeddings.append(image_features.float())  # FP32ã«æˆ»ã™
            
            # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if self.device == "cuda":
                torch.cuda.empty_cache()
        
        return torch.cat(all_embeddings, dim=0) if all_embeddings else torch.empty(0, self.embedding_dim, device=self.device)
    
    def encode_multimodal(
        self,
        texts: List[str],
        images: List[Image.Image]
    ) -> torch.Tensor:
        """ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®çµ±åˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°"""
        text_embeddings = self.encode_text(texts)
        image_embeddings = self.encode_image(images)
        
        # æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹
        if image_embeddings.size(1) != text_embeddings.size(1):
            image_embeddings = nn.functional.adaptive_avg_pool1d(
                image_embeddings.unsqueeze(0), 
                text_embeddings.size(1)
            ).squeeze(0)
        
        # çµåˆ
        combined = torch.cat([text_embeddings, image_embeddings], dim=1)
        
        # èåˆ
        fused_embeddings = self.fusion_layer(combined)
        
        # æœ€çµ‚çš„ã«L2æ­£è¦åŒ–
        fused_embeddings = nn.functional.normalize(fused_embeddings, p=2, dim=1)
        
        return fused_embeddings
    
    def get_gpu_memory_info(self) -> Dict[str, float]:
        """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            cached = torch.cuda.memory_reserved() / 1024**3      # GB
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            free = total - allocated
            
            return {
                "allocated_gb": allocated,
                "cached_gb": cached, 
                "free_gb": free,
                "total_gb": total,
                "utilization_percent": (allocated / total) * 100
            }
        return {"message": "CUDA not available"}
    
    def clear_gpu_cache(self):
        """GPUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPU cache cleared")


class VisualDocumentProcessor:
    """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, ocr_config: Dict = None, pdf_dir: str = None):
        self.ocr_config = ocr_config or {
            'lang': 'jpn+eng',
            'config': '--psm 6'
        }
        self.pdf_dir = pdf_dir  # PDFå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.pdf_text_cache = {}  # PDFãƒ†ã‚­ã‚¹ãƒˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        # PDFãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã‚’äº‹å‰æŠ½å‡º
        if pdf_dir:
            self._extract_pdf_texts(pdf_dir)
    
    def _extract_pdf_texts(self, pdf_dir: str):
        """PDFãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        pdf_path = Path(pdf_dir)
        if not pdf_path.exists():
            return
        
        for pdf_file in pdf_path.glob("*.pdf"):
            try:
                doc = fitz.open(str(pdf_file))
                pdf_name = pdf_file.stem
                
                # å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                for page_num in range(len(doc)):
                    page = doc[page_num]
                    text = page.get_text()
                    
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«å_ãƒšãƒ¼ã‚¸ç•ªå·
                    cache_key = f"{pdf_name}_page{page_num+1:03d}"
                    self.pdf_text_cache[cache_key] = text.strip()
                
                print(f"  âœ… PDFæŠ½å‡º: {pdf_name} ({len(doc)}ãƒšãƒ¼ã‚¸)")
                doc.close()  # ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã«ã‚¯ãƒ­ãƒ¼ã‚º
                
            except Exception as e:
                print(f"  âš ï¸ PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({pdf_file.name}): {e}")
            except Exception as e:
                print(f"  âš ï¸ PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({pdf_file.name}): {e}")
    
    def get_text_from_cache(self, image_filename: str) -> str:
        """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸPDFãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰.pngã‚’é™¤å»ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ä½œæˆ
        cache_key = image_filename.replace('.png', '').replace('.jpg', '').replace('.jpeg', '')
        return self.pdf_text_cache.get(cache_key, "")
    
    def extract_text_from_image(self, image_path: str) -> str:
        """OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return ""
            
            # å‰å‡¦ç†
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            # ãƒã‚¤ã‚ºé™¤å»
            denoised = cv2.fastNlMeansDenoising(gray)
            # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå‘ä¸Š
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(denoised)
            
            # OCRå®Ÿè¡Œ
            text = pytesseract.image_to_string(
                enhanced,
                lang=self.ocr_config['lang'],
                config=self.ocr_config['config']
            )
            
            return text.strip()
        except Exception as e:
            # OCRã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆTesseractæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å ´åˆï¼‰
            return ""
    
    def detect_layout_elements(self, image_path: str) -> List[Dict]:
        """ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ ã‚’æ¤œå‡º"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return []
            
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            elements = []
            
            # è¡¨ã®æ¤œå‡ºï¼ˆå˜ç´”ãªçŸ©å½¢æ¤œå‡ºï¼‰
            contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            for contour in contours:
                x, y, w, h = cv2.boundingRect(contour)
                if w > 100 and h > 50:  # æœ€å°ã‚µã‚¤ã‚ºé–¾å€¤
                    elements.append({
                        'type': 'table_or_figure',
                        'bbox': [x, y, w, h],
                        'area': w * h
                    })
            
            return elements
        except Exception as e:
            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ¤œå‡ºã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
            return []
    
    def process_visual_document(self, image_path: str) -> VisualDocument:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’å‡¦ç†"""
        # ã¾ãšPDFã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        image_filename = Path(image_path).name
        text_content = self.get_text_from_cache(image_filename)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„å ´åˆã¯OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not text_content:
            text_content = self.extract_text_from_image(image_path)
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ æ¤œå‡º
        layout_elements = self.detect_layout_elements(image_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            'image_size': self._get_image_size(image_path),
            'processing_time': time.time()
        }
        
        return VisualDocument(
            image_path=image_path,
            text_content=text_content,
            layout_elements=layout_elements,
            metadata=metadata
        )
    
    def _get_image_size(self, image_path: str) -> Tuple[int, int]:
        """ç”»åƒã‚µã‚¤ã‚ºã‚’å–å¾—"""
        try:
            with Image.open(image_path) as img:
                return img.size
        except:
            return (0, 0)


class BridgeRAPTORColBERT(TsunamiLessonRAPTOR):
    """
    æ©‹æ¢è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯å°‚ç”¨ RAPTOR + ColBERT ã‚·ã‚¹ãƒ†ãƒ 
    
    æ©Ÿèƒ½:
    - OCRãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”»åƒå‡¦ç†
    - ColVBERT / ColModernVBERT ã«ã‚ˆã‚‹çµ±åˆæ¤œç´¢
    - æ©‹æ¢è¨ºæ–­æ–‡æ›¸ã®æå‚·ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãƒ»å¯¾ç­–å·¥æ³•ã®æŠ½å‡º
    - è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ–‡æ›¸ã¸ã®å¯¾å¿œ
    """
    
    def __init__(
        self,
        embeddings_model,
        llm,
        colbert_config: Dict = None,
        visual_config: Dict = None,
        use_modern_vbert: bool = False,
        use_blip2_colbert: bool = False,  # æ–°ã—ã„BLIP2+ColBERTv2.0ã‚’ä½¿ç”¨
        pdf_source_dir: str = None,  # PDFå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        use_multimodal: bool = True,  # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨
        multimodal_weight: float = 0.3,  # ç”»åƒåŸ‹ã‚è¾¼ã¿ã®é‡ã¿
        **kwargs
    ):
        # ColVBERT / ColModernVBERT / ColBERT2BLIP2 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼é¸æŠ
        colbert_config = colbert_config or {}
        encoder_type = colbert_config.get('encoder_type', 'modern' if use_modern_vbert else 'standard')
        
        if use_blip2_colbert or encoder_type == 'blip2_colbert':
            # ColBERT v2.0 + BLIP2ï¼ˆæœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰
            self.colbert_encoder = ColBERT2BLIP2Encoder(
                blip2_model_name=colbert_config.get('blip2_model', 'Salesforce/blip2-flan-t5-xl'),
                colbert_model_name=colbert_config.get('colbert_model', 'colbert-ir/colbertv2.0'),
                embedding_dim=colbert_config.get('embedding_dim', 768),
                use_quantization=colbert_config.get('use_quantization', False),
                image_batch_size=colbert_config.get('image_batch_size', 8)  # image_batch_sizeã‚’æ¸¡ã™
            )
            self.encoder_type = 'ColBERT v2.0 + BLIP2'
        elif encoder_type == 'modern' or use_modern_vbert:
            # ColModernVBERTï¼ˆSigLIPä½¿ç”¨ï¼‰
            self.colbert_encoder = ColModernVBERTEncoder(
                text_model_name=colbert_config.get('text_model', 'google/siglip-base-patch16-224'),
                vision_model_name=colbert_config.get('vision_model', 'google/siglip-base-patch16-224'),
                embedding_dim=colbert_config.get('embedding_dim', 768),
                use_cross_attention=colbert_config.get('use_cross_attention', True)
            )
            self.encoder_type = 'ColModernVBERT (SigLIP)'
        else:
            # å¾“æ¥ã®ColVBERTï¼ˆBLIPä½¿ç”¨ï¼‰
            self.colbert_encoder = ColVBERTEncoder(
                text_model_name=colbert_config.get('text_model', 'intfloat/multilingual-e5-large'),
                vision_model_name=colbert_config.get('vision_model', 'Salesforce/blip-image-captioning-base'),
                embedding_dim=colbert_config.get('embedding_dim', 768)
            )
            self.encoder_type = 'ColVBERT (BLIP)'
        
        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–ï¼ˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™ï¼‰
        super().__init__(
            embeddings_model=embeddings_model,
            llm=llm,
            visual_encoder=self.colbert_encoder,
            use_multimodal=use_multimodal,
            multimodal_weight=multimodal_weight,
            **kwargs
        )
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ï¼ˆPDFå…ƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¸¡ã™ï¼‰
        visual_config = visual_config or {}
        self.visual_processor = VisualDocumentProcessor(
            ocr_config=visual_config.get('ocr_config'),
            pdf_dir=pdf_source_dir
        )
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.visual_documents: List[VisualDocument] = []
        self.visual_embeddings: Optional[np.ndarray] = None
        self.visual_index: Optional[faiss.Index] = None
        
        print(f"ğŸ–¼ï¸ Visual RAPTOR ColBERT initialized")
        print(f"   Encoder: {self.encoder_type}")
        print(f"   Device: {self.colbert_encoder.device}")
        if use_multimodal:
            print(f"   ğŸ¨ Multimodal embeddings: ENABLED (weight: {multimodal_weight})")
        if encoder_type == 'modern' or use_modern_vbert:
            print(f"   âœ¨ Using ColModernVBERT with SigLIP")
        else:
            print(f"   Text Model: {colbert_config.get('text_model', 'intfloat/multilingual-e5-large')}")
            print(f"   Vision Model: {colbert_config.get('vision_model', 'Salesforce/blip-image-captioning-base')}")
    
    def load_visual_documents(
        self,
        image_directory: str,
        supported_formats: List[str] = None
    ) -> List[VisualDocument]:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿"""
        if supported_formats is None:
            supported_formats = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        
        image_dir = Path(image_directory)
        image_files = []
        
        for ext in supported_formats:
            image_files.extend(image_dir.glob(f"*{ext}"))
            image_files.extend(image_dir.glob(f"*{ext.upper()}"))
        
        print(f"ğŸ“¸ Processing {len(image_files)} visual documents...")
        
        visual_docs = []
        for i, image_path in enumerate(image_files, 1):
            if i % 20 == 0 or i == 1:
                print(f"   Progress: {i}/{len(image_files)}")
            visual_doc = self.visual_processor.process_visual_document(str(image_path))
            visual_docs.append(visual_doc)
        
        self.visual_documents = visual_docs
        print(f"âœ… Loaded {len(visual_docs)} visual documents")
        
        return visual_docs
    
    def summarize_cluster(self, documents: List[Document]) -> str:
        """ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¦ç´„ï¼ˆæ©‹æ¢è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯ç”¨ï¼‰"""
        print(f"   ğŸ”„ Summarizing {len(documents)} documents...", end=" ", flush=True)
        
        import time
        start_time = time.time()
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        texts = []
        for doc in documents:
            if hasattr(doc, 'page_content'):
                texts.append(doc.page_content)
            elif hasattr(doc, 'cached_text'):
                texts.append(doc.cached_text)
            elif hasattr(doc, 'text_content'):
                texts.append(doc.text_content)
            else:
                texts.append(str(doc))
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã€8000æ–‡å­—ã¾ã§ï¼ˆé«˜é€ŸåŒ–ï¼‰
        combined_text = "\n\n".join(texts)
        max_input_length = 8000  # num_ctx=16384ã§ä½™è£•ã‚ã‚Š
        
        if len(combined_text) > max_input_length:
            # å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sample_ratio = max_input_length / len(combined_text)
            sampled_texts = []
            for text in texts:
                sample_len = int(len(text) * sample_ratio)
                if sample_len > 0:
                    sampled_texts.append(text[:sample_len])
            combined_text = "\n\n".join(sampled_texts)[:max_input_length]
        
        # æ©‹æ¢è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        
        prompt = ChatPromptTemplate.from_template(
            "ä»¥ä¸‹ã¯æ©‹æ¢è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯ã«é–¢ã™ã‚‹è¤‡æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚\n\n"
            "ã€è¦ç´„ã‚¿ã‚¹ã‚¯ã€‘\n"
            "- éƒ¨æã¨æå‚·ã®çŠ¶æ…‹ã€ãã®åŸå› ã¨è£œä¿®å·¥æ³•ã‚’è¦ç´„ã—ã¦ãã ã•ã„\n"
            "- 400-500æ–‡å­—ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„\n"
            "- ç®‡æ¡æ›¸ãã§ã¯ãªãã€æ®µè½å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„\n\n"
            "ã€ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{text}\n\n"
            "ã€è¦ç´„ã€‘"
        )
        
        chain = prompt | self.llm | StrOutputParser()
        
        try:
            summary = chain.invoke({"text": combined_text})
            elapsed = time.time() - start_time
            print(f"âœ… ({len(summary)} chars, {elapsed:.1f}s)")
            return summary
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"âš ï¸ Error ({elapsed:.1f}s): {str(e)[:100]}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ˆé ­1000æ–‡å­—ã‚’è¿”ã™
            return combined_text[:1000]
    
    def build_visual_index(self) -> faiss.Index:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        if not self.visual_documents:
            raise ValueError("ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"ğŸ” Building visual document index...")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’æº–å‚™
        texts = [doc.text_content for doc in self.visual_documents]
        images = []
        
        for doc in self.visual_documents:
            try:
                img = Image.open(doc.image_path).convert('RGB')
                images.append(img)
            except Exception as e:
                print(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {doc.image_path}: {e}")
                # ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ä½œæˆ
                images.append(Image.new('RGB', (224, 224), color='white'))
        
        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        embeddings = self.colbert_encoder.encode_multimodal(texts, images)
        embeddings_np = embeddings.detach().cpu().numpy()  # .detach()ã‚’è¿½åŠ 
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        embedding_dim = embeddings_np.shape[1]
        index = faiss.IndexFlatIP(embedding_dim)  # Inner Product for similarity
        index.add(embeddings_np.astype('float32'))
        
        self.visual_embeddings = embeddings_np
        self.visual_index = index
        
        print(f"âœ… Visual index built with {len(self.visual_documents)} documents")
        return index
    
    def search_visual_documents(
        self,
        query: str,
        query_image: Optional[str] = None,
        top_k: int = 5
    ) -> List[Tuple[VisualDocument, float]]:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’æ¤œç´¢"""
        if self.visual_index is None:
            raise ValueError("ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        if query_image:
            # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¯ã‚¨ãƒª
            query_img = Image.open(query_image).convert('RGB')
            query_embedding = self.colbert_encoder.encode_multimodal([query], [query_img])
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚¯ã‚¨ãƒª
            query_embedding = self.colbert_encoder.encode_text([query])
        
        query_vec = query_embedding.cpu().numpy().astype('float32')
        
        # æ¤œç´¢å®Ÿè¡Œ
        scores, indices = self.visual_index.search(query_vec, top_k)
        
        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.visual_documents):
                results.append((self.visual_documents[idx], float(score)))
        
        return results
    
    def create_hybrid_documents(self) -> List[Document]:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’LangChainã®Documentã«å¤‰æ›"""
        hybrid_docs = []
        
        for i, visual_doc in enumerate(self.visual_documents):
            # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµåˆ
            content = f"ç”»åƒãƒ‘ã‚¹: {visual_doc.image_path}\n"
            if visual_doc.text_content:
                content += f"æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ:\n{visual_doc.text_content}\n"
            
            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæƒ…å ±ã‚’è¿½åŠ 
            if visual_doc.layout_elements:
                content += f"ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ : {len(visual_doc.layout_elements)}å€‹ã®è¦ç´ ã‚’æ¤œå‡º\n"
                for elem in visual_doc.layout_elements:
                    content += f"- {elem['type']}: é¢ç©{elem['area']}\n"
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            metadata = {
                'source': visual_doc.image_path,
                'type': 'visual_document',
                'image_size': visual_doc.metadata.get('image_size'),
                'layout_elements_count': len(visual_doc.layout_elements),
                'visual_doc_index': i
            }
            
            doc = Document(page_content=content, metadata=metadata)
            hybrid_docs.append(doc)
        
        return hybrid_docs
    
    def build_hybrid_tree(
        self,
        text_documents: List[Document],
        visual_documents: List[VisualDocument] = None,
        save_dir: str = "saved_models/visual_raptor"
    ) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã®çµ±åˆãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰"""
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’Documentã«å¤‰æ›
        if visual_documents is None:
            visual_documents = self.visual_documents
        
        if visual_documents:
            hybrid_docs = self.create_hybrid_documents()
            all_documents = text_documents + hybrid_docs
        else:
            all_documents = text_documents
        
        print(f"ğŸŒ² Building hybrid tree with {len(text_documents)} text docs and {len(visual_documents)} visual docs")
        
        # è¦ªã‚¯ãƒ©ã‚¹ã®ãƒ„ãƒªãƒ¼æ§‹ç¯‰ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
        tree = self.build_disaster_tree(all_documents, save_dir)
        
        return tree
    
    def save_visual_components(self, save_dir: str):
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä¿å­˜"""
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        visual_docs_data = []
        for doc in self.visual_documents:
            visual_docs_data.append({
                'image_path': doc.image_path,
                'text_content': doc.text_content,
                'layout_elements': doc.layout_elements,
                'metadata': doc.metadata
            })
        
        with open(save_path / 'visual_documents.json', 'w', encoding='utf-8') as f:
            json.dump(visual_docs_data, f, ensure_ascii=False, indent=2)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        if self.visual_embeddings is not None:
            np.save(save_path / 'visual_embeddings.npy', self.visual_embeddings)
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if self.visual_index is not None:
            faiss.write_index(self.visual_index, str(save_path / 'visual_index.faiss'))
        
        print(f"âœ… Visual components saved to {save_dir}")
    
    def load_visual_components(self, save_dir: str):
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        save_path = Path(save_dir)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        visual_docs_file = save_path / 'visual_documents.json'
        if visual_docs_file.exists():
            with open(visual_docs_file, 'r', encoding='utf-8') as f:
                visual_docs_data = json.load(f)
            
            self.visual_documents = []
            for data in visual_docs_data:
                visual_doc = VisualDocument(
                    image_path=data['image_path'],
                    text_content=data['text_content'],
                    layout_elements=data['layout_elements'],
                    metadata=data['metadata']
                )
                self.visual_documents.append(visual_doc)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        embeddings_file = save_path / 'visual_embeddings.npy'
        if embeddings_file.exists():
            self.visual_embeddings = np.load(embeddings_file)
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        index_file = save_path / 'visual_index.faiss'
        if index_file.exists():
            self.visual_index = faiss.read_index(str(index_file))
        
        print(f"âœ… Visual components loaded from {save_dir}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    from langchain_ollama import OllamaEmbeddings, ChatOllama
    
    print("="*80)
    print("Bridge Diagnosis RAPTOR ColBERT System")
    print("æ©‹æ¢è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯å°‚ç”¨ã‚·ã‚¹ãƒ†ãƒ ")
    print("="*80)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    embeddings = OllamaEmbeddings(
        model="mxbai-embed-large",
        base_url="http://localhost:11434"
    )
    llm = ChatOllama(
        model="granite-code:8b",
        temperature=0,
        base_url="http://localhost:11434",
        timeout=300
    )
    
    # BridgeRAPTORColBERTåˆæœŸåŒ–
    bridge_raptor = BridgeRAPTORColBERT(
        embeddings_model=embeddings,
        llm=llm,
        colbert_config={
            'text_model': 'intfloat/multilingual-e5-large',
            'vision_model': 'Salesforce/blip2-opt-2.7b',
            'embedding_dim': 768
        },
        visual_config={
            'ocr_config': {
                'lang': 'jpn+eng',
                'config': '--psm 6'
            }
        },
        min_clusters=2,
        max_clusters=5,
        max_depth=2,
        chunk_size=500,
        chunk_overlap=100
    )
    
    print("âœ… Visual RAPTOR ColBERT ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    main()