"""
Rate-Limited Parallel Processing for PubMed API
PubMedã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«å¯¾å¿œã—ãŸä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 

Author: AI Assistant
Date: 2025-10-31
"""

import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple
import queue

class RateLimitedPubMedProcessor:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œPubMedä¸¦åˆ—å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, requests_per_second: float = 3.0, max_workers: int = 4):
        """
        Args:
            requests_per_second: ç§’ã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°åˆ¶é™
            max_workers: æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        self.requests_per_second = requests_per_second
        self.max_workers = min(max_workers, 4)  # PubMedã®ãŸã‚ä¿å®ˆçš„ã«åˆ¶é™
        self.request_times = queue.Queue()
        self.lock = threading.Lock()
        
    def _wait_for_rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’éµå®ˆã™ã‚‹ãŸã‚ã®å¾…æ©Ÿ"""
        current_time = time.time()
        
        with self.lock:
            # å¤ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»ã‚’å‰Šé™¤ï¼ˆ1ç§’ã‚ˆã‚Šå¤ã„ã‚‚ã®ï¼‰
            temp_times = []
            while not self.request_times.empty():
                req_time = self.request_times.get()
                if current_time - req_time < 1.0:
                    temp_times.append(req_time)
            
            # æˆ»ã™
            for t in temp_times:
                self.request_times.put(t)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if self.request_times.qsize() >= self.requests_per_second:
                sleep_time = 1.0 / self.requests_per_second
                time.sleep(sleep_time)
            
            # ç¾åœ¨ã®æ™‚åˆ»ã‚’è¨˜éŒ²
            self.request_times.put(current_time)
    
    def parallel_pubmed_search(self, queries: List[str], search_func, max_articles: int = 30) -> Dict[str, List[Any]]:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œã®ä¸¦åˆ—PubMedæ¤œç´¢"""
        
        print(f"ğŸ”„ Rate-limited parallel search with {self.max_workers} workers ({self.requests_per_second} req/s)")
        
        def search_with_rate_limit(query_data):
            query, index = query_data
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¾…æ©Ÿ
            self._wait_for_rate_limit()
            
            thread_start = time.time()
            try:
                result = search_func(query, max_articles)
                thread_time = time.time() - thread_start
                print(f"âœ“ Worker {index+1}: '{query[:50]}...' - {len(result) if result else 0}æœ¬ ({thread_time:.1f}s)")
                return query, result, thread_time
            except Exception as e:
                print(f"âœ— Worker {index+1} error: {e}")
                return query, [], 0
        
        # ã‚¯ã‚¨ãƒªã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä»˜åŠ 
        query_data = [(query, i) for i, query in enumerate(queries)]
        
        results = {}
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_query = {executor.submit(search_with_rate_limit, qd): qd for qd in query_data}
            
            for future in as_completed(future_to_query):
                query, result, thread_time = future.result()
                results[query] = result
        
        return results


def optimize_immune_raptor_parallel(raptor_tree, max_articles_per_query: int = 20, max_workers: int = 4):
    """
    å…ç–«RAGã‚·ã‚¹ãƒ†ãƒ ç”¨ã®æœ€é©åŒ–ã•ã‚ŒãŸä¸¦åˆ—å‡¦ç†
    ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å¼·åŒ–
    """
    
    print("\\nğŸš€ OPTIMIZED RATE-LIMITED PARALLEL PROCESSING")
    print("=" * 55)
    
    start_time = time.time()
    
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–
    processor = RateLimitedPubMedProcessor(
        requests_per_second=2.5,  # ä¿å®ˆçš„ãªãƒ¬ãƒ¼ãƒˆ
        max_workers=max_workers
    )
    
    # 1. ä¸¦åˆ—PubMedæ¤œç´¢ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼‰
    print("ğŸ“¡ Phase 1: PubMed Literature Retrieval")
    print("-" * 40)
    
    queries = raptor_tree.pubmed_retriever.immune_queries
    
    def safe_search_articles(query, max_articles):
        """å®‰å…¨ãªPubMedæ¤œç´¢ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆï¼‰"""
        try:
            # ã¾ãšã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            cached_articles = raptor_tree.pubmed_retriever._get_cached_articles(query)
            if cached_articles:
                return cached_articles[:max_articles]
            
            # APIã‹ã‚‰æ¤œç´¢ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼‰
            pmids = raptor_tree.pubmed_retriever.pubmed_api.search_articles(query, max_articles)
            if pmids:
                articles = raptor_tree.pubmed_retriever.pubmed_api.fetch_article_details(pmids)
                
                # é–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
                for article in articles:
                    article.relevance_score = raptor_tree.pubmed_retriever.pubmed_api.calculate_relevance_score(
                        article, raptor_tree.pubmed_retriever.target_terms
                    )
                
                return articles
            else:
                return []
        except Exception as e:
            print(f"Search error for '{query}': {e}")
            return []
    
    # ä¸¦åˆ—å®Ÿè¡Œ
    search_results = processor.parallel_pubmed_search(
        queries, safe_search_articles, max_articles_per_query
    )
    
    # çµæœçµ±åˆ
    all_articles = {}
    total_retrieved = 0
    for query, articles in search_results.items():
        total_retrieved += len(articles)
        for article in articles:
            if article.pmid not in all_articles:
                all_articles[article.pmid] = article
    
    raptor_tree.pubmed_articles = all_articles
    retrieval_time = time.time() - start_time
    
    print(f"ğŸ“Š Retrieval completed: {len(all_articles)} unique articles")
    print(f"   Total retrieved: {total_retrieved}, Time: {retrieval_time:.1f}s")
    
    # 2. ä¸¦åˆ—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼‰
    print("\\nâš¡ Phase 2: Parallel Text Encoding")
    print("-" * 40)
    
    encoding_start = time.time()
    
    if all_articles:
        articles_list = list(all_articles.items())
        chunk_size = max(1, len(articles_list) // max_workers)
        
        def encode_chunk_safe(chunk_data):
            """å®‰å…¨ãªãƒãƒ£ãƒ³ã‚¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
            chunk, chunk_index = chunk_data
            chunk_start = time.time()
            results = []
            
            try:
                for pmid, article in chunk:
                    embedding = raptor_tree.embedder.encode_pubmed_article(article)
                    results.append((pmid, embedding))
                
                chunk_time = time.time() - chunk_start
                print(f"âœ“ Encoding chunk {chunk_index+1}: {len(chunk)} articles ({chunk_time:.1f}s)")
                return results
            except Exception as e:
                print(f"âœ— Encoding error in chunk {chunk_index+1}: {e}")
                return []
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = []
        for i in range(0, len(articles_list), chunk_size):
            chunk = articles_list[i:i + chunk_size]
            chunks.append((chunk, len(chunks)))
        
        # ä¸¦åˆ—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ
        all_embeddings = {}
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            chunk_futures = [executor.submit(encode_chunk_safe, chunk_data) for chunk_data in chunks]
            
            for future in as_completed(chunk_futures):
                chunk_results = future.result()
                for pmid, embedding in chunk_results:
                    all_embeddings[pmid] = embedding
        
        raptor_tree.article_embeddings = all_embeddings
        encoding_time = time.time() - encoding_start
        
        print(f"ğŸ“Š Encoding completed: {len(all_embeddings)} embeddings in {encoding_time:.1f}s")
    else:
        encoding_time = 0
        print("ğŸ“Š No articles to encode")
    
    # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    total_time = time.time() - start_time
    
    print("\\nğŸ¯ PERFORMANCE SUMMARY")
    print("=" * 30)
    print(f"Total execution time: {total_time:.1f}s")
    print(f"  Literature retrieval: {retrieval_time:.1f}s ({retrieval_time/total_time*100:.0f}%)")
    print(f"  Text encoding: {encoding_time:.1f}s ({encoding_time/total_time*100:.0f}%)")
    print(f"Articles processed: {len(all_articles)}")
    print(f"Workers used: {max_workers}")
    print(f"Rate limit: {processor.requests_per_second} req/s")
    
    # åŠ¹ç‡æ€§è¨ˆç®—
    if len(all_articles) > 0:
        articles_per_second = len(all_articles) / total_time
        print(f"Processing rate: {articles_per_second:.1f} articles/second")
        
        # ä¸¦åˆ—åŠ¹ç‡æ¨å®š
        estimated_sequential = len(queries) * 2.0 + len(all_articles) * 0.1  # ä¿å®ˆçš„è¦‹ç©ã‚‚ã‚Š
        speedup = estimated_sequential / total_time if total_time > 0 else 1
        efficiency = speedup / max_workers * 100
        
        print(f"Estimated speedup: {speedup:.1f}x")
        print(f"Parallel efficiency: {efficiency:.0f}%")
    
    return {
        'total_time': total_time,
        'retrieval_time': retrieval_time,
        'encoding_time': encoding_time,
        'articles_processed': len(all_articles),
        'workers_used': max_workers,
        'rate_limit': processor.requests_per_second
    }