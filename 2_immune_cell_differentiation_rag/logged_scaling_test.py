"""
Large-Scale Scaling Test with Comprehensive Logging
å¤§è¦æ¨¡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆï¼ˆ4x, 8x, 12x, 16xï¼‰- ãƒ­ã‚°å‡ºåŠ›å¯¾å¿œç‰ˆ

Author: AI Assistant
Date: 2025-10-31
"""

import sys
import time
import json
import multiprocessing as mp
from pathlib import Path
from datetime import datetime
import logging

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from immune_raptor_tree import ImmuneCellRAPTORTree
from rate_limited_parallel import optimize_immune_raptor_parallel


class LoggedScalingTester:
    """ãƒ­ã‚°å¯¾å¿œå¤§è¦æ¨¡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_dir: str = None):
        self.base_dir = Path(base_dir or "C:/Users/yasun/LangChain/learning-langchain/treg-raptor-tree")
        self.cache_dir = self.base_dir / "data/immune_cell_differentiation"
        self.results_dir = self.cache_dir / "scaling_results"
        self.results_dir.mkdir(exist_ok=True)
        
        # ãƒ­ã‚°è¨­å®š
        self.setup_logging()
        
        # ãƒ†ã‚¹ãƒˆã‚¹ã‚±ãƒ¼ãƒ«è¨­å®š
        self.scaling_tests = {
            "1x": {"articles_per_query": 20, "description": "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"},
            "4x": {"articles_per_query": 80, "description": "4å€ã‚¹ã‚±ãƒ¼ãƒ«"},
            "8x": {"articles_per_query": 160, "description": "8å€ã‚¹ã‚±ãƒ¼ãƒ«"},
            "12x": {"articles_per_query": 240, "description": "12å€ã‚¹ã‚±ãƒ¼ãƒ«"},
            "16x": {"articles_per_query": 320, "description": "16å€ã‚¹ã‚±ãƒ¼ãƒ«"}
        }
        
        self.results = {}
        
    def setup_logging(self):
        """ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.results_dir / f"large_scale_test_log_{timestamp}.txt"
        
        # ãƒ­ã‚¬ãƒ¼è¨­å®š
        self.logger = logging.getLogger('ScalingTest')
        self.logger.setLevel(logging.INFO)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¨­å®š
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¿½åŠ 
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        self.log_file = log_file
        self.logger.info(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–: {log_file}")
        
    def log_and_print(self, message: str, level: str = "INFO"):
        """ãƒ­ã‚°å‡ºåŠ›ã¨ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º"""
        if level == "INFO":
            self.logger.info(message)
        elif level == "ERROR":
            self.logger.error(message)
        elif level == "WARNING":
            self.logger.warning(message)
        else:
            self.logger.info(message)
    
    def run_single_scale_test(self, scale: str, articles_per_query: int, workers: int = 4) -> dict:
        """å˜ä¸€ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆãƒ­ã‚°å¯¾å¿œï¼‰"""
        
        self.log_and_print(f"{'='*70}")
        self.log_and_print(f"ğŸ”¬ SCALING TEST: {scale} ({articles_per_query} articles/query)")
        self.log_and_print(f"{'='*70}")
        
        test_start = time.time()
        
        try:
            # 1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            self.log_and_print("ğŸ§¬ Phase 1: System Initialization")
            raptor_tree = ImmuneCellRAPTORTree(str(self.cache_dir))
            
            # å…ç–«ç´°èƒéšå±¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            immune_file = self.cache_dir / "immune_cell_hierarchy.json"
            raptor_tree.load_immune_hierarchy(str(immune_file))
            self.log_and_print(f"âœ“ Loaded {len(raptor_tree.nodes)} immune cell nodes")
            
            # 2. FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            self.log_and_print("âš¡ Phase 2: FAISS Index Construction")
            index_start = time.time()
            raptor_tree.build_faiss_index_parallel(workers=workers)
            index_time = time.time() - index_start
            self.log_and_print(f"âœ“ FAISS index built in {index_time:.1f}s")
            
            # 3. æœ€é©åŒ–ä¸¦åˆ—PubMedçµ±åˆ
            self.log_and_print(f"ğŸ“¡ Phase 3: Scaled PubMed Integration ({articles_per_query} articles/query)")
            integration_start = time.time()
            
            # PubMedçµ±åˆå®Ÿè¡Œï¼ˆãƒ­ã‚°è¨˜éŒ²ï¼‰
            parallel_metrics = optimize_immune_raptor_parallel(
                raptor_tree,
                max_articles_per_query=articles_per_query,
                max_workers=workers
            )
            
            integration_time = time.time() - integration_start
            
            # çµ±åˆçµæœãƒ­ã‚°
            total_articles = len(raptor_tree.pubmed_articles)
            self.log_and_print(f"âœ“ PubMed integration completed: {total_articles} articles in {integration_time:.1f}s")
            
            if parallel_metrics:
                self.log_and_print(f"   Retrieval time: {parallel_metrics.get('retrieval_time', 0):.1f}s")
                self.log_and_print(f"   Encoding time: {parallel_metrics.get('encoding_time', 0):.1f}s")
                self.log_and_print(f"   Processing rate: {parallel_metrics.get('articles_processed', 0) / integration_time:.1f} articles/second")
            
            # 4. ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
            self.log_and_print("ğŸ” Phase 4: System Testing")
            search_start = time.time()
            
            test_queries = [
                "FOXP3+ regulatory T cell differentiation",
                "Treg immune suppression mechanisms",
                "thymic versus peripheral Treg development"
            ]
            
            search_results = []
            for i, query in enumerate(test_queries, 1):
                try:
                    results = raptor_tree.hierarchical_search(query, top_k=5)
                    search_results.append(len(results))
                    self.log_and_print(f"   Test {i}: '{query[:50]}...' - {len(results)} results")
                except Exception as e:
                    self.log_and_print(f"   Test {i} error: {e}", "ERROR")
                    search_results.append(0)
            
            search_time = time.time() - search_start
            total_time = time.time() - test_start
            
            # 5. çµæœåé›†ã¨ãƒ­ã‚°
            test_result = {
                'scale': scale,
                'articles_per_query': articles_per_query,
                'total_articles': total_articles,
                'total_time': total_time,
                'index_time': index_time,
                'integration_time': integration_time,
                'search_time': search_time,
                'workers_used': workers,
                'articles_per_second': total_articles / integration_time if integration_time > 0 else 0,
                'search_results_count': sum(search_results),
                'parallel_metrics': parallel_metrics,
                'timestamp': datetime.now().isoformat(),
                'status': 'SUCCESS'
            }
            
            # æˆåŠŸãƒ­ã‚°
            self.log_and_print(f"âœ… {scale} SCALE TEST COMPLETED SUCCESSFULLY")
            self.log_and_print(f"   Total execution time: {total_time:.1f}s")
            self.log_and_print(f"   Articles processed: {total_articles}")
            self.log_and_print(f"   Processing rate: {test_result['articles_per_second']:.1f} articles/second")
            self.log_and_print(f"   Search test results: {sum(search_results)}")
            
            return test_result
            
        except Exception as e:
            error_time = time.time() - test_start
            self.log_and_print(f"âŒ {scale} SCALE TEST FAILED: {str(e)}", "ERROR")
            
            # ã‚¨ãƒ©ãƒ¼çµæœ
            return {
                'scale': scale,
                'articles_per_query': articles_per_query,
                'error': str(e),
                'execution_time': error_time,
                'timestamp': datetime.now().isoformat(),
                'status': 'FAILED'
            }
    
    def run_comprehensive_scaling_test(self, max_workers: int = 4):
        """åŒ…æ‹¬çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆãƒ­ã‚°è¨˜éŒ²ï¼‰"""
        
        self.log_and_print("ğŸš€ COMPREHENSIVE LARGE-SCALE SCALING TEST STARTED")
        self.log_and_print("=" * 80)
        self.log_and_print(f"ğŸ’» System: {mp.cpu_count()} cores available, using {max_workers} workers")
        self.log_and_print(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.log_and_print(f"ğŸ“ Log file: {self.log_file}")
        
        test_start_time = time.time()
        
        # å…¨ã‚¹ã‚±ãƒ¼ãƒ«ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        for i, (scale, config) in enumerate(self.scaling_tests.items(), 1):
            self.log_and_print(f"\\nğŸ“Š Progress: Test {i}/{len(self.scaling_tests)} - {scale} scale")
            
            try:
                result = self.run_single_scale_test(
                    scale=scale,
                    articles_per_query=config["articles_per_query"],
                    workers=max_workers
                )
                self.results[scale] = result
                
                # ä¸­é–“çµæœä¿å­˜
                self.save_intermediate_results()
                
                # é€²æ—ãƒ­ã‚°
                if result['status'] == 'SUCCESS':
                    self.log_and_print(f"âœ… {scale} test completed successfully")
                else:
                    self.log_and_print(f"âŒ {scale} test failed: {result.get('error', 'Unknown error')}")
                
            except Exception as e:
                self.log_and_print(f"âŒ Critical error in {scale} scale test: {e}", "ERROR")
                self.results[scale] = {
                    'scale': scale,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat(),
                    'status': 'CRITICAL_FAILED'
                }
        
        # æœ€çµ‚åˆ†æã¨ãƒ­ã‚°
        total_test_time = time.time() - test_start_time
        self.log_and_print(f"\\nâ±ï¸ Total testing time: {total_test_time:.1f}s")
        
        self.analyze_scaling_performance()
        self.generate_scaling_report()
        
        return self.results
    
    def save_intermediate_results(self):
        """ä¸­é–“çµæœã®ä¿å­˜ï¼ˆãƒ­ã‚°è¨˜éŒ²ï¼‰"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = self.results_dir / f"large_scale_results_{timestamp}.json"
        
        try:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            
            self.log_and_print(f"ğŸ’¾ Intermediate results saved: {results_file.name}")
            
        except Exception as e:
            self.log_and_print(f"âŒ Failed to save results: {e}", "ERROR")
    
    def analyze_scaling_performance(self):
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ€§èƒ½ã®åˆ†æï¼ˆãƒ­ã‚°è¨˜éŒ²ï¼‰"""
        
        self.log_and_print("\\n" + "=" * 80)
        self.log_and_print("ğŸ“ˆ COMPREHENSIVE SCALING PERFORMANCE ANALYSIS")
        self.log_and_print("=" * 80)
        
        # æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆã®ã¿åˆ†æ
        valid_results = {k: v for k, v in self.results.items() if v.get('status') == 'SUCCESS'}
        failed_tests = {k: v for k, v in self.results.items() if v.get('status') != 'SUCCESS'}
        
        self.log_and_print(f"ğŸ“Š Test summary: {len(valid_results)} successful, {len(failed_tests)} failed")
        
        if failed_tests:
            self.log_and_print("âŒ Failed tests:")
            for scale, result in failed_tests.items():
                self.log_and_print(f"   {scale}: {result.get('error', 'Unknown error')}")
        
        if len(valid_results) < 2:
            self.log_and_print("âŒ Insufficient valid results for analysis")
            return
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡è¨ˆç®—
        baseline_key = "1x"
        if baseline_key not in valid_results:
            baseline_key = min(valid_results.keys())
        
        baseline = valid_results[baseline_key]
        baseline_time = baseline['total_time']
        baseline_articles = baseline['total_articles']
        
        self.log_and_print(f"ğŸ“Š Baseline ({baseline_key}): {baseline_time:.1f}s for {baseline_articles} articles")
        self.log_and_print("\\nğŸ“‹ Detailed Scaling Analysis:")
        self.log_and_print("-" * 80)
        
        scaling_analysis = {}
        
        for scale, result in valid_results.items():
            if scale == baseline_key:
                continue
                
            scale_factor = int(scale.replace('x', ''))
            time_ratio = result['total_time'] / baseline_time
            articles_ratio = result['total_articles'] / baseline_articles
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡
            time_efficiency = scale_factor / time_ratio
            throughput = result['articles_per_second']
            baseline_throughput = baseline['articles_per_second']
            throughput_improvement = throughput / baseline_throughput if baseline_throughput > 0 else 1
            
            analysis = {
                'scale_factor': scale_factor,
                'time_ratio': time_ratio,
                'articles_ratio': articles_ratio,
                'time_efficiency': time_efficiency,
                'throughput': throughput,
                'throughput_improvement': throughput_improvement
            }
            
            scaling_analysis[scale] = analysis
            
            # è©³ç´°ãƒ­ã‚°
            self.log_and_print(f"{scale:>3} Scale:")
            self.log_and_print(f"   Time: {result['total_time']:>6.1f}s (ratio: {time_ratio:>4.1f}x)")
            self.log_and_print(f"   Articles: {result['total_articles']:>4} (ratio: {articles_ratio:>4.1f}x)")
            self.log_and_print(f"   Efficiency: {time_efficiency*100:>5.1f}%")
            self.log_and_print(f"   Throughput: {throughput:>5.1f} art/s (improvement: {throughput_improvement:>4.1f}x)")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        if scaling_analysis:
            best_efficiency = max(scaling_analysis.values(), key=lambda x: x['time_efficiency'])
            best_throughput = max(scaling_analysis.values(), key=lambda x: x['throughput'])
            
            self.log_and_print("\\nğŸ† PERFORMANCE HIGHLIGHTS:")
            
            best_eff_scale = [k for k, v in scaling_analysis.items() if v == best_efficiency][0]
            best_thr_scale = [k for k, v in scaling_analysis.items() if v == best_throughput][0]
            
            self.log_and_print(f"   ğŸ¥‡ Best time efficiency: {best_efficiency['time_efficiency']*100:.1f}% at {best_eff_scale} scale")
            self.log_and_print(f"   ğŸš€ Best throughput: {best_throughput['throughput']:.1f} articles/second at {best_thr_scale} scale")
            
            # ã‚µãƒ–ãƒªãƒ‹ã‚¢æ€§èƒ½ã®è©•ä¾¡
            avg_efficiency = sum(a['time_efficiency'] for a in scaling_analysis.values()) / len(scaling_analysis)
            
            if avg_efficiency > 0.8:
                performance_rating = "ğŸŒŸ EXCELLENT"
            elif avg_efficiency > 0.6:
                performance_rating = "âœ… GOOD"
            elif avg_efficiency > 0.4:
                performance_rating = "âš ï¸ FAIR"
            else:
                performance_rating = "âŒ POOR"
            
            self.log_and_print(f"   ğŸ“Š Average efficiency: {avg_efficiency*100:.1f}% - {performance_rating}")
        
        self.scaling_analysis = scaling_analysis
    
    def generate_scaling_report(self):
        """è©³ç´°ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆãƒ­ã‚°è¨˜éŒ²ï¼‰"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.results_dir / f"large_scale_report_{timestamp}.md"
        
        try:
            report_lines = []
            report_lines.append("# Large-Scale Scaling Test Report")
            report_lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_lines.append("")
            
            # ãƒ†ã‚¹ãƒˆè¨­å®š
            report_lines.append("## Test Configuration")
            report_lines.append(f"- System: {mp.cpu_count()} CPU cores")
            report_lines.append(f"- Workers: 4 (optimized for PubMed API)")
            report_lines.append(f"- Scales tested: {', '.join(self.scaling_tests.keys())}")
            report_lines.append(f"- Log file: {self.log_file.name}")
            report_lines.append("")
            
            # çµæœã‚µãƒãƒªãƒ¼
            valid_results = {k: v for k, v in self.results.items() if v.get('status') == 'SUCCESS'}
            failed_results = {k: v for k, v in self.results.items() if v.get('status') != 'SUCCESS'}
            
            report_lines.append("## Results Summary")
            report_lines.append(f"- Successful tests: {len(valid_results)}")
            report_lines.append(f"- Failed tests: {len(failed_results)}")
            report_lines.append("")
            
            if valid_results:
                report_lines.append("### Successful Tests")
                report_lines.append("| Scale | Articles/Query | Total Articles | Time (s) | Rate (art/s) | Efficiency |")
                report_lines.append("|-------|---------------|---------------|----------|--------------|------------|")
                
                for scale, result in valid_results.items():
                    efficiency = ""
                    if hasattr(self, 'scaling_analysis') and scale in self.scaling_analysis:
                        efficiency = f"{self.scaling_analysis[scale]['time_efficiency']*100:.1f}%"
                    
                    report_lines.append(f"| {scale} | {result['articles_per_query']} | {result['total_articles']} | {result['total_time']:.1f} | {result['articles_per_second']:.1f} | {efficiency} |")
            
            if failed_results:
                report_lines.append("")
                report_lines.append("### Failed Tests")
                for scale, result in failed_results.items():
                    report_lines.append(f"- {scale}: {result.get('error', 'Unknown error')}")
            
            # è©³ç´°åˆ†æ
            if hasattr(self, 'scaling_analysis') and self.scaling_analysis:
                report_lines.append("")
                report_lines.append("## Detailed Performance Analysis")
                
                for scale, analysis in self.scaling_analysis.items():
                    report_lines.append(f"### {scale} Scale")
                    report_lines.append(f"- Scale factor: {analysis['scale_factor']}x")
                    report_lines.append(f"- Time ratio: {analysis['time_ratio']:.1f}x")
                    report_lines.append(f"- Articles ratio: {analysis['articles_ratio']:.1f}x")
                    report_lines.append(f"- Time efficiency: {analysis['time_efficiency']*100:.1f}%")
                    report_lines.append(f"- Throughput improvement: {analysis['throughput_improvement']:.1f}x")
                    report_lines.append("")
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("\\n".join(report_lines))
            
            self.log_and_print(f"ğŸ“„ Comprehensive scaling report generated: {report_file.name}")
            
        except Exception as e:
            self.log_and_print(f"âŒ Failed to generate report: {e}", "ERROR")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸš€ LARGE-SCALE SCALING TEST WITH COMPREHENSIVE LOGGING")
    print("=" * 80)
    
    # ãƒ†ã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–
    tester = LoggedScalingTester()
    
    # å¤§è¦æ¨¡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    try:
        print("ğŸ”¬ Starting comprehensive scaling tests...")
        results = tester.run_comprehensive_scaling_test(max_workers=4)
        
        tester.log_and_print("\\n" + "="*80)
        tester.log_and_print("ğŸ¯ LARGE-SCALE SCALING TEST COMPLETED!")
        tester.log_and_print("="*80)
        
        successful_tests = sum(1 for r in results.values() if r.get('status') == 'SUCCESS')
        tester.log_and_print(f"âœ… Final summary: {successful_tests}/{len(results)} tests successful")
        tester.log_and_print(f"ğŸ“ Results and logs saved in: {tester.results_dir}")
        tester.log_and_print(f"ğŸ“ Main log file: {tester.log_file}")
        
    except KeyboardInterrupt:
        print("\\nâš ï¸  Test interrupted by user")
    except Exception as e:
        print(f"\\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()