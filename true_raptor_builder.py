#!/usr/bin/env python3
"""
True RAPTOR Tree Implementation with Clustering
Implements the full RAPTOR algorithm with recursive clustering and summarization
"""

import os
# Hugging Faceé«˜é€Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

import json
import pickle
import numpy as np
import faiss
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import torch
from transformers import (
    AutoModel, AutoTokenizer, AutoModelForCausalLM, 
    GPT2LMHeadModel, GPT2Tokenizer, GPTNeoXForCausalLM, OPTForCausalLM
)
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import logging
import time
from datetime import datetime
import os

# Hugging Face ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®š
os.environ["TRANSFORMERS_VERBOSITY"] = "info"  # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—è¡¨ç¤º

@dataclass
class RAPTORNode:
    """RAPTOR Tree Node with clustering support"""
    node_id: str
    parent_id: Optional[str]
    children: List[str]
    level: int
    content: str
    summary: str
    is_leaf: bool
    cluster_id: Optional[int]
    embedding: Optional[np.ndarray]
    source_documents: List[str]
    cluster_size: int = 0

class TrueRAPTORTree:
    """True RAPTOR Tree with Transformers-based embeddings and local LLM"""
    
    def __init__(self):
        # Transformersãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆæ—¢å­˜ã®ä¾å­˜é–¢ä¿‚ã‚’ä½¿ç”¨ï¼‰
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.embedding_model_name)
            self.embedding_model = AutoModel.from_pretrained(self.embedding_model_name).to(self.device)
            self.embedding_model.eval()
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ˆã‚ŠåŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«
            self.embedding_model_name = "distilbert-base-uncased"
            self.tokenizer = AutoTokenizer.from_pretrained(self.embedding_model_name)
            self.embedding_model = AutoModel.from_pretrained(self.embedding_model_name).to(self.device)
            self.embedding_model.eval()
        
        # ãƒ­ã‚°è¨­å®šï¼ˆæœ€åˆã«è¨­å®šï¼‰
        self.logger = logging.getLogger(__name__)
        
        # ãƒ­ãƒ¼ã‚«ãƒ«LLMåˆæœŸåŒ–ï¼ˆè¦ç´„ç”¨ï¼‰
        self.llm_tokenizer = None
        self.llm_model = None
        self._init_local_llm()
        
        self.nodes: Dict[str, RAPTORNode] = {}
        self.faiss_index = None
        self.article_embeddings = {}
        self.max_cluster_size = 30  # å‰Šæ¸›ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ‘åˆ¶
        self.min_cluster_size = 3
        self.max_levels = 4
        
    def _init_local_llm(self):
        """GPUå¯¾å¿œã®å¤§è¦æ¨¡OSSãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆè¦ç´„ç”¨ï¼‰"""
        try:
            # GPUä½¿ç”¨é‡ã‚’ç¢ºèª
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                self.logger.info(f"ğŸš€ GPU detected: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
                
                # GPUå®¹é‡ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
                if gpu_memory >= 24:  # 24GBä»¥ä¸Šã®å ´åˆ - å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«
                    llm_model_name = "facebook/opt-6.7b"  # OPT 6.7B
                    self.logger.info("ğŸ”¥ Using OPT-6.7B for GPU with 24GB+ memory")
                elif gpu_memory >= 16:  # 16GBä»¥ä¸Šã®å ´åˆ
                    llm_model_name = "facebook/opt-2.7b"  # OPT 2.7B
                    self.logger.info("ğŸš€ Using OPT-2.7B for GPU with 16GB+ memory")
                elif gpu_memory >= 12:  # 12GBä»¥ä¸Šã®å ´åˆ
                    llm_model_name = "facebook/opt-1.3b"  # OPT 1.3B
                    self.logger.info("âš¡ Using OPT-1.3B for GPU with 12GB+ memory")
                elif gpu_memory >= 8:  # 8GBä»¥ä¸Šã®å ´åˆ
                    llm_model_name = "microsoft/DialoGPT-large"
                    self.logger.info("ğŸ’ª Using DialoGPT-large for GPU with 8GB+ memory")
                else:  # 8GBæœªæº€ã®å ´åˆ
                    llm_model_name = "microsoft/DialoGPT-medium"
                    self.logger.info("ğŸ’¡ Using DialoGPT-medium for GPU with <8GB memory")
            else:
                llm_model_name = "distilgpt2"
                self.logger.info("ğŸ’» No GPU available, using CPU-optimized model")
            
            # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆGPUå¯¾å¿œï¼‰
            self.logger.info(f"ğŸ“¥ Downloading tokenizer for {llm_model_name}...")
            self.llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
            
            # GPUä½¿ç”¨æ™‚ã®æœ€é©åŒ–
            if torch.cuda.is_available():
                self.logger.info(f"ğŸ“¥ Downloading model {llm_model_name} (GPU-optimized)...")
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    llm_model_name,
                    torch_dtype=torch.float16,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
                    device_map="auto",  # è‡ªå‹•GPUé…ç½®
                    low_cpu_mem_usage=True  # CPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›
                ).to(self.device)
            else:
                self.logger.info(f"ğŸ“¥ Downloading model {llm_model_name} (CPU mode)...")
                self.llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name).to(self.device)
            
            self.llm_model.eval()
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š
            if self.llm_tokenizer.pad_token is None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token
                
            self.logger.info(f"âœ… Large-scale LLM initialized: {llm_model_name}")
            self.logger.info(f"ğŸ¯ Device: {self.device}")
            
            # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                cached = torch.cuda.memory_reserved() / 1024**3
                self.logger.info(f"ğŸ“Š GPU Memory - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Large-scale LLM initialization failed: {e}")
            self.logger.info("ğŸ“ Falling back to template-based summarization")
            self.llm_model = None
            self.llm_tokenizer = None
        
    def encode_text(self, text: str) -> np.ndarray:
        """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            padding=True, 
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.embedding_model(**inputs)
            # [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        
        return embedding.flatten()
        
    def encode_documents(self, documents: List[str]) -> np.ndarray:
        """æ–‡æ›¸ç¾¤ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰"""
        self.logger.info(f"ğŸ”¤ Encoding {len(documents)} documents using {self.embedding_model_name}...")
        
        embeddings = []
        batch_size = 8  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è€ƒæ…®
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            batch_embeddings = []
            
            for doc in batch:
                try:
                    # é•·ã„æ–‡æ›¸ã¯åˆ‡ã‚Šè©°ã‚ã‚‹
                    truncated_doc = doc[:1000] if len(doc) > 1000 else doc
                    embedding = self.encode_text(truncated_doc)
                    batch_embeddings.append(embedding)
                except Exception as e:
                    self.logger.warning(f"Encoding error for document: {e}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
                    embedding_dim = 384 if "MiniLM" in self.embedding_model_name else 768
                    batch_embeddings.append(np.zeros(embedding_dim))
            
            embeddings.extend(batch_embeddings)
            self.logger.info(f"  âœ“ Encoded batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}")
        
        return np.array(embeddings)
    
    def optimal_clusters(self, embeddings: np.ndarray, max_k: int = 10) -> int:
        """æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ±ºå®šï¼ˆã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æï¼‰"""
        if len(embeddings) < 2:
            return 1
        
        max_k = min(max_k, len(embeddings) - 1)
        if max_k < 2:
            return 1
            
        best_k = 2
        best_score = -1
        
        for k in range(2, max_k + 1):
            try:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(embeddings)
                score = silhouette_score(embeddings, cluster_labels)
                
                if score > best_score:
                    best_score = score
                    best_k = k
            except:
                continue
                
        return best_k
    
    def cluster_documents(self, embeddings: np.ndarray, documents: List[str]) -> Dict[int, List[int]]:
        """æ–‡æ›¸ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        if len(documents) <= self.min_cluster_size:
            return {0: list(range(len(documents)))}
        
        n_clusters = self.optimal_clusters(embeddings, max_k=min(10, len(documents) // 2))
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(embeddings)
        
        clusters = {}
        for idx, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(idx)
        
        return clusters
    
    def generate_llm_summary(self, documents: List[str]) -> str:
        """GPUå¯¾å¿œã®å¤§è¦æ¨¡LLMã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ç´„ã‚’ç”Ÿæˆ"""
        if not self.llm_model or not self.llm_tokenizer:
            # LLMãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹è¦ç´„
            return self._template_based_summary(documents)
        
        try:
            # GPUä½¿ç”¨é‡ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
            if torch.cuda.is_available():
                torch.cuda.empty_cache()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            
            # æ–‡æ›¸ã‚’çµåˆï¼ˆå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨ã«ã‚ˆã‚Šå¤šãã®æƒ…å ±ï¼‰
            combined_text = " ".join(doc[:200] for doc in documents[:5])  # æœ€å¤§5æ–‡æ›¸ã€å„200æ–‡å­—
            
            # å…ç–«å­¦å°‚ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = f"""Summarize the following immune cell research findings in a concise scientific manner.
Focus on key mechanisms, cell types, and biological processes.

Research findings: {combined_text}

Scientific summary:"""
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆGPUæœ€é©åŒ–ï¼‰
            inputs = self.llm_tokenizer.encode(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=800,  # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨ã«ã‚ˆã‚Šé•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                padding=True
            )
            inputs = inputs.to(self.device)
            
            # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨æœ€é©åŒ–ï¼‰
            generation_kwargs = {
                "max_new_tokens": 100,  # ã‚ˆã‚Šé•·ã„è¦ç´„
                "temperature": 0.7,
                "do_sample": True,
                "top_p": 0.9,  # nucleus sampling
                "top_k": 50,
                "repetition_penalty": 1.1,
                "pad_token_id": self.llm_tokenizer.eos_token_id,
                "attention_mask": torch.ones_like(inputs),
                "no_repeat_ngram_size": 3  # ç¹°ã‚Šè¿”ã—é˜²æ­¢
            }
            
            # GPUä½¿ç”¨æ™‚ã®è¿½åŠ æœ€é©åŒ–
            if torch.cuda.is_available():
                generation_kwargs["use_cache"] = True
            
            # ç”Ÿæˆå®Ÿè¡Œ
            with torch.no_grad():
                outputs = self.llm_model.generate(inputs, **generation_kwargs)
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»ã—ã¦è¦ç´„éƒ¨åˆ†ã®ã¿æŠ½å‡º
            if "Scientific summary:" in generated_text:
                summary = generated_text.split("Scientific summary:")[-1].strip()
            else:
                summary = generated_text[len(prompt):].strip()
            
            # è¦ç´„ã®å¾Œå‡¦ç†
            summary = self._post_process_summary(summary)
            
            # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # è¦ç´„ãŒçŸ­ã™ãã‚‹å ´åˆã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if len(summary) < 20:
                self.logger.warning("Generated summary too short, using template-based fallback")
                return self._template_based_summary(documents)
            
            return summary[:400]  # æœ€å¤§400æ–‡å­—
            
        except Exception as e:
            self.logger.warning(f"GPU LLM summary generation failed: {e}")
            # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return self._template_based_summary(documents)
    
    def _post_process_summary(self, summary: str) -> str:
        """ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã®å¾Œå‡¦ç†"""
        # ä¸å®Œå…¨ãªæ–‡ã‚’é™¤å»
        sentences = summary.split('.')
        complete_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and not sentence.endswith(('and', 'or', 'the', 'a', 'an')):
                complete_sentences.append(sentence)
        
        processed_summary = '. '.join(complete_sentences)
        if processed_summary and not processed_summary.endswith('.'):
            processed_summary += '.'
        
        return processed_summary
    
    def _template_based_summary(self, documents: List[str]) -> str:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®è¦ç´„ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        if len(documents) == 1:
            return documents[0][:400] + "..." if len(documents[0]) > 400 else documents[0]
        
        # å„æ–‡æ›¸ã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        immune_keywords = ["T cell", "B cell", "FOXP3", "IL-10", "TGF-Î²", "CTLA-4", "regulatory", "immune", "differentiation"]
        
        summaries = []
        found_keywords = set()
        
        for i, doc in enumerate(documents[:5]):  # æœ€å¤§5æ–‡æ›¸
            # æ–‡æ›¸ã®æœ€åˆã®éƒ¨åˆ†ã‚’å–å¾—
            summary = doc[:120].strip()
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
            for keyword in immune_keywords:
                if keyword.lower() in doc.lower():
                    found_keywords.add(keyword)
            
            if summary:
                summaries.append(f"Doc{i+1}: {summary}")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®èª¬æ˜ã‚’è¿½åŠ 
        keyword_desc = f"[Keywords: {', '.join(sorted(found_keywords))}]" if found_keywords else ""
        
        # åŸºæœ¬çš„ãªçµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        combined_summary = " | ".join(summaries)
        cluster_info = f"[Cluster of {len(documents)} immune cell research documents] {keyword_desc} {combined_summary}"
        
        # æœ€å¤§é•·åˆ¶é™
        return cluster_info[:500] + "..." if len(cluster_info) > 500 else cluster_info
    
    def summarize_cluster(self, documents: List[str]) -> str:
        """ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ç´„ã‚’ç”Ÿæˆï¼ˆLLMã¾ãŸã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰"""
        return self.generate_llm_summary(documents)
    
    def build_raptor_tree(self, documents: List[str], document_ids: List[str]) -> None:
        """çœŸã®RAPTORãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰"""
        self.logger.info(f"ğŸŒ³ RAPTOR Tree construction started with {len(documents)} documents")
        
        # ãƒ¬ãƒ™ãƒ«0: ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼ˆå…ƒæ–‡æ›¸ï¼‰
        current_level_docs = documents
        current_level_ids = document_ids
        level = 0
        
        # å…¨ä½“ã®åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—
        all_embeddings = self.encode_documents(current_level_docs)
        
        while len(current_level_docs) > 1 and level < self.max_levels:
            self.logger.info(f"ğŸ“Š Processing level {level}: {len(current_level_docs)} nodes")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            clusters = self.cluster_documents(all_embeddings[:len(current_level_docs)], current_level_docs)
            
            next_level_docs = []
            next_level_ids = []
            next_level_embeddings = []
            
            for cluster_id, doc_indices in clusters.items():
                if len(doc_indices) == 0:
                    continue
                    
                cluster_docs = [current_level_docs[i] for i in doc_indices]
                cluster_doc_ids = [current_level_ids[i] for i in doc_indices]
                
                # ã‚¯ãƒ©ã‚¹ã‚¿è¦ç´„ã‚’ç”Ÿæˆ
                cluster_summary = self.summarize_cluster(cluster_docs)
                
                # æ–°ã—ã„ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
                node_id = f"raptor_L{level + 1}_C{cluster_id}_{int(time.time())}"
                
                # è¦ç´„ã®åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—
                summary_embedding = self.encode_text(cluster_summary)
                
                # ãƒãƒ¼ãƒ‰ã‚’ä¿å­˜
                node = RAPTORNode(
                    node_id=node_id,
                    parent_id=None,  # è¦ªã¯å¾Œã§è¨­å®š
                    children=cluster_doc_ids if level == 0 else [],
                    level=level + 1,
                    content=cluster_summary,
                    summary=cluster_summary,
                    is_leaf=False,
                    cluster_id=cluster_id,
                    embedding=summary_embedding,
                    source_documents=cluster_doc_ids,
                    cluster_size=len(cluster_docs)
                )
                
                self.nodes[node_id] = node
                
                # æ¬¡ã®ãƒ¬ãƒ™ãƒ«ã®æº–å‚™
                next_level_docs.append(cluster_summary)
                next_level_ids.append(node_id)
                next_level_embeddings.append(summary_embedding)
                
                self.logger.info(f"  âœ“ Cluster {cluster_id}: {len(cluster_docs)} docs â†’ {node_id}")
            
            # ãƒ¬ãƒ™ãƒ«ã‚¢ãƒƒãƒ—
            current_level_docs = next_level_docs
            current_level_ids = next_level_ids
            if next_level_embeddings:
                all_embeddings = np.array(next_level_embeddings)
            level += 1
            
            # å˜ä¸€ãƒãƒ¼ãƒ‰ã«ãªã£ãŸã‚‰çµ‚äº†
            if len(current_level_docs) <= 1:
                break
        
        # ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ä½œæˆ
        if current_level_docs:
            root_summary = self.summarize_cluster(current_level_docs)
            root_id = f"raptor_root_{int(time.time())}"
            
            root_node = RAPTORNode(
                node_id=root_id,
                parent_id=None,
                children=current_level_ids,
                level=level + 1,
                content=root_summary,
                summary=root_summary,
                is_leaf=False,
                cluster_id=0,
                embedding=self.encode_text(root_summary),
                source_documents=document_ids,
                cluster_size=len(documents)
            )
            
            self.nodes[root_id] = root_node
            self.logger.info(f"ğŸŒŸ Root node created: {root_id}")
        
        self.logger.info(f"âœ… RAPTOR Tree completed: {len(self.nodes)} total nodes across {level + 1} levels")
    
    def save_tree(self, output_path: str) -> None:
        """ãƒ„ãƒªãƒ¼ã‚’ä¿å­˜ï¼ˆJSON serializableå½¢å¼ã§ï¼‰"""
        tree_data = {
            'nodes': {},
            'metadata': {
                'creation_time': datetime.now().isoformat(),
                'total_nodes': len(self.nodes),
                'levels': max(node.level for node in self.nodes.values()) if self.nodes else 0,
                'algorithm': 'RAPTOR with Local LLM and Clustering'
            }
        }
        
        for node_id, node in self.nodes.items():
            tree_data['nodes'][node_id] = {
                'node_id': node.node_id,
                'parent_id': node.parent_id,
                'children': node.children,
                'level': int(node.level),  # numpyå‹ã‚’Python intã«å¤‰æ›
                'content': node.content,
                'summary': node.summary,
                'is_leaf': bool(node.is_leaf),  # numpy boolã‚’Python boolã«å¤‰æ›
                'cluster_id': int(node.cluster_id) if node.cluster_id is not None else None,
                'source_documents': node.source_documents,
                'cluster_size': int(node.cluster_size),
                'embedding': node.embedding.tolist() if node.embedding is not None else None
            }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(tree_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ RAPTOR Tree saved: {output_path}")

class TrueRAPTORBuilder:
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’ä½¿ç”¨ã—ãŸçœŸã®RAPTORã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…ãƒ“ãƒ«ãƒ€ãƒ¼"""
    
    def __init__(self):
        self.raptor_tree = TrueRAPTORTree()
        self.setup_logging()
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_dir = Path("data/immune_cell_differentiation/scaling_results")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        log_file = log_dir / f"raptor_local_llm_build_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"RAPTOR with Local LLM Builder Log initialized: {log_file}")
    
    def create_sample_documents(self) -> Tuple[List[str], List[str]]:
        """å…ç–«ç´°èƒç ”ç©¶ã®ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã‚’ä½œæˆ"""
        documents = [
            "Hematopoietic stem cells (HSCs) are multipotent progenitor cells that reside in the bone marrow and give rise to all blood cell lineages through tightly regulated differentiation processes.",
            "Common lymphoid progenitors (CLPs) express key surface markers including IL-7RÎ± and Sca-1, and represent the earliest committed precursors of the lymphoid lineage.",
            "CD4+ T helper cells differentiate from naive T cells upon antigen recognition and provide crucial support for immune responses through cytokine production and cellular interactions.",
            "Natural regulatory T cells (nTregs) develop in the thymus and constitutively express the transcription factor FOXP3, which is essential for their suppressive function.",
            "Induced regulatory T cells (iTregs) can be generated from conventional CD4+ T cells in peripheral tissues through exposure to TGF-Î² and other immunosuppressive signals.",
            "FOXP3 transcription factor serves as the master regulator of regulatory T cell development and function, controlling the expression of numerous target genes involved in immune suppression.",
            "Regulatory T cells employ multiple mechanisms to suppress immune responses, including contact-dependent inhibition through CTLA-4 and PD-1 signaling pathways.",
            "Cytokine production by regulatory T cells, particularly IL-10 and TGF-Î², plays crucial roles in maintaining immune tolerance and preventing autoimmune diseases.",
            "Thymic development of natural Tregs requires specific strength of TCR signaling and interactions with medullary thymic epithelial cells expressing tissue-specific antigens.",
            "Peripheral induction of regulatory T cells occurs in gut-associated lymphoid tissues and at sites of inflammation, contributing to local immune homeostasis.",
            "CTLA-4 checkpoint molecule on regulatory T cells enables competitive inhibition of CD28-mediated costimulation, thereby limiting T cell activation.",
            "Metabolic programming of regulatory T cells involves the mTOR pathway and influences the balance between glycolysis and oxidative phosphorylation.",
            "Dysfunction of regulatory T cells is implicated in various autoimmune diseases including multiple sclerosis, rheumatoid arthritis, and type 1 diabetes.",
            "Tumor-infiltrating regulatory T cells create an immunosuppressive microenvironment that facilitates cancer progression and resistance to immunotherapy.",
            "Therapeutic modulation of regulatory T cell function represents a promising approach for treating autoimmune diseases and enhancing cancer immunotherapy.",
            "Single-cell transcriptomics has revealed significant heterogeneity within regulatory T cell populations across different tissues and disease states.",
            "Epigenetic regulation through DNA methylation and histone modifications ensures stable maintenance of FOXP3 expression in regulatory T cells.",
            "Age-related changes in regulatory T cell frequency and function contribute to increased susceptibility to autoimmune diseases in elderly populations.",
            "Gut microbiota influences the development and function of peripheral regulatory T cells through production of short-chain fatty acids and other metabolites.",
            "Regulatory T cell plasticity allows adaptation to tissue-specific environments while maintaining core suppressive capabilities through context-dependent gene expression programs."
        ]
        
        document_ids = [f"immune_research_{i:03d}" for i in range(len(documents))]
        
        # è¿½åŠ ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ–‡æ›¸ã‚’ç”Ÿæˆ
        extended_docs = []
        extended_ids = []
        
        for i, doc in enumerate(documents):
            extended_docs.append(doc)
            extended_ids.append(document_ids[i])
            
            # å…ç–«å­¦çš„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            if i < 15:  # æœ€åˆã®15æ–‡æ›¸ã«ã¤ã„ã¦ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
                variant = doc.replace("cells", "cell populations").replace("function", "biological activity").replace("development", "differentiation")
                extended_docs.append(variant)
                extended_ids.append(f"immune_research_var_{i:03d}")
        
        self.logger.info(f"ğŸ“š Created {len(extended_docs)} sample immune cell research documents")
        return extended_docs, extended_ids
    
    def build_local_llm_raptor_tree(self):
        """Transformersãƒ™ãƒ¼ã‚¹ã®çœŸã®RAPTORãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰"""
        self.logger.info("ğŸš€ RAPTOR WITH LOCAL LLM CONSTRUCTION STARTED")
        self.logger.info("=" * 60)
        
        start_time = time.time()
        
        # 1. ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ä½œæˆ
        self.logger.info("ğŸ“š Phase 1: Creating Sample Documents")
        documents, document_ids = self.create_sample_documents()
        
        # 2. RAPTORãƒ„ãƒªãƒ¼æ§‹ç¯‰
        self.logger.info("ğŸŒ³ Phase 2: Building RAPTOR Tree with Local LLM Clustering")
        self.raptor_tree.build_raptor_tree(documents, document_ids)
        
        # 3. ä¿å­˜
        self.logger.info("ğŸ’¾ Phase 3: Saving RAPTOR Tree with Local LLM")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("data/immune_cell_differentiation/raptor_trees")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        tree_file = output_dir / f"raptor_local_llm_tree_{timestamp}.json"
        self.raptor_tree.save_tree(str(tree_file))
        
        total_time = time.time() - start_time
        
        # çµæœã‚µãƒãƒªãƒ¼
        self.logger.info("âœ… RAPTOR WITH LOCAL LLM CONSTRUCTION COMPLETED")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“Š Results Summary:")
        self.logger.info(f"   Total execution time: {total_time:.1f}s")
        self.logger.info(f"   Input documents: {len(documents)}")
        self.logger.info(f"   Generated nodes: {len(self.raptor_tree.nodes)}")
        if self.raptor_tree.nodes:
            self.logger.info(f"   Tree levels: {max(node.level for node in self.raptor_tree.nodes.values())}")
        self.logger.info(f"   Output file: {tree_file.name}")
        self.logger.info(f"   Model used: {self.raptor_tree.embedding_model_name}")
        if self.raptor_tree.llm_model:
            # ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
            model_name = getattr(self.raptor_tree.llm_model, 'name_or_path', 'unknown')
            if 'opt' in model_name.lower():
                self.logger.info(f"   LLM used: {model_name} (Meta OPT series)")
            elif 'neox' in model_name.lower():
                self.logger.info(f"   LLM used: {model_name} (EleutherAI GPT-NeoX)")
            elif 'dialogo' in model_name.lower():
                self.logger.info(f"   LLM used: {model_name} (Microsoft DialoGPT)")
            else:
                self.logger.info(f"   LLM used: {model_name} (GPU-accelerated)")
                
            # GPUçµ±è¨ˆæƒ…å ±
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                cached = torch.cuda.memory_reserved() / 1024**3
                self.logger.info(f"   GPU Memory: Allocated {allocated:.2f}GB, Cached {cached:.2f}GB")
        else:
            self.logger.info(f"   LLM used: template-based (fallback)")
        
        return tree_file

if __name__ == "__main__":
    builder = TrueRAPTORBuilder()
    result_file = builder.build_local_llm_raptor_tree()
    
    if result_file:
        print(f"\nâœ… RAPTOR Tree with GPU-Accelerated LLM construction completed!")
        print(f"ğŸ“ Output: {result_file}")
        print(f"ğŸ” This tree contains CLUSTERED nodes with GPU-generated summaries")
        print(f"ğŸŒ³ Each level represents hierarchical abstraction of immune cell research content")
        print(f"ğŸ“Š Now you should see MANY MORE than 5 nodes due to clustering!")
        print(f"ğŸš€ Summaries generated using GPU-accelerated large language model!")
        
        # GPUä½¿ç”¨çµ±è¨ˆã‚’è¡¨ç¤º
        if torch.cuda.is_available():
            final_allocated = torch.cuda.memory_allocated() / 1024**3
            final_cached = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ¯ Final GPU Memory: Allocated {final_allocated:.2f}GB, Cached {final_cached:.2f}GB")
            
            # GPUã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            torch.cuda.empty_cache()
            print(f"ğŸ§¹ GPU cache cleared")
        else:
            print(f"ğŸ’» Executed on CPU (no GPU available)")