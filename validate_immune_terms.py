#!/usr/bin/env python3
"""
å…ç–«å­¦ç”¨èªæ¤œè¨¼ãƒ„ãƒ¼ãƒ«
Immune Terminology Validation Tool

ç”Ÿæˆã•ã‚ŒãŸRAPTORãƒ„ãƒªãƒ¼ã®ãƒ©ãƒ™ãƒ«ãŒå…ç–«å­¦çš„ã«æ­£ã—ã„ã‹ã‚’æ¤œè¨¼
"""

import json
from pathlib import Path
from datetime import datetime
from immune_cell_vocab import (
    generate_immune_label, 
    validate_immune_terminology,
    extract_level_keywords,
    IMMUNE_HIERARCHY_VOCAB,
    LEVEL_COLOR_MAPPING
)

class ImmuneTerminologyValidator:
    """å…ç–«å­¦ç”¨èªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.validation_results = []
        self.corrections = []
    
    def load_latest_tree(self):
        """æœ€æ–°ã®RAPTORãƒ„ãƒªãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
        data_dir = Path("data/immune_cell_differentiation/raptor_trees")
        if not data_dir.exists():
            data_dir = Path(".")
            
        json_files = list(data_dir.glob("*raptor*tree*.json"))
        if not json_files:
            raise FileNotFoundError("RAPTORãƒ„ãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
        latest_file = max(json_files, key=lambda p: p.stat().st_mtime)
        print(f"ğŸ“ Loading: {latest_file}")
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            return json.load(f), latest_file.name
    
    def validate_all_nodes(self, tree_data):
        """å…¨ãƒãƒ¼ãƒ‰ã®å…ç–«å­¦ç”¨èªã‚’æ¤œè¨¼"""
        
        print("\nğŸ”¬ å…ç–«å­¦ç”¨èªæ¤œè¨¼é–‹å§‹")
        print("=" * 60)
        
        validation_summary = {
            'total_nodes': 0,
            'valid_nodes': 0,
            'invalid_nodes': 0,
            'warnings': 0,
            'level_analysis': {}
        }
        
        for node_id, node_data in tree_data['nodes'].items():
            if any(x in node_id for x in ['_L', 'root']):  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã®ã¿
                validation_summary['total_nodes'] += 1
                
                # ãƒ¬ãƒ™ãƒ«åˆ¤å®š
                if 'root' in node_id:
                    level = 4
                elif '_L3_' in node_id:
                    level = 3
                elif '_L2_' in node_id:
                    level = 2
                elif '_L1_' in node_id:
                    level = 1
                else:
                    level = 0
                
                # ãƒ¬ãƒ™ãƒ«åˆ¥çµ±è¨ˆåˆæœŸåŒ–
                if level not in validation_summary['level_analysis']:
                    validation_summary['level_analysis'][level] = {
                        'count': 0, 'valid': 0, 'invalid': 0, 'keywords_found': []
                    }
                
                validation_summary['level_analysis'][level]['count'] += 1
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºå–å¾—
                content = node_data.get('content', '')
                cluster_size = node_data.get('cluster_size', 1)
                cluster_id = node_id.split('_C')[1].split('_')[0] if '_C' in node_id else '0'
                
                # å…ç–«å­¦çš„ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
                immune_label = generate_immune_label(content, level, cluster_id, cluster_size)
                
                # ç”¨èªå¦¥å½“æ€§æ¤œè¨¼
                is_valid, validation_msg = validate_immune_terminology(immune_label)
                
                # ãƒ¬ãƒ™ãƒ«å›ºæœ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
                level_keywords = extract_level_keywords(content, level)
                validation_summary['level_analysis'][level]['keywords_found'].extend(level_keywords)
                
                # çµæœè¨˜éŒ²
                result = {
                    'node_id': node_id,
                    'level': level,
                    'original_content_preview': content[:100] + "..." if len(content) > 100 else content,
                    'generated_label': immune_label,
                    'is_valid': is_valid,
                    'validation_message': validation_msg,
                    'keywords_found': level_keywords,
                    'cluster_size': cluster_size
                }
                
                self.validation_results.append(result)
                
                if is_valid:
                    validation_summary['valid_nodes'] += 1
                    validation_summary['level_analysis'][level]['valid'] += 1
                    status = "âœ…"
                else:
                    validation_summary['invalid_nodes'] += 1
                    validation_summary['level_analysis'][level]['invalid'] += 1
                    status = "âŒ"
                
                print(f"{status} {node_id}")
                print(f"   ãƒ¬ãƒ™ãƒ«: {level} ({LEVEL_COLOR_MAPPING.get(level, {}).get('description', 'Unknown')})")
                print(f"   ãƒ©ãƒ™ãƒ«: {immune_label}")
                print(f"   æ¤œè¨¼: {validation_msg}")
                if level_keywords:
                    print(f"   ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(level_keywords[:5])}")
                print("-" * 60)
        
        return validation_summary
    
    def generate_corrections(self):
        """ä¿®æ­£ææ¡ˆã®ç”Ÿæˆ"""
        
        print("\nğŸ”§ ä¿®æ­£ææ¡ˆ")
        print("=" * 60)
        
        corrections_made = 0
        
        for result in self.validation_results:
            if not result['is_valid']:
                corrections_made += 1
                
                # ä¿®æ­£ææ¡ˆã®ç”Ÿæˆ
                level = result['level']
                content = result['original_content_preview']
                cluster_size = result['cluster_size']
                
                # ã‚ˆã‚Šé©åˆ‡ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ææ¡ˆ
                suggested_keywords = self._suggest_keywords(content, level)
                
                # ä¿®æ­£ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ç”Ÿæˆ
                if suggested_keywords:
                    corrected_label = self._generate_corrected_label(level, suggested_keywords, cluster_size)
                else:
                    corrected_label = f"{LEVEL_COLOR_MAPPING.get(level, {}).get('name', f'L{level}')}\n({cluster_size})"
                
                correction = {
                    'node_id': result['node_id'],
                    'level': level,
                    'original_label': result['generated_label'],
                    'corrected_label': corrected_label,
                    'suggested_keywords': suggested_keywords,
                    'reason': result['validation_message']
                }
                
                self.corrections.append(correction)
                
                print(f"ğŸ”§ {result['node_id']}")
                print(f"   ä¿®æ­£å‰: {result['generated_label']}")
                print(f"   ä¿®æ­£å¾Œ: {corrected_label}")
                print(f"   ç†ç”±: {result['validation_message']}")
                print("-" * 60)
        
        if corrections_made == 0:
            print("âœ… ä¿®æ­£ãŒå¿…è¦ãªãƒãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        return corrections_made
    
    def _suggest_keywords(self, content, level):
        """ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸé©åˆ‡ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ææ¡ˆ"""
        
        if level == 0:
            vocab_set = IMMUNE_HIERARCHY_VOCAB["hsc_level"]
        elif level == 1:
            vocab_set = IMMUNE_HIERARCHY_VOCAB["clp_level"]
        elif level == 2:
            vocab_set = IMMUNE_HIERARCHY_VOCAB["cd4_t_level"]
        elif level == 3:
            vocab_set = IMMUNE_HIERARCHY_VOCAB["treg_level"]
        else:
            return []
        
        # æ—¥æœ¬èªãƒ»è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰é–¢é€£ç”¨èªã‚’æ¤œç´¢
        all_keywords = set()
        all_keywords.update(vocab_set["japanese"])
        all_keywords.update(vocab_set["english"])
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        found_keywords = []
        content_lower = content.lower()
        
        for keyword in all_keywords:
            if keyword.lower() in content_lower:
                found_keywords.append(keyword)
        
        return found_keywords[:3]  # æœ€å¤§3ã¤
    
    def _generate_corrected_label(self, level, keywords, cluster_size):
        """ä¿®æ­£ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ"""
        
        level_info = LEVEL_COLOR_MAPPING.get(level, {"name": f"L{level}", "description": "ä¸æ˜"})
        
        if keywords:
            keyword_text = "ãƒ»".join(keywords)
            if len(keyword_text) > 20:
                keyword_text = keyword_text[:17] + "..."
            return f"{level_info['name']}\n{keyword_text}\n({cluster_size})"
        else:
            return f"{level_info['name']}\n{level_info['description']}\n({cluster_size})"
    
    def print_validation_summary(self, summary):
        """æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        
        print(f"\nğŸ“Š æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        print(f"ç·ãƒãƒ¼ãƒ‰æ•°: {summary['total_nodes']}")
        print(f"âœ… é©åˆ‡: {summary['valid_nodes']}")
        print(f"âŒ ä¸é©åˆ‡: {summary['invalid_nodes']}")
        print(f"æ­£è§£ç‡: {summary['valid_nodes']/summary['total_nodes']*100:.1f}%")
        
        print(f"\nğŸ“ˆ ãƒ¬ãƒ™ãƒ«åˆ¥åˆ†æ:")
        for level, analysis in summary['level_analysis'].items():
            level_info = LEVEL_COLOR_MAPPING.get(level, {"name": f"L{level}", "description": "ä¸æ˜"})
            accuracy = analysis['valid'] / analysis['count'] * 100 if analysis['count'] > 0 else 0
            
            print(f"  {level_info['name']} ({level_info['description']}):")
            print(f"    ãƒãƒ¼ãƒ‰æ•°: {analysis['count']}")
            print(f"    æ­£è§£ç‡: {accuracy:.1f}%")
            
            # ç™ºè¦‹ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é »åº¦åˆ†æ
            if analysis['keywords_found']:
                unique_keywords = list(set(analysis['keywords_found']))[:5]
                print(f"    ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(unique_keywords)}")
    
    def save_validation_report(self, summary, filename=None):
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"immune_terminology_validation_{timestamp}.json"
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': summary,
            'validation_results': self.validation_results,
            'corrections': self.corrections
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filename}")

def main():
    print("ğŸ§¬ å…ç–«å­¦ç”¨èªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    validator = ImmuneTerminologyValidator()
    
    try:
        # ãƒ„ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        tree_data, filename = validator.load_latest_tree()
        print(f"ğŸ“Š Tree loaded: {len(tree_data['nodes'])} nodes")
        
        # å…¨ãƒãƒ¼ãƒ‰æ¤œè¨¼
        summary = validator.validate_all_nodes(tree_data)
        
        # ä¿®æ­£ææ¡ˆç”Ÿæˆ
        corrections_count = validator.generate_corrections()
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        validator.print_validation_summary(summary)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        validator.save_validation_report(summary)
        
        print(f"\nâœ… æ¤œè¨¼å®Œäº†!")
        print(f"ğŸ“Š ç·ãƒãƒ¼ãƒ‰æ•°: {summary['total_nodes']}")
        print(f"ğŸ¯ æ­£è§£ç‡: {summary['valid_nodes']/summary['total_nodes']*100:.1f}%")
        print(f"ğŸ”§ ä¿®æ­£ææ¡ˆ: {corrections_count}ä»¶")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()