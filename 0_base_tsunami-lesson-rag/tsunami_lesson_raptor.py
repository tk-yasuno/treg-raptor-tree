"""
æ±æ—¥æœ¬å¤§éœ‡ç½ æ•™è¨“ç¶™æ‰¿ RAPTOR ã‚·ã‚¹ãƒ†ãƒ 
Tsunami Lesson RAPTOR: Hierarchical Retrieval for Disaster Education

æ±æ—¥æœ¬å¤§éœ‡ç½ã®æ•™è¨“ã‚’éšå±¤çš„ã«æ§‹é€ åŒ–ã—ã€åŠ¹ç‡çš„ãªæ¤œç´¢ã¨è¦ç´„ã‚’å®Ÿç¾ã™ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ 
- å¾©èˆˆåºè³‡æ–™ã€ä»Šæ‘æ–‡å½¦æ•™æˆã®ç ”ç©¶ã€é˜²ç½æ•™è‚²äº‹ä¾‹ã‚’çµ±åˆ
- RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) ã«ã‚ˆã‚‹éšå±¤æ¤œç´¢
- FAISS + K-means ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹æœ€é©åŒ–

Version: 1.0 - Tsunami Lesson Edition
"""

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
import faiss
import numpy as np
from typing import List, Dict, Tuple, Optional
import time
import json
import pickle
from pathlib import Path
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# GPUæœ€é©åŒ–ç‰ˆã®RAPTORRetrieverEvalã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ï¼‰
from raptor_eval import RAPTORRetrieverEval


class TsunamiLessonRAPTOR(RAPTORRetrieverEval):
    """
    æ±æ—¥æœ¬å¤§éœ‡ç½ã®æ•™è¨“ã‚’ç¶™æ‰¿ã™ã‚‹ãŸã‚ã®RAPTORã‚·ã‚¹ãƒ†ãƒ 
    
    ç‰¹å¾´:
    - ç½å®³æ•™è¨“ã«ç‰¹åŒ–ã—ãŸãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    - éšå±¤çš„ãªçŸ¥è­˜æ§‹é€ ã®æ§‹ç¯‰
    - æ–‡è„ˆã‚’ä¿æŒã—ãŸè¦ç´„ç”Ÿæˆ
    """
    
    def __init__(
        self,
        embeddings_model,
        llm,
        min_clusters: int = 2,
        max_clusters: int = 5,
        max_depth: int = 3,
        chunk_size: int = 500,
        chunk_overlap: int = 100,
        selection_strategy: str = 'silhouette',
        metric_weights: Optional[Dict[str, float]] = None,
        disaster_domain: str = 'tohoku_earthquake',
        **kwargs  # è¦ªã‚¯ãƒ©ã‚¹ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚‹
    ):
        """
        Args:
            disaster_domain: ç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³ ('tohoku_earthquake', 'general_disaster' ãªã©)
            metric_weights: ãƒ¡ãƒˆãƒªãƒƒã‚¯é‡ã¿ï¼ˆcombinedæˆ¦ç•¥ç”¨ï¼‰
            max_depth: ãƒ„ãƒªãƒ¼ã®æœ€å¤§æ·±ã•ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ï¼‰
            selection_strategy: ã‚¯ãƒ©ã‚¹ã‚¿æ•°é¸æŠæˆ¦ç•¥ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'silhouette'ï¼‰
            **kwargs: è¦ªã‚¯ãƒ©ã‚¹(RAPTORRetrieverEval)ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®metric_weightsã‚’è¨­å®šï¼ˆsilhouetteé‡è¦–ï¼‰
        if metric_weights is None:
            metric_weights = {'silhouette': 1.0, 'dbi': 0.0, 'chi': 0.0}
        
        super().__init__(
            embeddings_model=embeddings_model,
            llm=llm,
            min_clusters=min_clusters,
            max_clusters=max_clusters,
            max_depth=max_depth,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            selection_strategy=selection_strategy,
            metric_weights=metric_weights,
            **kwargs  # n_iterãªã©ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™
        )
        
        self.disaster_domain = disaster_domain
        print(f"ğŸŒŠ Tsunami Lesson RAPTOR initialized")
        print(f"   Domain: {disaster_domain}")
        print(f"   Optimized for disaster education and lesson inheritance")
    
    def create_disaster_specific_prompt(self) -> ChatPromptTemplate:
        """
        ç½å®³æ•™è¨“ã«ç‰¹åŒ–ã—ãŸè¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        """
        template = """ã‚ãªãŸã¯æ±æ—¥æœ¬å¤§éœ‡ç½ã®æ•™è¨“ã‚’æ¬¡ä¸–ä»£ã«ç¶™æ‰¿ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®æ–‡æ›¸ç¾¤ã¯ã€ç½å®³å¯¾å¿œã‚„é˜²ç½æ•™è‚²ã«é–¢ã™ã‚‹é‡è¦ãªæƒ…å ±ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚

ã“ã‚Œã‚‰ã®æ–‡æ›¸ã‚’ã€ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰è¦ç´„ã—ã¦ãã ã•ã„ï¼š
1. ç½å®³æ™‚ã®å…·ä½“çš„ãªå¯¾å¿œã‚„äº‹ä¾‹
2. å¾—ã‚‰ã‚ŒãŸæ•™è¨“ã‚„å­¦ã³
3. ä»Šå¾Œã®é˜²ç½ã«æ´»ã‹ã›ã‚‹çŸ¥è¦‹
4. æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚„å›ºæœ‰åè©ã¯æ­£ç¢ºã«ä¿æŒ

æ–‡æ›¸ç¾¤ï¼š
{documents}

è¦ç´„ï¼ˆæ—¥æœ¬èªã§300-500æ–‡å­—ï¼‰ï¼š"""
        
        return ChatPromptTemplate.from_template(template)
    
    def load_tohoku_earthquake_data(self, file_path: str = None) -> List[Document]:
        """
        æ±æ—¥æœ¬å¤§éœ‡ç½ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            file_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: tohoku_earthquake_data.txtï¼‰
        """
        if file_path is None:
            file_path = Path(__file__).parent / "tohoku_earthquake_data.txt"
        
        print(f"ğŸ“– Loading disaster knowledge base: {file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥èª­ã¿è¾¼ã¿ï¼ˆTextLoaderã®æ–‡å­—æ•°åˆ¶é™ã‚’å›é¿ï¼‰
        with open(file_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        print(f"Loaded document length: {len(full_text)} characters")
        
        # Document ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        doc = Document(page_content=full_text, metadata={"source": str(file_path)})
        
        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
        )
        chunks = text_splitter.split_documents([doc])
        
        print(f"Split into {len(chunks)} chunks")
        print(f"âœ… Loaded {len(chunks)} chunks from disaster knowledge base")
        print(f"   Average chunk size: {sum(len(c.page_content) for c in chunks) // len(chunks)} characters")
        
        return chunks
    
    def build_disaster_tree(
        self,
        documents: List[Document],
        save_dir: str = "saved_models/tsunami_lesson"
    ) -> Dict:
        """
        ç½å®³æ•™è¨“ã®éšå±¤ãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰
        
        Args:
            documents: å…¥åŠ›ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            save_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            ãƒ„ãƒªãƒ¼æ§‹é€ ã®è¾æ›¸
        """
        print("\n" + "="*80)
        print("ğŸŒ² Building Tsunami Lesson Tree (ç½å®³æ•™è¨“éšå±¤ãƒ„ãƒªãƒ¼æ§‹ç¯‰)")
        print("="*80)
        
        start_time = time.time()
        
        # ç½å®³ç‰¹åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦è¦ç´„
        disaster_prompt = self.create_disaster_specific_prompt()
        
        # å…ƒã®LLMãƒã‚§ãƒ¼ãƒ³ã‚’ä¸€æ™‚çš„ã«ç½®ãæ›ãˆ
        original_llm = self.llm
        self.llm = disaster_prompt | self.llm | StrOutputParser()
        
        # éšå±¤æ§‹ç¯‰
        tree = self.build_tree(documents, depth=0)
        
        # LLMãƒã‚§ãƒ¼ãƒ³ã‚’æˆ»ã™
        self.llm = original_llm
        
        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        total_time = time.time() - start_time
        print(f"\nâœ… Tree construction completed in {total_time:.2f} seconds")
        print(f"   Total nodes: {self._count_nodes(tree)}")
        print(f"   Tree depth: {self._get_tree_depth(tree)}")
        
        # ä¿å­˜ï¼ˆãƒ„ãƒªãƒ¼ã‚’ã‚»ãƒƒãƒˆ - ä¸¡æ–¹ã®å±æ€§ã«è¨­å®šï¼‰
        self.tree = tree
        self.tree_structure = tree  # retrieveãƒ¡ã‚½ãƒƒãƒ‰ãŒå‚ç…§ã™ã‚‹
        if save_dir:
            self.save(save_dir)
        
        return tree
    
    def _count_nodes(self, tree: Dict, count: int = 0) -> int:
        """ãƒ„ãƒªãƒ¼ã®ãƒãƒ¼ãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        count += 1
        if 'children' in tree:
            for child in tree['children']:
                count = self._count_nodes(child, count)
        return count
    
    def _get_tree_depth(self, tree: Dict, current_depth: int = 0) -> int:
        """ãƒ„ãƒªãƒ¼ã®æ·±ã•ã‚’å–å¾—"""
        if 'children' not in tree or len(tree['children']) == 0:
            return current_depth
        
        max_child_depth = 0
        for child in tree['children']:
            child_depth = self._get_tree_depth(child, current_depth + 1)
            max_child_depth = max(max_child_depth, child_depth)
        
        return max_child_depth
    
    def search_lessons(
        self,
        query: str,
        tree: Dict = None,
        top_k: int = 5,
        use_hierarchical: bool = True
    ) -> List[Tuple[str, float, int]]:
        """
        ç½å®³æ•™è¨“ã‚’æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            tree: ãƒ„ãƒªãƒ¼æ§‹é€ ï¼ˆNoneã®å ´åˆã¯ä¿å­˜æ¸ˆã¿ã‚’èª­ã¿è¾¼ã¿ï¼‰
            top_k: å–å¾—ã™ã‚‹æ–‡æ›¸æ•°
            use_hierarchical: éšå±¤æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            
        Returns:
            (æ–‡æ›¸å†…å®¹, ã‚¹ã‚³ã‚¢, éšå±¤ãƒ¬ãƒ™ãƒ«) ã®ãƒªã‚¹ãƒˆ
        """
        print(f"\nğŸ” Searching for lessons: '{query}'")
        
        # ãƒ„ãƒªãƒ¼ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€self.treeã¾ãŸã¯self.tree_structureã‚’ä½¿ç”¨
        if tree is None:
            if hasattr(self, 'tree') and self.tree is not None:
                # build_disaster_treeã§æ§‹ç¯‰ã—ãŸå ´åˆ
                self.tree_structure = self.tree
            elif hasattr(self, 'tree_structure') and self.tree_structure is not None:
                # loadã§èª­ã¿è¾¼ã‚“ã å ´åˆ
                pass  # self.tree_structureã‚’ãã®ã¾ã¾ä½¿ç”¨
            else:
                raise ValueError("ãƒ„ãƒªãƒ¼ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«build_disaster_tree()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        else:
            # æ˜ç¤ºçš„ã«ãƒ„ãƒªãƒ¼ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
            self.tree_structure = tree
        
        # åŸºåº•ã‚¯ãƒ©ã‚¹ã®retrieveãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ï¼ˆå†…éƒ¨ã§self.tree_structureã‚’ä½¿ç”¨ï¼‰
        documents = self.retrieve(query, top_k)
        
        # Document ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ (content, score, level) ã®ã‚¿ãƒ—ãƒ«ã«å¤‰æ›
        results = []
        for doc in documents:
            content = doc.page_content
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ã‚³ã‚¢ã¨ãƒ¬ãƒ™ãƒ«ã‚’å–å¾—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
            # æ³¨: search_treeãƒ¡ã‚½ãƒƒãƒ‰ã¯'similarity'ã‚­ãƒ¼ã§ä¿å­˜ã™ã‚‹
            score = doc.metadata.get('similarity', doc.metadata.get('score', 0.0))
            level = doc.metadata.get('level', 0)
            results.append((content, score, level))
        
        print(f"âœ… Found {len(results)} relevant documents")
        
        return results
    
    def generate_lesson_summary(
        self,
        query: str,
        search_results: List[Tuple[str, float, int]],
        context_window: int = 3
    ) -> str:
        """
        æ¤œç´¢çµæœã‹ã‚‰æ•™è¨“è¦ç´„ã‚’ç”Ÿæˆ
        
        Args:
            query: å…ƒã®ã‚¯ã‚¨ãƒª
            search_results: æ¤œç´¢çµæœ
            context_window: ä½¿ç”¨ã™ã‚‹æ–‡æ›¸æ•°
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸè¦ç´„
        """
        # ä¸Šä½ã®æ–‡æ›¸ã‚’é¸æŠ
        top_docs = search_results[:context_window]
        
        # æ–‡æ›¸ã‚’çµåˆ
        context = "\n\n---\n\n".join([doc for doc, _, _ in top_docs])
        
        # è¦ç´„ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = ChatPromptTemplate.from_template(
            """ã‚ãªãŸã¯é˜²ç½æ•™è‚²ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€æä¾›ã•ã‚ŒãŸæ–‡æ›¸ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æ•™è¨“ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

è³ªå•: {query}

å‚è€ƒæ–‡æ›¸:
{context}

å›ç­”ï¼ˆé‡è¦ãªæ•™è¨“ã¨ãã®æ ¹æ‹ ã‚’å«ã‚ã¦ã€300-500æ–‡å­—ã§ï¼‰:"""
        )
        
        chain = prompt | self.llm | StrOutputParser()
        
        print(f"\nğŸ’¡ Generating lesson summary...")
        
        summary = chain.invoke({
            "query": query,
            "context": context
        })
        
        return summary
    
    def export_tree_visualization(
        self,
        tree: Dict,
        output_path: str = "tsunami_lesson_tree.json"
    ):
        """
        ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’å¯è¦–åŒ–ç”¨ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        
        Args:
            tree: ãƒ„ãƒªãƒ¼æ§‹é€ 
            output_path: å‡ºåŠ›å…ˆãƒ‘ã‚¹
        """
        print(f"\nğŸ“Š Exporting tree structure for visualization...")
        
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒ„ãƒªãƒ¼æ§‹é€ ã‚’ä½œæˆ
        def simplify_tree(node, level=0):
            simplified = {
                "level": level,
                "has_summary": "summary" in node and node["summary"] is not None,
                "summary_length": len(node.get("summary", "")) if node.get("summary") else 0,
                "num_chunks": len(node.get("chunk_ids", [])),
                "children": []
            }
            
            if "children" in node:
                for child in node["children"]:
                    simplified["children"].append(simplify_tree(child, level + 1))
            
            return simplified
        
        viz_tree = simplify_tree(tree)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(viz_tree, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Tree structure exported to {output_path}")
    
    def evaluate_search_quality(
        self,
        test_queries: List[str],
        tree: Dict,
        ground_truth: Dict[str, List[str]] = None
    ) -> Dict[str, float]:
        """
        æ¤œç´¢å“è³ªã‚’è©•ä¾¡
        
        Args:
            test_queries: ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
            tree: ãƒ„ãƒªãƒ¼æ§‹é€ 
            ground_truth: æ­£è§£ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¯ã‚¨ãƒª -> æœŸå¾…ã•ã‚Œã‚‹å›ç­”ã®ãƒªã‚¹ãƒˆï¼‰
            
        Returns:
            è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        print(f"\nğŸ“ˆ Evaluating search quality with {len(test_queries)} queries...")
        
        results = {
            'avg_search_time': 0.0,
            'avg_num_results': 0.0,
            'queries_tested': len(test_queries)
        }
        
        total_time = 0.0
        total_results = 0
        
        for query in test_queries:
            start = time.time()
            search_results = self.search_lessons(query, tree, top_k=5)
            search_time = time.time() - start
            
            total_time += search_time
            total_results += len(search_results)
        
        results['avg_search_time'] = total_time / len(test_queries)
        results['avg_num_results'] = total_results / len(test_queries)
        
        print(f"âœ… Evaluation completed")
        print(f"   Average search time: {results['avg_search_time']:.3f} seconds")
        print(f"   Average results per query: {results['avg_num_results']:.1f}")
        
        return results


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    from langchain_ollama import OllamaEmbeddings, ChatOllama
    
    print("="*80)
    print("Tsunami Lesson RAPTOR System")
    print("æ±æ—¥æœ¬å¤§éœ‡ç½ æ•™è¨“ç¶™æ‰¿ã‚·ã‚¹ãƒ†ãƒ ")
    print("="*80)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    print("\nInitializing models...")
    embeddings = OllamaEmbeddings(
        model="mxbai-embed-large",
        base_url="http://localhost:11434"
    )
    llm = ChatOllama(
        model="granite-code:8b",
        temperature=0,
        base_url="http://localhost:11434",
        timeout=300  # 5åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    )
    
    # TsunamiLessonRAPTORåˆæœŸåŒ–
    raptor = TsunamiLessonRAPTOR(
        embeddings_model=embeddings,
        llm=llm,
        min_clusters=2,
        max_clusters=5,
        max_depth=3,
        chunk_size=500,
        chunk_overlap=100,
        selection_strategy='silhouette',
        metric_weights={
            'silhouette': 1.0,
            'dbi': 0.0,
            'chi': 0.0
        },
        disaster_domain='tohoku_earthquake'
    )
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n" + "="*80)
    print("ğŸ“š Step 1: Loading Disaster Knowledge Base")
    print("="*80)
    documents = raptor.load_tohoku_earthquake_data()
    
    # ãƒ„ãƒªãƒ¼æ§‹ç¯‰
    print("\n" + "="*80)
    print("ğŸŒ² Step 2: Building Hierarchical Tree")
    print("="*80)
    tree = raptor.build_disaster_tree(documents, save_dir="saved_models/tsunami_lesson")
    
    # ãƒ„ãƒªãƒ¼å¯è¦–åŒ–ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    raptor.export_tree_visualization(tree, "tsunami_lesson_tree_viz.json")
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    print("\n" + "="*80)
    print("ğŸ” Step 3: Testing Search Functionality")
    print("="*80)
    
    test_queries = [
        "æ´¥æ³¢é¿é›£ã§æœ‰åŠ¹ã ã£ãŸè¡Œå‹•ã¯ï¼Ÿ",
        "ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒ‘ãƒ¼ãƒˆæ–¹å¼ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "é‡œçŸ³ã®å¥‡è·¡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "ç½å®³æ™‚ã®æƒ…å ±ä¼é”ã§é‡è¦ãªã“ã¨ã¯ï¼Ÿ",
        "å¾©èˆˆã¾ã¡ã¥ãã‚Šã®èª²é¡Œã¯ä½•ã§ã™ã‹ï¼Ÿ"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*80}")
        print(f"Query {i}/{len(test_queries)}: {query}")
        print('='*80)
        
        # æ¤œç´¢å®Ÿè¡Œ
        results = raptor.search_lessons(query, tree, top_k=3)
        
        # çµæœè¡¨ç¤º
        print(f"\nğŸ“„ Top {len(results)} Results:")
        for j, (content, score, level) in enumerate(results, 1):
            print(f"\n[{j}] Score: {score:.4f} | Level: {level}")
            print(f"Content preview: {content[:200]}...")
        
        # è¦ç´„ç”Ÿæˆ
        summary = raptor.generate_lesson_summary(query, results, context_window=2)
        print(f"\nğŸ’¡ Generated Summary:")
        print(summary)
        print()
    
    # è©•ä¾¡
    print("\n" + "="*80)
    print("ğŸ“Š Step 4: Evaluation")
    print("="*80)
    eval_results = raptor.evaluate_search_quality(test_queries, tree)
    
    print("\nâœ… All tasks completed successfully!")
    print("="*80)


if __name__ == "__main__":
    main()
