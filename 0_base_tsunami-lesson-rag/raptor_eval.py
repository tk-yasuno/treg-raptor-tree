"""
RAPTOR with FAISS + Multiple Clustering Evaluation Metrics

è¤‡æ•°ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡æŒ‡æ¨™ã‚’çµ±åˆã—ãŸ RAPTOR å®Ÿè£…
- Silhouette Score (ã‚¯ãƒ©ã‚¹ã‚¿ã®å‡é›†åº¦ã¨åˆ†é›¢åº¦)
- Davies-Bouldin Index (ã‚¯ãƒ©ã‚¹ã‚¿é–“é¡ä¼¼åº¦ã¨ã‚¯ãƒ©ã‚¹ã‚¿å†…åˆ†æ•£)
- Calinski-Harabasz Index (åˆ†æ•£æ¯”ç‡)
- BIC (Bayesian Information Criterion)
- AIC (Akaike Information Criterion)

Version: 3.0 - Evaluation Metrics Edition
"""

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
import faiss
import numpy as np
from typing import List, Dict, Tuple, Optional
import time
import json
import pickle
from pathlib import Path
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score


class RAPTORRetrieverEval:
    """
    RAPTOR with Multiple Clustering Evaluation Metrics
    
    è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ã§ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æœ€é©åŒ–
    """
    
    def __init__(
        self,
        embeddings_model,
        llm,
        min_clusters: int = 2,
        max_clusters: int = 10,
        max_depth: int = 3,
        chunk_size: int = 800,
        chunk_overlap: int = 200,
        n_iter: int = 20,
        selection_strategy: str = 'silhouette',
        metric_weights: Dict[str, float] = None,
        visual_encoder=None,
        use_multimodal: bool = False,
        multimodal_weight: float = 0.5
    ):
        """
        Args:
            embeddings_model: Embeddings model
            llm: LLM for summarization
            min_clusters: æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿æ•°
            max_clusters: æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿æ•°
            max_depth: æœ€å¤§éšå±¤æ·±ã•
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            chunk_overlap: ãƒãƒ£ãƒ³ã‚¯é‡è¤‡
            n_iter: FAISS k-meansã®åå¾©å›æ•°
            selection_strategy: ã‚¯ãƒ©ã‚¹ã‚¿æ•°é¸æŠæˆ¦ç•¥
                - 'silhouette': Silhouette Score (æ¨å¥¨)
                - 'dbi': Davies-Bouldin Index
                - 'chi': Calinski-Harabasz Index
                - 'bic': BIC (å¾“æ¥æ‰‹æ³•)
                - 'aic': AIC
                - 'combined': è¤‡æ•°æŒ‡æ¨™ã®çµ„ã¿åˆã‚ã›
            metric_weights: combinedæˆ¦ç•¥ã§ã®é‡ã¿
                ä¾‹: {'silhouette': 0.4, 'dbi': 0.3, 'chi': 0.3}
            visual_encoder: ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆColVBERT/ColModernVBERTï¼‰
            use_multimodal: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            multimodal_weight: ç”»åƒåŸ‹ã‚è¾¼ã¿ã®é‡ã¿ï¼ˆ0.0-1.0ï¼‰
        """
        self.embeddings_model = embeddings_model
        self.llm = llm
        self.min_clusters = min_clusters
        self.max_clusters = max_clusters
        self.max_depth = max_depth
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.n_iter = n_iter
        self.selection_strategy = selection_strategy
        self.visual_encoder = visual_encoder
        self.use_multimodal = use_multimodal
        self.multimodal_weight = multimodal_weight
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é‡ã¿è¨­å®š
        # å®Ÿé¨“çµæœ: CHIã¯k=2ã‚’å¼·ãå¥½ã‚€ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚‹ãŸã‚é™¤å¤–
        # Silhouette + DBI ã®ãƒãƒ©ãƒ³ã‚¹ã§æœ€é©åŒ–
        if metric_weights is None:
            self.metric_weights = {
                'silhouette': 0.5,  # ã‚¯ãƒ©ã‚¹ã‚¿å“è³ªï¼ˆãƒŸã‚¯ãƒ­è¦–ç‚¹ï¼‰
                'dbi': 0.5,         # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†é›¢åº¦ï¼ˆãƒã‚¯ãƒ­è¦–ç‚¹ï¼‰
                'chi': 0.0          # CHIé™¤å¤–ï¼ˆk=2ãƒã‚¤ã‚¢ã‚¹å›é¿ï¼‰
            }
        else:
            self.metric_weights = metric_weights
        
        self.tree_structure = {}
        self.stats = {
            'selections': [],
            'silhouette_scores': [],
            'dbi_scores': [],
            'chi_scores': [],
            'bic_scores': [],
            'aic_scores': []
        }
        
        print(f"RAPTOR with {selection_strategy.upper()} evaluation initialized")
        print(f"   Parameters:")
        print(f"   - Cluster range: {min_clusters}-{max_clusters}")
        print(f"   - Strategy: {selection_strategy}")
        if selection_strategy == 'combined':
            print(f"   - Weights: {metric_weights}")
        print(f"   - max_depth: {max_depth}")
        print(f"   - chunk_size: {chunk_size}")
    
    def load_and_split_documents(self, file_path: str, encoding: str = "utf-8") -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        loader = TextLoader(file_path, encoding=encoding)
        documents = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
        )
        
        chunks = text_splitter.split_documents(documents)
        
        print(f"Loaded document length: {len(documents[0].page_content)} characters")
        print(f"Split into {len(chunks)} chunks")
        
        return chunks
    
    def embed_documents(self, documents: List[Document], batch_size: int = 40) -> np.ndarray:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒƒãƒã§åŸ‹ã‚è¾¼ã¿ï¼ˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œãƒ»GPUæœ€é©åŒ–ç‰ˆï¼‰"""
        import torch
        from PIL import Image
        
        all_embeddings = []
        total_batches = (len(documents) + batch_size - 1) // batch_size
        
        if self.use_multimodal and self.visual_encoder:
            print(f"  ğŸ¨ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {len(documents)}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ, {total_batches}ãƒãƒƒãƒ (ã‚µã‚¤ã‚º: {batch_size})")
        else:
            print(f"  ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {len(documents)}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ, {total_batches}ãƒãƒƒãƒ (ã‚µã‚¤ã‚º: {batch_size})")
        
        for batch_idx, i in enumerate(range(0, len(documents), batch_size), 1):
            batch = documents[i:i + batch_size]
            
            # ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
            texts = []
            for doc in batch:
                if hasattr(doc, 'page_content'):
                    texts.append(doc.page_content)
                elif hasattr(doc, 'cached_text'):
                    texts.append(doc.cached_text)
                elif isinstance(doc, str):
                    texts.append(doc)
                else:
                    texts.append(str(doc))
            
            # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å‡¦ç†ï¼ˆGPUæœ€é©åŒ–ï¼‰
            if self.use_multimodal and self.visual_encoder:
                try:
                    # ç”»åƒæº–å‚™
                    images = []
                    valid_indices = []
                    for idx, doc in enumerate(batch):
                        if hasattr(doc, 'image_path') and doc.image_path:
                            try:
                                img = Image.open(doc.image_path).convert('RGB')
                                images.append(img)
                                valid_indices.append(idx)
                            except Exception as e:
                                pass
                    
                    if images:
                        # âœ¨ GPUæœ€é©åŒ–: ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®åŸ‹ã‚è¾¼ã¿ã‚’GPUä¸Šã§èåˆ
                        with torch.no_grad():
                            # 1. ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆCPUã§ç”Ÿæˆï¼‰
                            text_embeddings_np = np.array(
                                self.embeddings_model.embed_documents(texts), 
                                dtype=np.float32
                            )
                            text_embeddings_tensor = torch.from_numpy(text_embeddings_np).to(
                                self.visual_encoder.device
                            )
                            
                            # 2. ç”»åƒåŸ‹ã‚è¾¼ã¿ï¼ˆGPUã§ç”Ÿæˆï¼‰
                            image_embeddings_tensor = self.visual_encoder.encode_image(images)
                            
                            # 3. GPUä¸Šã§èåˆå‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªè»¢é€ã‚’æœ€å°åŒ–ï¼‰
                            for local_idx, doc_idx in enumerate(valid_indices):
                                text_emb = text_embeddings_tensor[doc_idx]
                                img_emb = image_embeddings_tensor[local_idx]
                                
                                # æ¬¡å…ƒèª¿æ•´
                                if text_emb.shape[0] != img_emb.shape[0]:
                                    if img_emb.shape[0] < text_emb.shape[0]:
                                        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆGPUä¸Šã§ï¼‰
                                        padding = torch.zeros(
                                            text_emb.shape[0] - img_emb.shape[0],
                                            device=img_emb.device,
                                            dtype=img_emb.dtype
                                        )
                                        img_emb = torch.cat([img_emb, padding])
                                    else:
                                        # ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ
                                        img_emb = img_emb[:text_emb.shape[0]]
                                
                                # GPUä¸Šã§é‡ã¿ä»˜ãçµåˆ
                                combined = (1 - self.multimodal_weight) * text_emb + \
                                          self.multimodal_weight * img_emb
                                text_embeddings_tensor[doc_idx] = combined
                            
                            # 4. ä¸€æ‹¬ã§CPUã«è»¢é€ï¼ˆè»¢é€å›æ•°ã‚’å‰Šæ¸›ï¼‰
                            text_embeddings = text_embeddings_tensor.cpu().numpy()
                    else:
                        # ç”»åƒãªã—ã®å ´åˆ
                        text_embeddings = np.array(
                            self.embeddings_model.embed_documents(texts), 
                            dtype=np.float32
                        )
                            
                except Exception as e:
                    print(f"    âš ï¸ ç”»åƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {batch_idx}): {e}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ä½¿ç”¨
                    text_embeddings = np.array(
                        self.embeddings_model.embed_documents(texts), 
                        dtype=np.float32
                    )
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                text_embeddings = np.array(
                    self.embeddings_model.embed_documents(texts), 
                    dtype=np.float32
                )
            
            all_embeddings.extend(text_embeddings)
            
            # é€²è¡ŒçŠ¶æ³ã‚’è¡¨ç¤º
            if batch_idx % 10 == 0 or batch_idx == total_batches:
                print(f"    ãƒãƒƒãƒ {batch_idx}/{total_batches} å®Œäº† ({len(all_embeddings)}/{len(documents)})")
        
        if self.use_multimodal and self.visual_encoder:
            print(f"  âœ… ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: {len(all_embeddings)}å€‹")
        else:
            print(f"  âœ… ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: {len(all_embeddings)}å€‹")
        
        return np.array(all_embeddings, dtype=np.float32)
    
    def faiss_kmeans(self, X: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        FAISSã«ã‚ˆã‚‹é«˜é€Ÿk-means
        
        Args:
            X: ãƒ‡ãƒ¼ã‚¿è¡Œåˆ— (n_samples, n_features)
            k: ã‚¯ãƒ©ã‚¹ã‚¿æ•°
            
        Returns:
            labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«
            centroids: ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒ
        """
        n_samples, d = X.shape
        
        if n_samples < k:
            k = n_samples
        
        kmeans = faiss.Kmeans(
            d=d,
            k=k,
            niter=self.n_iter,
            verbose=False,
            gpu=False
        )
        
        kmeans.train(X)
        _, labels = kmeans.index.search(X, 1)
        
        return labels.ravel(), kmeans.centroids
    
    def compute_silhouette(self, X: np.ndarray, labels: np.ndarray) -> float:
        """
        Silhouette Score ã‚’è¨ˆç®—
        
        ç¯„å›²: -1 ã€œ +1 (å¤§ãã„ã»ã©è‰¯ã„)
        """
        n_clusters = len(np.unique(labels))
        
        if n_clusters <= 1 or n_clusters >= len(X):
            return -1.0
        
        try:
            return silhouette_score(X, labels, metric='euclidean')
        except:
            return -1.0
    
    def compute_dbi(self, X: np.ndarray, labels: np.ndarray) -> float:
        """
        Davies-Bouldin Index ã‚’è¨ˆç®—
        
        å°ã•ã„ã»ã©è‰¯ã„
        """
        n_clusters = len(np.unique(labels))
        
        if n_clusters <= 1:
            return float('inf')
        
        try:
            return davies_bouldin_score(X, labels)
        except:
            return float('inf')
    
    def compute_chi(self, X: np.ndarray, labels: np.ndarray) -> float:
        """
        Calinski-Harabasz Index ã‚’è¨ˆç®—
        
        å¤§ãã„ã»ã©è‰¯ã„
        """
        n_clusters = len(np.unique(labels))
        
        if n_clusters <= 1 or n_clusters >= len(X):
            return 0.0
        
        try:
            return calinski_harabasz_score(X, labels)
        except:
            return 0.0
    
    def compute_bic(self, X: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -> float:
        """BICè¨ˆç®—ï¼ˆå¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒç”¨ï¼‰"""
        n_samples, dim = X.shape
        k = len(np.unique(labels))
        
        distances = []
        for i in range(n_samples):
            cluster_id = labels[i]
            centroid = centroids[cluster_id]
            dist = np.linalg.norm(X[i] - centroid)
            distances.append(dist)
        
        distances = np.array(distances)
        variance = np.sum(distances ** 2) / (n_samples - k)
        
        penalty = k * np.log(n_samples) * dim
        likelihood = n_samples * np.log(variance + 1e-10)
        
        return penalty + likelihood
    
    def compute_aic(self, X: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -> float:
        """AICè¨ˆç®—ï¼ˆå¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒç”¨ï¼‰"""
        n_samples, dim = X.shape
        k = len(np.unique(labels))
        
        distances = []
        for i in range(n_samples):
            cluster_id = labels[i]
            centroid = centroids[cluster_id]
            dist = np.linalg.norm(X[i] - centroid)
            distances.append(dist)
        
        distances = np.array(distances)
        variance = np.sum(distances ** 2) / (n_samples - k)
        
        penalty = 2 * k * dim
        likelihood = n_samples * np.log(variance + 1e-10)
        
        return penalty + likelihood
    
    def normalize_scores(self, scores: List[float], reverse: bool = False) -> np.ndarray:
        """
        ã‚¹ã‚³ã‚¢ã‚’0-1ã«æ­£è¦åŒ–
        
        Args:
            scores: ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ
            reverse: Trueã®å ´åˆã€å¤§ãã„å€¤ãŒè‰¯ã„ã‚¹ã‚³ã‚¢ã‚’åè»¢ï¼ˆå°ã•ã„å€¤ãŒè‰¯ã„å½¢å¼ã«ï¼‰
        """
        scores = np.array(scores)
        min_score = np.min(scores)
        max_score = np.max(scores)
        
        if max_score - min_score < 1e-10:
            return np.zeros_like(scores)
        
        normalized = (scores - min_score) / (max_score - min_score)
        
        if reverse:
            normalized = 1.0 - normalized
        
        return normalized
    
    def select_optimal_k(self, X: np.ndarray) -> Tuple[int, np.ndarray, np.ndarray, Dict]:
        """
        è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°é¸æŠ
        
        Returns:
            best_k: æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°
            best_labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«
            best_centroids: ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒ
            scores_dict: å„æŒ‡æ¨™ã®ã‚¹ã‚³ã‚¢
        """
        n_samples = X.shape[0]
        
        max_k = min(self.max_clusters, n_samples)
        min_k = min(self.min_clusters, max_k)
        
        print(f"\nğŸ” Evaluating cluster count using {self.selection_strategy.upper()}...")
        print(f"   Range: {min_k} to {max_k} clusters")
        
        # å„kã«å¯¾ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨è©•ä¾¡
        silhouette_scores = []
        dbi_scores = []
        chi_scores = []
        bic_scores = []
        aic_scores = []
        labels_list = []
        centroids_list = []
        
        for k in range(min_k, max_k + 1):
            labels, centroids = self.faiss_kmeans(X, k)
            labels_list.append(labels)
            centroids_list.append(centroids)
            
            # å„è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
            sil = self.compute_silhouette(X, labels)
            dbi = self.compute_dbi(X, labels)
            chi = self.compute_chi(X, labels)
            bic = self.compute_bic(X, labels, centroids)
            aic = self.compute_aic(X, labels, centroids)
            
            silhouette_scores.append(sil)
            dbi_scores.append(dbi)
            chi_scores.append(chi)
            bic_scores.append(bic)
            aic_scores.append(aic)
            
            print(f"   k={k}: Sil={sil:.4f}, DBI={dbi:.4f}, CHI={chi:.2f}, BIC={bic:.2f}, AIC={aic:.2f}")
        
        # æˆ¦ç•¥ã«å¿œã˜ã¦æœ€é©kã‚’é¸æŠ
        if self.selection_strategy == 'silhouette':
            best_idx = np.argmax(silhouette_scores)
            print(f"\nâœ… Strategy: Silhouette Score (maximize)")
            print(f"   Selected k={min_k + best_idx} (Score={silhouette_scores[best_idx]:.4f})")
            
        elif self.selection_strategy == 'dbi':
            best_idx = np.argmin(dbi_scores)
            print(f"\nâœ… Strategy: Davies-Bouldin Index (minimize)")
            print(f"   Selected k={min_k + best_idx} (Score={dbi_scores[best_idx]:.4f})")
            
        elif self.selection_strategy == 'chi':
            best_idx = np.argmax(chi_scores)
            print(f"\nâœ… Strategy: Calinski-Harabasz Index (maximize)")
            print(f"   Selected k={min_k + best_idx} (Score={chi_scores[best_idx]:.2f})")
            
        elif self.selection_strategy == 'bic':
            best_idx = np.argmin(bic_scores)
            print(f"\nâœ… Strategy: BIC (minimize)")
            print(f"   Selected k={min_k + best_idx} (Score={bic_scores[best_idx]:.2f})")
            
        elif self.selection_strategy == 'aic':
            best_idx = np.argmin(aic_scores)
            print(f"\nâœ… Strategy: AIC (minimize)")
            print(f"   Selected k={min_k + best_idx} (Score={aic_scores[best_idx]:.2f})")
            
        elif self.selection_strategy == 'combined':
            # æ­£è¦åŒ–ã—ã¦çµ„ã¿åˆã‚ã›
            sil_norm = self.normalize_scores(silhouette_scores, reverse=True)  # å¤§â†’å°ã«åè»¢
            dbi_norm = self.normalize_scores(dbi_scores, reverse=False)  # å°ã•ã„æ–¹ãŒè‰¯ã„
            chi_norm = self.normalize_scores(chi_scores, reverse=True)  # å¤§â†’å°ã«åè»¢
            
            # åŠ é‡ã‚¹ã‚³ã‚¢ï¼ˆå°ã•ã„ã»ã©è‰¯ã„å½¢å¼ã«çµ±ä¸€ï¼‰
            combined = (
                self.metric_weights.get('silhouette', 0.4) * sil_norm +
                self.metric_weights.get('dbi', 0.3) * dbi_norm +
                self.metric_weights.get('chi', 0.3) * chi_norm
            )
            
            best_idx = np.argmin(combined)
            print(f"\nâœ… Strategy: Combined Metrics")
            print(f"   Weights: {self.metric_weights}")
            print(f"   Selected k={min_k + best_idx}")
            print(f"   - Silhouette: {silhouette_scores[best_idx]:.4f}")
            print(f"   - DBI: {dbi_scores[best_idx]:.4f}")
            print(f"   - CHI: {chi_scores[best_idx]:.2f}")
            print(f"   - Combined score: {combined[best_idx]:.4f}")
        
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Silhouette
            best_idx = np.argmax(silhouette_scores)
            print(f"\nâš ï¸  Unknown strategy, using Silhouette")
        
        best_k = min_k + best_idx
        best_labels = labels_list[best_idx]
        best_centroids = centroids_list[best_idx]
        
        scores_dict = {
            'silhouette': silhouette_scores,
            'dbi': dbi_scores,
            'chi': chi_scores,
            'bic': bic_scores,
            'aic': aic_scores,
            'k_range': list(range(min_k, max_k + 1))
        }
        
        # çµ±è¨ˆè¨˜éŒ²
        self.stats['selections'].append(best_k)
        self.stats['silhouette_scores'].append(silhouette_scores[best_idx])
        self.stats['dbi_scores'].append(dbi_scores[best_idx])
        self.stats['chi_scores'].append(chi_scores[best_idx])
        
        return best_k, best_labels, best_centroids, scores_dict
    
    def cluster_documents(self, X: np.ndarray) -> Tuple[np.ndarray, int]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        best_k, labels, centroids, scores = self.select_optimal_k(X)
        return labels, best_k
    
    def summarize_cluster(self, documents: List[Document]) -> str:
        """ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¦ç´„ï¼ˆ8000æ–‡å­—å¯¾å¿œç‰ˆï¼‰"""
        print(f"   ğŸ”„ Summarizing {len(documents)} documents...", end=" ", flush=True)
        
        import time
        start_time = time.time()
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        texts = []
        for doc in documents:
            if hasattr(doc, 'page_content'):
                texts.append(doc.page_content)
            elif hasattr(doc, 'cached_text'):
                texts.append(doc.cached_text)
            elif hasattr(doc, 'text_content'):
                texts.append(doc.text_content)
            else:
                texts.append(str(doc))
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã€8000æ–‡å­—ã¾ã§ï¼ˆé«˜é€ŸåŒ–ï¼‰
        combined_text = "\n\n".join(texts)
        max_input_length = 8000  # num_ctx=16384ã§ä½™è£•ã‚ã‚Š
        
        if len(combined_text) > max_input_length:
            # å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sample_ratio = max_input_length / len(combined_text)
            sampled_texts = []
            for text in texts:
                sample_len = int(len(text) * sample_ratio)
                if sample_len > 0:
                    sampled_texts.append(text[:sample_len])
            combined_text = "\n\n".join(sampled_texts)[:max_input_length]
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è¦ç´„
        prompt = ChatPromptTemplate.from_template(
            "ä»¥ä¸‹ã¯ç½å®³æ•™è¨“ã«é–¢ã™ã‚‹è¤‡æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚\n\n"
            "ã€è¦ç´„ã‚¿ã‚¹ã‚¯ã€‘\n"
            "- ä¸»è¦ãªç½å®³äº‹ä¾‹ã€æ•™è¨“ã€å¯¾ç­–ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„\n"
            "- 300-500æ–‡å­—ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„\n"
            "- ç®‡æ¡æ›¸ãã§ã¯ãªãã€æ®µè½å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„\n\n"
            "ã€ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{text}\n\n"
            "ã€è¦ç´„ã€‘"
        )
        
        chain = prompt | self.llm | StrOutputParser()
        
        try:
            summary = chain.invoke({"text": combined_text})
            elapsed = time.time() - start_time
            print(f"âœ… ({len(summary)} chars, {elapsed:.1f}s)")
            return summary
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"âš ï¸ Error ({elapsed:.1f}s): {str(e)[:100]}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ˆé ­1000æ–‡å­—ã‚’è¿”ã™
            return combined_text[:1000]
    
    def _hierarchical_summarize(self, documents: List[Document]) -> str:
        """å¤§é‡æ–‡æ›¸ã®éšå±¤çš„è¦ç´„å‡¦ç†"""
        print(f"\n      ğŸ“Š Hierarchical summarization for {len(documents)} docs...", end=" ", flush=True)
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        texts = []
        for doc in documents:
            if hasattr(doc, 'page_content'):
                texts.append(doc.page_content)
            elif hasattr(doc, 'cached_text'):
                texts.append(doc.cached_text)
            elif hasattr(doc, 'text_content'):
                texts.append(doc.text_content)
            else:
                texts.append(str(doc))
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‹•çš„ã«æ±ºå®šï¼ˆæ–‡æ›¸æ•°ã«å¿œã˜ã¦ï¼‰
        if len(documents) > 200:
            batch_size = 20
        elif len(documents) > 100:
            batch_size = 15
        else:
            batch_size = 10
        
        print(f"\n      ğŸ”„ Processing in batches of {batch_size}...", end=" ", flush=True)
        
        # ãƒãƒƒãƒè¦ç´„ã‚’å®Ÿè¡Œ
        batch_summaries = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_summary = self._summarize_batch(batch_texts, f"batch {i//batch_size + 1}")
            if batch_summary:
                batch_summaries.append(batch_summary)
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
            if i % (batch_size * 5) == 0:
                progress = (i + batch_size) / len(texts) * 100
                print(f"\n      ğŸ“ˆ Progress: {progress:.1f}% ({i + batch_size}/{len(texts)})", end=" ", flush=True)
        
        print(f"\n      ğŸ”— Combining {len(batch_summaries)} batch summaries...", end=" ", flush=True)
        
        # ãƒãƒƒãƒè¦ç´„ã‚’çµ±åˆ
        if len(batch_summaries) == 1:
            final_summary = batch_summaries[0]
        else:
            final_summary = self._combine_summaries(batch_summaries)
        
        print(f"âœ… Done ({len(final_summary)} chars)")
        return final_summary
    
    def _standard_summarize(self, documents: List[Document]) -> str:
        """æ¨™æº–çš„ãªè¦ç´„å‡¦ç†ï¼ˆ50æ–‡æ›¸ä»¥ä¸‹ï¼‰"""
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        texts = []
        for doc in documents:
            if hasattr(doc, 'page_content'):
                texts.append(doc.page_content)
            elif hasattr(doc, 'cached_text'):
                texts.append(doc.cached_text)
            elif hasattr(doc, 'text_content'):
                texts.append(doc.text_content)
            else:
                texts.append(str(doc))
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã€é©åˆ‡ãªé•·ã•ã«åˆ¶é™ (8000æ–‡å­—ã¾ã§)
        combined_text = "\n\n".join(texts)
        max_input_length = 8000
        if len(combined_text) > max_input_length:
            # å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sample_ratio = max_input_length / len(combined_text)
            sampled_texts = []
            for text in texts:
                sample_len = int(len(text) * sample_ratio)
                if sample_len > 0:
                    sampled_texts.append(text[:sample_len])
            combined_text = "\n\n".join(sampled_texts)[:max_input_length]
        
        # ã‚ˆã‚Šå…·ä½“çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = ChatPromptTemplate.from_template(
            "ä»¥ä¸‹ã¯ç½å®³æ•™è¨“ã«é–¢ã™ã‚‹è¤‡æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚\n\n"
            "ã€è¦ç´„ã‚¿ã‚¹ã‚¯ã€‘\n"
            "- ä¸»è¦ãªç½å®³äº‹ä¾‹ã€æ•™è¨“ã€å¯¾ç­–ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„\n"
            "- 300-500æ–‡å­—ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„\n"
            "- ç®‡æ¡æ›¸ãã§ã¯ãªãã€æ®µè½å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„\n\n"
            "ã€ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{text}\n\n"
            "ã€è¦ç´„ã€‘"
        )
        
        return self._execute_summarization(prompt, combined_text)
    
    def _summarize_batch(self, texts: List[str], batch_label: str = "") -> str:
        """ãƒãƒƒãƒãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„"""
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆï¼ˆæœ€å¤§8000æ–‡å­—ã¾ã§ï¼‰
        combined_text = "\n\n".join(texts)
        max_length = 8000
        if len(combined_text) > max_length:
            # æ–‡æ›¸ã‚’å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sample_ratio = max_length / len(combined_text)
            sampled_texts = []
            for text in texts:
                sample_len = int(len(text) * sample_ratio)
                if sample_len > 0:
                    sampled_texts.append(text[:sample_len])
            combined_text = "\n\n".join(sampled_texts)[:max_length]
        
        # ãƒãƒƒãƒç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = ChatPromptTemplate.from_template(
            "ä»¥ä¸‹ã®ç½å®³é–¢é€£æ–‡æ›¸ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\n"
            "ã€è¦ç´„æ–¹é‡ã€‘\n"
            "- é‡è¦ãªç½å®³äº‹ä¾‹ã¨æ•™è¨“ã‚’æŠ½å‡º\n"
            "- 300-400æ–‡å­—ã§ç°¡æ½”ã«\n"
            "- æ®µè½å½¢å¼ã§è¨˜è¿°\n\n"
            "ã€æ–‡æ›¸ã€‘\n{text}\n\n"
            "ã€è¦ç´„ã€‘"
        )
        
        return self._execute_summarization(prompt, combined_text, batch_label)
    
    def _combine_summaries(self, summaries: List[str]) -> str:
        """è¤‡æ•°ã®è¦ç´„ã‚’çµ±åˆ"""
        if not summaries:
            return "è¦ç´„ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        if len(summaries) == 1:
            return summaries[0]
        
        # è¦ç´„ã‚’çµ±åˆ
        combined_summaries = "\n\n".join(summaries)
        
        # çµ±åˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = ChatPromptTemplate.from_template(
            "ä»¥ä¸‹ã¯ç½å®³æ•™è¨“ã«é–¢ã™ã‚‹è¤‡æ•°ã®è¦ç´„ã§ã™ã€‚ã“ã‚Œã‚‰ã‚’çµ±åˆã—ã¦ã€\n"
            "ä¸€ã¤ã®åŒ…æ‹¬çš„ãªè¦ç´„ï¼ˆ400-600æ–‡å­—ï¼‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n"
            "ã€çµ±åˆæ–¹é‡ã€‘\n"
            "- é‡è¤‡ã‚’é¿ã‘ã€æœ€ã‚‚é‡è¦ãªå†…å®¹ã‚’æŠ½å‡º\n"
            "- ç½å®³ã®ç¨®é¡ã€æ™‚æœŸã€å ´æ‰€ã€æ•™è¨“ã‚’æ˜ç¢ºã«\n"
            "- æ®µè½å½¢å¼ã§è«–ç†çš„ã«æ§‹æˆ\n\n"
            "ã€è¦ç´„ç¾¤ã€‘\n{summaries}\n\n"
            "ã€çµ±åˆè¦ç´„ã€‘"
        )
        
        return self._execute_summarization(prompt, combined_summaries, "final_integration")
    
    def _execute_summarization(self, prompt, text: str, context: str = "") -> str:
        """è¦ç´„å®Ÿè¡Œã®å…±é€šå‡¦ç†"""
        chain = prompt | self.llm | StrOutputParser()
        
        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ï¼ˆæœ€å¤§3å›è©¦è¡Œã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼‰
        max_retries = 3
        for attempt in range(max_retries):
            try:
                import time
                start_time = time.time()
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…¥åŠ›å¤‰æ•°ã‚’ç¢ºèª
                input_key = "summaries" if hasattr(prompt, 'input_variables') and "summaries" in prompt.input_variables else "text"
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ (30ç§’)
                summary = chain.invoke({input_key: text})
                
                elapsed = time.time() - start_time
                
                # è¦ç´„ãŒçŸ­ã™ãã‚‹å ´åˆã¯è­¦å‘Š
                if len(summary) < 50:
                    if attempt < max_retries - 1:
                        time.sleep(1)
                        continue
                
                if context:
                    print(f"      âœ… {context} done ({len(summary)} chars, {elapsed:.1f}s)", end=" ", flush=True)
                else:
                    print(f"âœ… Done ({len(summary)} chars, {elapsed:.1f}s)")
                return summary
                
            except Exception as e:
                elapsed = time.time() - start_time if 'start_time' in locals() else 0
                if attempt < max_retries - 1:
                    if context:
                        print(f"      âš ï¸ {context} retry {attempt + 1}...", end=" ", flush=True)
                    time.sleep(2)
                else:
                    if context:
                        print(f"      âŒ {context} failed ({elapsed:.1f}s)", end=" ", flush=True)
                    else:
                        print(f"âŒ Failed after {max_retries} attempts ({elapsed:.1f}s)")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®1000æ–‡å­—ã‚’è¿”ã™
                    fallback = text[:1000]
                    if len(fallback) < 1000:
                        fallback = text
                    return fallback + "..."
        
        # ä¸‡ãŒä¸€ã“ã“ã«åˆ°é”ã—ãŸå ´åˆ
        return text[:1000] + "..."
    
    def build_tree(self, documents: List[Document], depth: int = 0) -> Dict:
        """å†å¸°çš„ã«éšå±¤ãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        print(f"\n{'='*80}")
        print(f"Building tree at depth {depth} with {len(documents)} documents")
        
        # GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤ºï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        try:
            if hasattr(self, 'embeddings_model') and hasattr(self.embeddings_model, 'device'):
                import torch
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    print(f"ğŸ”§ GPU Memory: {memory_used:.2f}GB / {memory_total:.2f}GB ({memory_used/memory_total*100:.1f}%)")
        except:
            pass
        
        print(f"{'='*80}")
        
        # ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰æ¡ä»¶: max_depthåˆ°é”ã€min_clustersæœªæº€ã€ã¾ãŸã¯10ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæœªæº€
        MIN_LEAF_SIZE = 10  # ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ã®æœ€å°æ–‡æ›¸æ•°
        
        if depth >= self.max_depth:
            print(f"âœ‹ Reached max depth ({self.max_depth}). Creating leaf node.")
            return {
                'depth': depth,
                'documents': documents,
                'summaries': [],
                'clusters': {}
            }
        
        if len(documents) < MIN_LEAF_SIZE:
            print(f"âœ‹ Document count ({len(documents)}) is below minimum leaf size ({MIN_LEAF_SIZE}). Creating leaf node.")
            return {
                'depth': depth,
                'documents': documents,
                'summaries': [],
                'clusters': {}
            }
        
        if len(documents) < self.min_clusters:
            print(f"âœ‹ Document count ({len(documents)}) is below min_clusters ({self.min_clusters}). Creating leaf node.")
            return {
                'depth': depth,
                'documents': documents,
                'summaries': [],
                'clusters': {}
            }
        
        # å¤§é‡æ–‡æ›¸ã®å ´åˆã¯å‡¦ç†æ™‚é–“ã®è­¦å‘Š
        if len(documents) > 200:
            print(f"âš ï¸  Large document set detected. This may take several minutes...")
            print(f"   ğŸ’¡ Tip: Consider increasing min_clusters to reduce tree depth")
        
        # Embedding
        embed_start = time.time()
        print(f"ğŸ”„ Generating embeddings for {len(documents)} documents...")
        embeddings = self.embed_documents(documents)
        embed_time = time.time() - embed_start
        print(f"â±ï¸  Embedding time: {embed_time:.2f}ç§’ ({embed_time/60:.1f}åˆ†)")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        cluster_start = time.time()
        print(f"ğŸ”„ Clustering {len(documents)} documents...")
        labels, n_clusters = self.cluster_documents(embeddings)
        cluster_time = time.time() - cluster_start
        print(f"â±ï¸  Clustering time: {cluster_time:.2f}ç§’")
        print(f"ğŸ“Š Generated {n_clusters} clusters")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºçµ±è¨ˆ
        cluster_sizes = {}
        for label in labels:
            cluster_sizes[label] = cluster_sizes.get(label, 0) + 1
        
        avg_cluster_size = sum(cluster_sizes.values()) / len(cluster_sizes)
        max_cluster_size = max(cluster_sizes.values())
        min_cluster_size = min(cluster_sizes.values())
        print(f"ğŸ“ˆ Cluster sizes - Avg: {avg_cluster_size:.1f}, Max: {max_cluster_size}, Min: {min_cluster_size}")
        
        # å¤§ããªã‚¯ãƒ©ã‚¹ã‚¿ã®è­¦å‘Š
        large_clusters = [cid for cid, size in cluster_sizes.items() if size > 100]
        if large_clusters:
            print(f"âš ï¸  Large clusters detected: {large_clusters} (may take longer to summarize)")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«å‡¦ç†
        clusters = {}
        summaries = []
        
        summarize_start = time.time()
        for cluster_id in range(n_clusters):
            cluster_docs = [doc for i, doc in enumerate(documents) if labels[i] == cluster_id]
            print(f"\nğŸ“¦ Cluster {cluster_id}: {len(cluster_docs)} documents")
            
            if len(cluster_docs) == 0:
                continue
            
            # è¦ç´„ç”Ÿæˆ
            cluster_start_time = time.time()
            summary_text = self.summarize_cluster(cluster_docs)
            cluster_end_time = time.time()
            
            summary_doc = Document(
                page_content=summary_text,
                metadata={'cluster_id': cluster_id, 'depth': depth}
            )
            summaries.append(summary_doc)
            
            print(f"   â±ï¸  Cluster {cluster_id} summary: {cluster_end_time - cluster_start_time:.1f}ç§’")
            
            # å†å¸°çš„ã«å­ãƒãƒ¼ãƒ‰ã‚’æ§‹ç¯‰ï¼ˆå°ã•ãªã‚¯ãƒ©ã‚¹ã‚¿ã®ã¿ï¼‰
            if len(cluster_docs) >= self.min_clusters and depth < self.max_depth - 1:
                children = self.build_tree(cluster_docs, depth + 1)
            else:
                children = {
                    'depth': depth + 1,
                    'documents': cluster_docs,
                    'summaries': [],
                    'clusters': {}
                }
            
            clusters[cluster_id] = {
                'summary': summary_doc,
                'documents': cluster_docs,
                'children': children
            }
        
        total_summarize_time = time.time() - summarize_start
        print(f"\nâ±ï¸  Total summarization time: {total_summarize_time:.1f}ç§’ ({total_summarize_time/60:.1f}åˆ†)")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            import gc
            gc.collect()
            if hasattr(self, 'embeddings_model'):
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    print("ğŸ§¹ GPU cache cleared")
        except:
            pass
        
        return {
            'depth': depth,
            'documents': documents,
            'summaries': summaries,
            'clusters': clusters
        }
    
    def search_tree(self, tree: Dict, query: str, top_k: int = 5) -> List[Document]:
        """ãƒ„ãƒªãƒ¼ã‚’æ¤œç´¢"""
        if not tree or 'clusters' not in tree:
            return []
        
        # ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰
        if not tree['clusters']:
            docs = tree.get('documents', [])
            if not docs:
                return []
            
            query_embedding = np.array(self.embeddings_model.embed_query(query), dtype=np.float32)
            doc_embeddings = self.embed_documents(docs)
            
            similarities = np.dot(doc_embeddings, query_embedding) / (
                np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_embedding)
            )
            
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                doc = docs[idx]
                doc.metadata['similarity'] = float(similarities[idx])
                results.append(doc)
            
            return results
        
        # å†…éƒ¨ãƒãƒ¼ãƒ‰
        summaries = tree.get('summaries', [])
        if not summaries:
            return []
        
        query_embedding = np.array(self.embeddings_model.embed_query(query), dtype=np.float32)
        summary_embeddings = self.embed_documents(summaries)
        
        similarities = np.dot(summary_embeddings, query_embedding) / (
            np.linalg.norm(summary_embeddings, axis=1) * np.linalg.norm(query_embedding)
        )
        
        best_cluster_idx = np.argmax(similarities)
        cluster_id = summaries[best_cluster_idx].metadata['cluster_id']
        
        print(f"Selected cluster {cluster_id} at depth {tree['depth']} (similarity: {similarities[best_cluster_idx]:.4f})")
        
        best_cluster = tree['clusters'][cluster_id]
        return self.search_tree(best_cluster['children'], query, top_k)
    
    def index(self, file_path: str, encoding: str = "utf-8"):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ RAPTOR Indexing with {self.selection_strategy.upper()}")
        print(f"{'='*80}")
        print(f"ğŸ“„ File: {file_path}")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        documents = self.load_and_split_documents(file_path, encoding)
        self.tree_structure = self.build_tree(documents)
        
        total_time = time.time() - start_time
        
        print(f"\n{'='*80}")
        print(f"âœ… Indexing complete!")
        print(f"   Total time: {total_time:.2f}ç§’ ({int(total_time//60)}:{int(total_time%60):02d})")
        print(f"{'='*80}")
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:
        """ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
        print(f"\n{'='*80}")
        print(f"ğŸ” Searching for: '{query}'")
        print(f"{'='*80}")
        
        results = self.search_tree(self.tree_structure, query, top_k)
        
        print(f"âœ… Found {len(results)} results")
        
        return results
    
    def save(self, save_dir: str):
        """
        RAGãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆå¿…è¦æœ€å°é™ï¼‰
        
        ä¿å­˜å†…å®¹:
        1. tree_structure.json - ãƒ„ãƒªãƒ¼æ§‹é€ ï¼ˆè¦ç´„å«ã‚€ï¼‰
        2. stats.json - çµ±è¨ˆæƒ…å ±
        3. config.json - è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Args:
            save_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\n{'='*80}")
        print(f"ğŸ’¾ Saving RAPTOR model to: {save_dir}")
        print(f"{'='*80}")
        
        # 1. ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’JSONå½¢å¼ã§ä¿å­˜
        tree_dict = self._tree_to_dict(self.tree_structure)
        with open(save_path / "tree_structure.json", "w", encoding="utf-8") as f:
            json.dump(tree_dict, f, ensure_ascii=False, indent=2)
        print(f"âœ… Saved tree_structure.json")
        
        # 2. çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜ï¼ˆnumpyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›ï¼‰
        stats_serializable = self._make_serializable(self.stats)
        with open(save_path / "stats.json", "w", encoding="utf-8") as f:
            json.dump(stats_serializable, f, ensure_ascii=False, indent=2)
        print(f"âœ… Saved stats.json")
        
        # 3. è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
        config = {
            'min_clusters': self.min_clusters,
            'max_clusters': self.max_clusters,
            'max_depth': self.max_depth,
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'n_iter': self.n_iter,
            'selection_strategy': self.selection_strategy,
            'metric_weights': self.metric_weights
        }
        with open(save_path / "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"âœ… Saved config.json")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’è¡¨ç¤º
        total_size = sum(f.stat().st_size for f in save_path.glob("*.json"))
        print(f"\nğŸ“Š Total size: {total_size / 1024:.2f} KB")
        print(f"{'='*80}")
    
    def _make_serializable(self, obj):
        """numpyå‹ãªã©ã‚’JSON serializable ãªå½¢å¼ã«å¤‰æ›"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    
    def _tree_to_dict(self, node: Dict) -> Dict:
        """ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’JSON serializable ãªè¾æ›¸ã«å¤‰æ›"""
        result = {
            'depth': node.get('depth', 0),
            'summaries': [],
            'documents': [],  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚‚ä¿å­˜
            'clusters': {}
        }
        
        # è¦ç´„ã‚’æ–‡å­—åˆ—åŒ–
        summaries = node.get('summaries', [])
        for summary in summaries:
            if isinstance(summary, Document):
                result['summaries'].append({
                    'content': summary.page_content,
                    'metadata': summary.metadata
                })
            else:
                result['summaries'].append({'content': str(summary), 'metadata': {}})
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚‚ä¿å­˜ï¼ˆå®Œå…¨ãªæ¤œç´¢ã®ãŸã‚ï¼‰
        documents = node.get('documents', [])
        for doc in documents:
            if isinstance(doc, Document):
                result['documents'].append({
                    'content': doc.page_content,
                    'metadata': doc.metadata
                })
            else:
                result['documents'].append({'content': str(doc), 'metadata': {}})
        
        # å­ãƒãƒ¼ãƒ‰ã‚’å†å¸°çš„ã«å¤‰æ›
        clusters = node.get('clusters', {})
        for cluster_id, cluster_data in clusters.items():
            result['clusters'][str(cluster_id)] = {
                'summary': self._doc_to_dict(cluster_data.get('summary')),
                'documents': [],  # å­ãƒãƒ¼ãƒ‰ã®documentsã¯childrenã«å«ã¾ã‚Œã‚‹
                'children': self._tree_to_dict(cluster_data['children']) if 'children' in cluster_data else {}
            }
        
        return result
    
    def _doc_to_dict(self, doc) -> Optional[Dict]:
        """Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›"""
        if doc is None:
            return None
        if isinstance(doc, Document):
            return {
                'content': doc.page_content,
                'metadata': doc.metadata
            }
        return {'content': str(doc), 'metadata': {}}
    
    @classmethod
    def load(cls, save_dir: str, embeddings_model, llm):
        """
        ä¿å­˜ã•ã‚ŒãŸRAGãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            save_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            embeddings_model: Embeddings model
            llm: LLM for summarization
            
        Returns:
            RAPTORRetrieverEval ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        save_path = Path(save_dir)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“‚ Loading RAPTOR model from: {save_dir}")
        print(f"{'='*80}")
        
        # è¨­å®šã‚’èª­ã¿è¾¼ã¿
        with open(save_path / "config.json", "r", encoding="utf-8") as f:
            config = json.load(f)
        print(f"âœ… Loaded config.json")
        
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        instance = cls(
            embeddings_model=embeddings_model,
            llm=llm,
            **config
        )
        
        # ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’èª­ã¿è¾¼ã¿
        with open(save_path / "tree_structure.json", "r", encoding="utf-8") as f:
            tree_dict = json.load(f)
        instance.tree_structure = instance._dict_to_tree(tree_dict)
        print(f"âœ… Loaded tree_structure.json")
        
        # çµ±è¨ˆæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        with open(save_path / "stats.json", "r", encoding="utf-8") as f:
            instance.stats = json.load(f)
        print(f"âœ… Loaded stats.json")
        
        print(f"{'='*80}")
        print(f"âœ… Model loaded successfully!")
        print(f"{'='*80}")
        
        return instance
    
    def _dict_to_tree(self, tree_dict: Dict) -> Dict:
        """è¾æ›¸ã‹ã‚‰ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’å¾©å…ƒ"""
        result = {
            'depth': tree_dict.get('depth', 0),
            'summaries': [],
            'documents': [],  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚å¾©å…ƒ
            'clusters': {}
        }
        
        # è¦ç´„ã‚’Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¾©å…ƒ
        for summary_dict in tree_dict.get('summaries', []):
            doc = Document(
                page_content=summary_dict['content'],
                metadata=summary_dict.get('metadata', {})
            )
            result['summaries'].append(doc)
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¾©å…ƒ
        for doc_dict in tree_dict.get('documents', []):
            doc = Document(
                page_content=doc_dict['content'],
                metadata=doc_dict.get('metadata', {})
            )
            result['documents'].append(doc)
        
        # å­ãƒãƒ¼ãƒ‰ã‚’å†å¸°çš„ã«å¾©å…ƒ
        clusters = tree_dict.get('clusters', {})
        for cluster_id, cluster_data in clusters.items():
            summary_dict = cluster_data.get('summary')
            summary_doc = None
            if summary_dict:
                summary_doc = Document(
                    page_content=summary_dict['content'],
                    metadata=summary_dict.get('metadata', {})
                )
            
            # childrenãŒç©ºã§ãªã„å ´åˆã®ã¿å†å¸°çš„ã«å¾©å…ƒ
            children_data = cluster_data.get('children')
            if children_data and children_data.get('clusters'):
                # å†…éƒ¨ãƒãƒ¼ãƒ‰
                children = self._dict_to_tree(children_data)
            elif children_data:
                # ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼šchildrenã®documentsã‚’æŒã¤
                children = self._dict_to_tree(children_data)
            else:
                # childrenãªã—
                children = {}
            
            result['clusters'][int(cluster_id)] = {
                'summary': summary_doc,
                'documents': [],  # å­ãƒãƒ¼ãƒ‰ã®documentsã¯childrenã«å«ã¾ã‚Œã‚‹
                'children': children
            }
        
        return result
